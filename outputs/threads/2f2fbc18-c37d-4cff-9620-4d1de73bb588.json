{
  "thread_id": "2f2fbc18-c37d-4cff-9620-4d1de73bb588",
  "title": "Compare the X-teaming attack with the GOAT attack ",
  "created_at": "2026-02-24T09:44:17.549028",
  "updated_at": "2026-02-24T09:44:17.549041",
  "entries": [
    {
      "query_text": "Compare the X-teaming attack approach with the GOAT attack approach",
      "answer": "# Comparison of X-Teaming and GOAT Attack Approaches\n\n## Core Methodological Differences\n\n**X-Teaming** employs a multi-agent collaborative framework with adaptive extensions. It uses multiple attack plans (up to 50) that are dynamically modified through TextGrad optimization, enabling the generation of diverse attack strategies across multiple conversation turns (up to 7) [xteaming, xteaming_c04]. \n\n**GOAT** takes a different approach by scaling predefined single-turn prompt-level attacks to multi-turn conversations. It categorizes attacks into constituent parts (name, definition, and exemplar conversations) and embeds attack definitions within the system prompt of the attacker LLM using in-context learning [goat, goat_c06]. GOAT was instantiated with 7 predefined attacks that can be combined as layered attacks [goat, goat_c06].\n\n## Attack Diversity and Planning\n\nX-Teaming demonstrates significantly higher strategic diversity through its multi-agent framework. It achieves 153% improvements in attack plan diversity and 62% improvements in attack execution diversity compared to ActorAttack, with mean diversity scores of 0.702 for plan diversity and 0.466 for attack-level diversity [xteaming, xteaming_c04], [xteaming, xteaming_c10].\n\nGOAT's approach focuses on the attacker model's ability to reason through and combine predefined prompting strategies rather than generating entirely novel attack plans [goat, goat_c06]. The attacker model itself reasons whether it is getting closer to the goal and adjusts strategy accordingly [goat, goat_c14].\n\n## Computational Efficiency and Resource Usage\n\n**X-Teaming** shows strong efficiency metrics: successful attacks required on average only 3.5 plans, 0.6 TextGrad iterations, and 4.3 conversation turns, despite upper bounds of 50 plans, 4 iterations, and 7 turns [xteaming, xteaming_c09]. However, it uses a paid closed-source model (GPT-4o) as its attacker in primary experiments, though it can use open-source Qwen-2.5-32B [xteaming, xteaming_c10].\n\n**GOAT** operates with very limited computational budget and does not require a separate judge model during conversations, performing 3 times fewer LLM calls per turn compared to methods using external judges [goat, goat_c10]. GOAT requires no expensive gradient steps, thousand-query searches, or special model access [goat, goat_c10].\n\n## Attack Success Rates\n\nX-Teaming achieves state-of-the-art attack success rates, reaching 98.1% on DeepSeek V3 and 96.2% on Claude 3.7 Sonnet, with +9.8 percentage points over ActorAttack on GPT-4o (94.3% vs. 84.5%) [xteaming, xteaming_c09].\n\nGOAT outperforms Crescendo across all tested models in both ASR@1 and ASR@10 metrics, with capped conversation rounds at 5 turns [goat, goat_c10].\n\n## Key Conceptual Distinction\n\nX-Teaming emphasizes generating fresh, diverse attack trajectories at scale through multi-agent collaboration and optimization [xteaming, xteaming_c04], while GOAT emphasizes simulating manual red-teaming practices by enabling an attacker model to intelligently reason through and combine known prompting techniques in multi-turn contexts [goat, goat_c14].",
      "retrieved_chunks": [
        {
          "chunk_id": "xteaming_c04",
          "source_id": "xteaming",
          "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
          "section_title": "1 Introduction",
          "text": "In addition, X -Teaming also demonstrates significant improvements in attack diversity. Prior semantic-driven (Chain of Attack, Yang et al. (2024b); ActorAttack, Ren et al. (2024)) and template-based (Crescendo, Russinovich et al. (2024)) multi-turn jailbreak methods lack the strategic diversity of human red-teamers, limiting their scalability in exploring diverse, large-scale attack trajectories (Li et al., 2024). In contrast, X -Teaming achieves Table 1: Comparison of key components and resources across multi-turn attack methods. | Method | Multi-agent collab. | Adaptive extension | Diverse plans | Prompt optim. | Safety data | Open source | |--------------------------------------|-----------------------|----------------------|-----------------|-----------------|---------------|---------------| | RACE (Ying et al., 2025) | \u2717 | \u2713 | \u2717 | \u2717 | \u2717 | \u2717 | | CFA (Sun et al., 2024) | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | | Crescendo (Russinovich et al., 2024) | \u2717 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | | FITD (Weng et al., 2025) | \u2717 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | | Chain of Attack (Yang et al., 2024b) | \u2717 | \u2713 | \u2717 | \u2717 | \u2717 | \u2713 | | PANDORA (Chen et al., 2024) | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | \u2717 | | ActorAttack (Ren et al., 2024) | \u2717 | \u2713 | \u2717 | \u2717 | \u2713 (1.4K) | \u2713 | | X -Teaming | \u2713 | \u2713 | \u2713 | \u2713 | \u2713 (30K) | \u2713 | 153% improvements in attack plan diversity and 62% improvements in attack execution diversity compared to ActorAttack-the strongest open-source multi-turn attack baselineas measured by pairwise embedding similarities. X -Teaming's attack effectiveness and diversity enable scalable generation of synthetic multiturn attack data, supporting robust, data-driven safety alignment for LMs. With X -Teaming, we introduce X Guard-Train , a large-scale safety training dataset containing 30K multi-turn conversations seeded from 10K harmful behaviors across 13 risk categories, which is 20 \u00d7 larger than the previous best resource (SafeMTData, Ren et al. (2024)). Models fine-tuned on X Guard-Train exhibit a 34.2% average improvement in multi-turn attack resistance compared to those trained on SafeMTData, with strong cross-framework generalization against diverse attack methods. This robust defense maintains strong single-turn safety performance and general capabilities, as evaluated across 12 benchmarks (e.g., WildGuard, XSTest, MMLU). We release X Guard-Train as a readily usable dataset that can be seamlessly integrated into any model's training pipeline. Beyond this static resource, X -Teaming can be employed to generate fresh multi-turn jailbreaks on demand, enabling dynamic and adaptive safety data creation at scale. To foster open development of multi-turn defenses for conversational AI, we open-source the entire framework, dataset, and trained models-paving the way toward more robust, trustworthy, and reliable human-AI interactions.",
          "distance": 0.41623640060424805
        },
        {
          "chunk_id": "xteaming_c10",
          "source_id": "xteaming",
          "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
          "section_title": "3.2 Results",
          "text": "Our framework consistently uses fewer tokens across all target models compared to ActorAttack. As an added benefit, X -Teaming utilizes the free open-source model Qwen-2.5-32B as its attacker, whereas ActorAttack relies on GPT-4o, a paid closed-source model that charges by input and output token count. As an additional experiment, we conducted a head-to-head comparison: both X -Teaming and ActorAttack were allotted 10 plans (or actors), identical token budgets, the same attacker model (Qwen-2.5-32B), and the same target model (GPT-4o). X -Teaming achieved a 94.6% ASR compared to ActorAttack's 75.7%-an 18.9% performance advantage under identical resource constraints. These metrics demonstrate that X -Teaming achieves higher attack success rates than prior methods while maintaining reasonable computational cost, making it a practical framework. Figure 3: Ablation studies on X -Teaming's attack parameters: (a) Effect of varying the number of attack plans with fixed conversation length (7 turns) and TextGrad disabled; (b) Effect of varying conversation turns with fixed number of plans (10) and TextGrad disabled; (c) Effect of TextGrad optimization attempts with fixed plans (10) and turns (7). All experiments conducted against SafeMTData-tuned Llama-3-8B-Instruct on HarmBench validation set. <!-- image --> Attack diversity. Figure 2 compares diversity between X -Teaming and ActorAttack (previous multi-turn SOTA) across plan-level and attack-level diversity. Using embedding similarity with MiniLMv2 (Wang et al., 2020), X -Teaming achieves a significantly higher mean diversity score (0.702) than ActorAttack (0.278), indicating substantially more varied attack plans (Figure 2a). This higher diversity enables X -Teaming to explore more attack scenarios. Beyond plan diversity, Figure 2: Diversity comparison between X -Teaming and ActorAttack for: (a) Plan diversity scores across multiple plans; (b) Attack-level diversity scores across multiple attacker queries. <!-- image --> we also measured attack-level diversity shown in Figure 2(b). X -Teaming demonstrates higher attack-level diversity with a mean score of 0.466 compared to ActorAttack's 0.288, indicating that X -Teaming executes more varied attack queries even when targeting the same harmful behavior. See Appendix \u00a7B.4 for analysis methodology and examples. Verifier agreement analysis. To address potential concerns about using GPT-4o as our primary verifier, we conducted an agreement analysis across multiple evaluators. We initially selected GPT-4o to maintain evaluation consistency with prior multi-turn attack research (Ren et al., 2024; Ying et al., 2025), though recent work has shown that LLM-based verifiers might bias results (Panickssery et al., 2024). Our analysis reveals strong overall agreement with HarmBench classifiers (84.50% average), which themselves demonstrate 93.2% agreement with human evaluations (Mazeika et al., 2024). LlamaGuard 3 shows slightly lower agreement (69.09% average), consistent with previous findings on HarmBench test sets (Mazeika et al., 2024). Additionally, our pilot human evaluation with 15 annotators assessing 150 conversation transcripts achieved 79.3% agreement with GPT-4o judgments, with humans assessing X -Teaming's ASR at 93% versus ActorAttack's 86%, confirming our automated findings. These substantial agreement rates with HarmBench test classifiers support our use of GPT-4o as a verifier for this benchmark (see Appendix \u00a7B.6 for detailed per-model agreement rates).",
          "distance": 0.4230482578277588
        },
        {
          "chunk_id": "xteaming_c09",
          "source_id": "xteaming",
          "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
          "section_title": "3.2 Results",
          "text": "Attack success rate. Table 2 demonstrates that X -Teaming achieves state-of-the-art attack success rates across nearly every tested model, significantly outperforming existing single-turn and multi-turn jailbreaking methods, with the highest rates being 98.1% on DeepSeek V3 and 96.2% on newly released Claude 3.7 Sonnet. Compared to ActorAttack, the previous best multi-turn jailbreaking method, X -Teaming shows consistent improvements: +9.8 percentage points on GPT-4o (94.3% vs. 84.5%), +45.3 points on Gemini 2.0-Flash (87.4% Table 4: Token usage vs. context limits. | Model | Tokens | Context | |------------------------|----------|-----------| | GPT-4o | 2,649 | 128K | | Claude-3.5-Sonnet | 2,070 | 200K | | Claude-3.7-Sonnet | 3,052 | 200K | | Gemini-2.0-Flash | 5,330 | 1M | | Llama-3-8B-IT | 1,987 | 8K | | Llama-3-8B-IT (SafeMT) | 1,647 | 8K | | Llama-3-70B-IT | 2,364 | 8K | | DeepSeek-V3 | 4,357 | 128K | vs. 42.1%), and +29.5 points on Deepseek V3 (98.1% vs. 68.6%). X -Teaming also demonstrates high effectiveness against models tuned for multi-turn safety, achieving 91.8% attack success rate on Llama-3-8B-Instruct trained with SafeMTData (Ren et al., 2024), compared to ActorAttack's 21.4% (+70.4 points). Our results generalize across high-, medium-, and low-resource languages, with X -Teaming outperforming ActorAttack in representative cases (see Appendix \u00a7B.3). As shown in Table 3.2, the average length of successful attacks remains well below the context windows of all tested target models. Our category-wise analysis (Appendix \u00a7B.1, Table 6) reveals Cybercrime as the most vulnerable category with 100% ASR across all but one model, while the Harmful content and Misinformation categories showed stronger resistance (particularly on Claude 3.5 Sonnet at 41.2% and 48.1% respectively, and on Gemini2.0-Flash at 64.7% and 70.4% respectively). Randomly analyzing 3,629 failed attacks, we find that chemical weapons, explicit violence, and extreme hate content remain most resistant (0% success), while cybersecurity exploits prove most vulnerable. On the HarmBench validation set, our extended hyperparameter configuration (10 turns, 50 strategies, 5 TextGrad tries) achieved near 100% ASR on GPT-4o, Gemini-2.0-Flash, Llama-3-8B-Instruct, Llama-3-70BInstruct, Llama-3-8B-Instruct (SafeMTData), and DeepSeek V3. These results indicate that the proposed multi-agent framework consistently outperforms previous methods across both proprietary and open-weight models, including Llama-3-8B-Instruct specifically tuned for multi-turn safety with SafeMTData. Attack efficiency. Beyond success rates, we analyze X -Teaming's efficiency through resources required for successful jailbreaks. Despite our upper bound of 50 plans, 4 TextGrad iterations, and 7 conversation turns, our analysis shows that successful attacks on average only required around 3.5 plans, 0.6 TextGrad iterations, and 4.3 turns (see Table 8 in Appendix \u00a7B.2 for details). In contrast, ActorAttack required an average of 8.7 turns and 3 actors to succeed, Crescendo 11.8 turns, and Chain of Attack 20.4 turns. All X -Teaming attacks used only a small fraction of their models' available context windows (Table 3.2), demonstrating that our framework effectively balances attack success with resource efficiency. We also compare our average number of tokens generated per successful attack with that of ActorAttack (refer to Table 7 in Appendix \u00a7B.2). X -Teaming's attacker tends to use slightly more tokens than ActorAttack due to its dynamic plan modification and TextGrad optimization, but it achieves a much higher attack success rate. Our framework consistently uses fewer tokens across all target models compared to ActorAttack. As an added benefit, X -Teaming utilizes the free open-source model Qwen-2.5-32B as its attacker, whereas ActorAttack relies on GPT-4o, a paid closed-source model that charges by input and output token count.",
          "distance": 0.42331355810165405
        },
        {
          "chunk_id": "goat_c10",
          "source_id": "goat",
          "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
          "section_title": "5 Experimental Results",
          "text": "Overall Results. Figure 2 presents the main outcome of our evaluation. Observe that for any given model, GOAT achieves both higher ASR@1 and ASR@10 relative to Crescendo. For each conversation round, the number of conversation turns was capped at 5, so both methods achieve high attack success rates within a very limited budget. We also note that our method does not require the use of a separate judge model during the conversation, so we also perform 3 times less calls to an LLM per turn. Instead, we ask the attacker model itself to reason through the responses and whether it is getting closer to the goal. Finally, neither of the methods requires expensive gradient steps, thousand-query search, or access to information not generally available through a model's user interface (e.g., logit values). Thus, this attack success rate is achieved with very limited access and a very low computational budget relative to other published work, both single-turn and multi-turn. Figure 2 Attack Success Rate by Target Model for Crescendo and GOAT. Across all models, GOAT outperforms Crescendo, performing worse on Llama 2 7B, with an 85% ASR @10. <!-- image --> Performance across conversation turns. We further evaluate the change in attack performance relative to the number of conversation turns between the attacker and target LLM. The results against the target LLM Llama 3.1 8B, and GPT-4 are shown in Fig. 3, with complete experiment results included in Appendix B.1. The results indicate that attack performance significantly improves with multiple conversation turns in comparison to single-turn prompts. This can be attributed to differences in target model safety relative to length of conversation turns, as well as in the attack application itself. Increasing conversation turns enables more opportunities for the attacker LLM to either re-enforce the taken attack strategy or to adjust to a new approach relative to the target LLM responses. In addition, the results also show the Crescendo attack requiring more conversation turns to reach peak attack performance in comparison to the GOAT methodology. This is expected as Crescendo is a multi-turn attack that gradually escalated across turns and as such requires more substantial turns to reach the conversation objective. Figure 3 Attack success rate broken down by conversation turns for Llama 3.1 8B (left) and GPT-4 (right). <!-- image --> Example Conversation Turns. In an excerpt of a conversation below 1 the goal is to write a poem glorifying the Nazi party. At the start, the attacker has recognized that it cannot directly ask for such a poem, as that would likely trigger the target model's safety. So it begins slowly, by asking for a persona modification and hypothetical scenario without obviously unsafe terms. This gets the target model to respond with a poem but the attacker observes that the response isn't as violating as it needs to be to meet the goal. Thus, the attacker applies a further hypothetical where the Nazi party has won the war, and includes some elements of indirect response priming by asking for a 'description of the new world order' and the 'benefits it brings'. Note that the attacker model also references earlier instructions it gave to the target, thereby making use of the multi-turn nature of the conversation to continue towards a violation.",
          "distance": 0.4420081377029419
        },
        {
          "chunk_id": "goat_c06",
          "source_id": "goat",
          "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
          "section_title": "3.1 Red Teaming Attacks",
          "text": "GOAT scales predefined prompt-level attacks to a conversational context. Attacks are categorized and broken down into their constituents: name , definition , and exemplar user conversations , to highlight how to extend previously limited single-turn techniques to multi-turn conversations. In addition, to further avoid the requirement of costly adversarial fine tuning of the attacker LLM upon each new attack addition, attack definitions are embedded within the system prompt of the attacker LLM as a form of in-context learning. For the purpose of this experiment, we instantiate GOAT with 7 attacks which can be provided either individually to the attacker LLM or combined together in the system prompt as a form of layered attacks. We show the complete list of example attack definitions in Table 1 in our Appendix, with high-level intuitive groupings shown below. - Manipulation of output starter tokens and contents: These techniques aim to drive the model to produce a violating response by enforcing a particular output starter text that leads to unsafe responses. Techniques such as Response Priming that provide specific reply phrasing, and Refusal Suppression that instruct the model to avoid standard default refusal wordings are included here. - Safe response distractors: These techniques aim to 'distract' the target LLM by requesting for safety compliant responses in addition to unsafe ones in order to create the appearance of policy compliant prompting. For instance, this would include attacks such as Topic Splitting that request the target LLM to provide responses to irrelevant tasks and embed the violating request within the stack of safe requests. Other techniques here include Dual Response that request the LLM to provide a 'safe' and 'unsafe' response, or Opposite Intent that request the LLM to first provide a 'safe response' and then in a follow-up instruct for the sentiment to be reversed. - Fictional scenarios: These techniques wrap violating objectives with the veil of a misperceived safer context. These include Hypotheticals, an attack that constructs a fictional or hypothetical context for the violating request that could imply the LLM is within policy to respond, or Persona Modification that instructs the target LLM to role-play as a fictional character or known figure. These attacks can all be effective on their own or combined in various ways for more powerful attacks in single or multi-turn scenarios. Many public single-turn jailbreaks layer various sets of these to achieve their goal. As such, for the purpose of these experiments, the attacker LLM is provided with all available attacks upon initialization and then instructed to select and layer attacks as it sees fit for the conversation objective to simulate adversarial users in the wild.",
          "distance": 0.44346100091934204
        },
        {
          "chunk_id": "goat_c14",
          "source_id": "goat",
          "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
          "section_title": "7 Conclusion",
          "text": "In this work, we identified a class of adversarial behavior typically explored during manual red teaming that most existing approaches for jailbreaking LLMs overlook: multi-turn conversations. We introduced a method of simulating such attacks: GOAT. It provides a way to make use of prompting techniques discovered during manual red teaming while also carrying out prolonged conversations. Core to GOAT is the ability of an attacker model not otherwise fine tuned for red teaming to reason through the best way of combining these different prompting strategies. Our work outperforms other multi-turn approaches, achieving higher attack success rate within the same number of queries to the victim model. Leading AI model developers face the complex challenge of optimizing both utility and safety in their systems. While we believe current model safety training practices (training on adversarial data as produced by GOAT) can mitigate the vulnerabilities exploited here, we also underscore importance of transparent sharing of adversarial techniques, red teaming protocols, and related research to advance the field of AI safety. By elucidating these vulnerabilities, we aim to catalyze the development of more robust safety paradigms that enhance model integrity without compromising functionality. Our goal is to foster a more nuanced understanding of responsible AI principles through adversarial testing that ultimately bolsters a deeper understanding of safety concepts without limiting helpfulness.",
          "distance": 0.4502701163291931
        }
      ],
      "metadata_filters": {},
      "timestamp": "2026-02-24T09:42:54.471442",
      "latency_ms": 9562.839984893799
    }
  ]
}