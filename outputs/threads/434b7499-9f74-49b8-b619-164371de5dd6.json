{
  "thread_id": "434b7499-9f74-49b8-b619-164371de5dd6",
  "title": "What were popular LLM jailbreak techniques were proposed in ",
  "created_at": "2026-02-24T09:39:54.725829",
  "updated_at": "2026-02-24T09:39:54.725845",
  "entries": [
    {
      "query_text": "What were popular LLM jailbreak techniques were proposed in 2024?",
      "answer": "# LLM Jailbreak Techniques Proposed in 2024\n\nBased on the provided context, the following jailbreak techniques were proposed in 2024:\n\n## Multi-Turn and Gradual Escalation Approaches\n\n**Crescendo** is a notable 2024 jailbreak that uses multi-turn conversations to gradually escalate benign prompts into harmful requests. [crescendo, crescendo_c05] Unlike many other jailbreaks, Crescendo does not employ adversarial or malicious text\u2014instead, it relies on the model's own outputs to construct the jailbreak, making it a black-box approach. The paper notes that Crescendo achieved higher Attack Success Rates compared to other state-of-the-art techniques and poses significant challenges for detection due to its use of benign, human-readable prompts. [crescendo, crescendo_c05]\n\n## Persuasion-Based Approaches\n\n**Persuasion taxonomy-based jailbreaks** were developed by Zeng et al. (2024), which employ principles from social science to jailbreak LLMs. [safedecoding, safedecoding_c06]\n\n## Automated Red Teaming\n\n**GOAT (Generative Offensive Agent Tester)** is a 2024 approach for automated red teaming that dynamically combines multiple jailbreaking techniques across conversation turns. [goat, goat_c04] The work also improved upon Crescendo's performance by applying new reasoning techniques to combine different adversarial prompting approaches. [goat, goat_c04]\n\nThese techniques represent a shift toward multi-turn, dynamic, and more sophisticated jailbreaking methods compared to single-prompt approaches.",
      "retrieved_chunks": [
        {
          "chunk_id": "safedecoding_c06",
          "source_id": "safedecoding",
          "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
          "section_title": "2.1 Jailbreak Attacks",
          "text": "Current jailbreak attacks can be categorized into two main classes: empirical jailbreak attacks and optimization-based adversarial attacks . For empirical jailbreak attacks, Liu et al. (2023b) demonstrates prompt engineering can effectively jailbreak ChatGPT. Wei et al. (2023a) identify the root causes of LLMs' susceptibility to jailbreak attacks as competing objectives and generalization mismatch. Li et al. (2023a) show LLMs can be easily hypnotized to generate harmful content. Zeng et al. (2024) employs a persuasion taxonomy from social science to jailbreak LLMs. Huang et al. (2023) find alterations in decoding settings are sufficient to jailbreak many open-source language models. Jiang et al. (2024) develop an ASCII-art based prompt to jailbreak LLMs. Deng et al. (2023c) identify the multilingual jailbreak challenges of LLMs. Optimization-based attacks, which identify adversarial prompts through optimization techniques, can be classified into the following three types (Zeng et al., 2024): (1) Gradient-based methods (Zou et al., 2023; Jones et al., 2023; Zhu et al., 2023) optimize and generate adversarial inputs using gradients (2) Genetic algorithms-based methods (Liu et al., 2023a) utilize mutation and crossover to discover effective jailbreak prompts, and (3) Edit-based methods (Chao et al., 2023) leverage a pre-trained LLM to revise and enhance the adversarial prompt to subvert alignment.",
          "distance": 0.33406561613082886
        },
        {
          "chunk_id": "artprompt_c05",
          "source_id": "artprompt",
          "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
          "section_title": "2 Related Work",
          "text": "Jailbreak Attacks. As LLMs become increasingly integrated in real-world applications, misuses of LLMs and safety concerns (Bender et al., 2021; Bommasani et al., 2021; Carlini et al., 2021; Ganguli et al., 2022; Weidinger et al., 2021) have attracted attention. In particular, multiple jailbreak attacks against LLMs have been developed. Zou et al. (2023) and Jones et al. (2023) proposed gradientbased methods to search for inputs to LLMs that can trigger undesired outputs. Another line of work (Liu et al., 2023) used hierarchical genetic algorithm to automatically generate jailbreak prompts. Chao et al. (2023) proposed to use a pre-trained LLM to generate adversarial prompt to the victim LLM. Alternatively, Mozes et al. (2023) and Kang et al. (2023) exploited instruction-following behaviors of LLMs to disrupt LLM safety. Manuallycrafted prompts for jailbreaking LLMs were constructed by Deng et al. (2023) and Yu et al. (2023). In context demonstrations were used in (Wei et al., 2023b; Wang et al., 2023b). Defenses against Jailbreak Attacks. We categorize current defense against jailbreak attacks into the following two categories. The first is Detection-based Defenses , which involve applying input or output filters to detect and block potentially harmful user prompts. For example, Jain et al. (2023) adopted input perplexity as an input detection mechanism to defend against jailbreak attacks. Helbling et al. (2023) leverages LLM's own capabilities to detect whether it generates harmful outputs. SmoothLLM (Robey et al., 2023) detected harmful inputs by randomly perturbing multiple copies of inputs and aggregating the corresponding outputs to detect adversarial inputs. The second category is Mitigation-based Defenses . Jain et al. (2023) used paraphrase and retokenization to modify the input to reduce the success rate of gradientbased jailbreak attacks. Li et al. (2023b) proposed rewindable auto-regressive inference that allows LLMs to evaluate their own model outputs, and then use the evaluation result to guide the generation towards safer and more reliable outputs. Other defenses leveraged in-context prompt demonstration to enhance the safety awareness of LLMs (Wei et al., 2023b; Xie et al., 2023; Zhang et al., 2023b). Xu et al. (2024) leveraged an expert model focusing on safety and developed a safety-aware decoding strategy for LLMs to mitigate jailbreak attacks. ASCII Art Text. ASCII Art can be used to generate visuals using printable elements and enrich the text environment with enhanced human expressions (Wagner, 2023). Content that can be presented by ASCII Art are diverse, including portraits, objects, and rich-formatting texts. In our paper, we focus on (rich-formatting) texts represented as ASCII Art. We will demonstrate that SOTA LLMs are inadequately equipped to recognize prompts encoding information using ASCII art.",
          "distance": 0.3352353572845459
        },
        {
          "chunk_id": "agentsmith_c05",
          "source_id": "agentsmith",
          "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
          "section_title": "2. Related Work",
          "text": "We primarily introduce related work on multi-agent systems and jailbreaking (M)LLMs, deferring full discussions to Appendix A. Multi-agent systems. A popular recent trend is to create multi-agent systems based on (M)LLMs for downstream applications. Park et al. (2023) propose simulating human behaviors based on multiple LLM agents and discuss the information diffusion phenomenon: as agents communicate, information can spread from agent to agent; Qian et al. (2023) create ChatDev to allow multiple agent roles to communicate and collaborate using conversations to complete the software development life cycle. Similarly, several efforts use multi-agent cooperation to improve performance on different tasks (Du et al., 2023; Wang et al., 2023; Zhang et al., 2023; Chan et al., 2023; Liang et al., 2023). Furthermore, to facilitate the development of multi-agent systems, various multi-agent frameworks have recently been proposed, including CAMEL (Li et al., 2023a), AutoGen (Wu et al., 2023), AgentVerse (Chen et al., 2023), MetaGPT (Hong et al., 2023a), just name a few. In particular, AutoGen provides a practical example of how to build a multi-agent system based on GPT-4V and LLaVA (Li, 2023). Jailbreaking (M)LLMs. LLMs such as ChatGPT/GPT4 (OpenAI, 2023) and LLaMA 2 (Touvron et al., 2023) are typically aligned to generate helpful and harmless responses to human queries, following the training pipeline of human/AI alignment (Ouyang et al., 2022; Ganguli et al., 2022; Bai et al., 2022; Korbak et al., 2023). However, recent research has shown that LLMs can be jailbroken to generate objectionable content by either manually designed or automatically crafted prompts (Zou et al., 2023; Liu et al., 2023f; Rao et al., 2023; Li et al., 2023c; Zhu et al., 2023; Lapid et al., 2023; Liu et al., 2023e; Chao et al., 2023). Moreover, Tian et al. (2023) investigate the safety issues of LLM-based agents. Aside from generating adversarial prompts to jailbreak LLMs, there is another line of redteaming work to attack the alignment of MLLMs using adversarial images (Zhang et al., 2022; Zhao et al., 2023; Qi et al., 2023a; Bailey et al., 2023; Tu et al., 2023; Shayegani et al., 2023; Yin et al., 2023).",
          "distance": 0.37315309047698975
        },
        {
          "chunk_id": "crescendo_c05",
          "source_id": "crescendo",
          "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
          "section_title": "2 Related Works",
          "text": "Jailbreaking large language models (LLMs) is a relatively new topic; however, due to the widespread deployment of LLMs in various applications, several works have explored jailbreaks. We present some of the latest related works here and compare them to Crescendo. Numerous users have published jailbreaks on different platforms like X or other websites [3,4]. Some works [22,27] have investigated these jailbreaks in the wild. These works demonstrate that while most of the jailbreaks are already patched, some were still active at the time of their evaluation. Other works [16,35] show the effect of manipulating the LLM's inference hyperparameters, e.g., temperature, topK, and decoding techniques on the safety alignment. Their results indicate that manipulating these parameters can significantly boost jailbreak success rates and enable the model to overcome its safety alignment. Wei et al. [28] identifies two primary modes of failure (competing objectives and mismatched generalization) in safety alignment. Similarly, [13] illustrates how varying languages, i.e., mismatched generalization, can be used to bypass safety regulations. Expanding on the competing objectives identified by [16], [36] proposes an optimization-based approach for jailbreaking. This method involves optimizing an adversarial suffix that, when appended to a prohibited or malicious task, allows it to bypass the model's safety mechanisms. [21] refines the optimization technique from [36], crafting more stealthy adversarial suffixes that blend in by using normal text rather than random symbols. It is critical to acknowledge that both [21] and [36] require white-box access to fine-tune these adversarial suffixes. Other works explore the utilization of Large Language Models (LLMs) to optimize jailbreaks, such as the Prompt Automatic Iterative Refinement (PAIR) [10] and the Chain of Attack (CoA) [32]. The Contextual Interaction Attack (CIA) [11] introduces an alternative approach for generating multi-turn jailbreaks, which involves creating all dialogue turns with a single LLM call. Similarly, [31] employs a comparable strategy for generating multi-turn interactions and compiles a dataset of 2-turn jailbreaks. Furthermore, [14] and [12] suggest a different method, which involves fine-tuning an LLM to produce jailbreaks. Other techniques [26,33] adapt LLMs to function as fuzzers. Another line of research involves techniques where the adversary explicitly adds context to the query to construct the jailbreak. For example, the Many-shot Jailbreak (MSJ) [7] inserts multiple malicious in-context examples into the target model's context, and [20] incorporates fabricated interactions between the LLM and the user in the history. The main distinction between these methods lies in their scope. MSJ includes over 100 examples, demonstrating that a higher quantity of malicious in-context examples is necessary to jailbreak LLMs effectively, whereas [20] employs a series of simulated multiturn interactions instead of various malicious prompts and responses. In contrast to these jailbreak attacks, Crescendo does not use any adversarial or malicious text in its prompts. The inputs for Crescendo are completely human-readable and innocuous. This is due to Crescendo's design, which relies on the model's own output rather than crafting explicit content. Additionally, Crescendo does not involve explicit optimizations, hence is a black-box jailbreak. Furthermore, our tool, Crescendomation, achieves a higher Attack Success Rate (ASR) compared to other state-of-the-art jailbreak techniques, as shown later in Figure 6 and Table 4. Finally, as Crescendo employs benign questions and prompts to execute the jailbreak, it poses a significantly greater challenge for detection and mitigation compared to other techniques. This is demonstrated by its ability to bypass the safety measures implemented in publicly available AI chat services, as shown in Table 2).",
          "distance": 0.37377166748046875
        },
        {
          "chunk_id": "safedecoding_c31",
          "source_id": "safedecoding",
          "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
          "section_title": "References",
          "text": "2023. SmoothLLM: Defending large language models against jailbreaking attacks. ArXiv preprint , abs/2310.03684. - Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. ArXiv preprint , abs/2312.11805. - Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay - Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint , abs/2307.09288. - Yihan Wang, Zhouxing Shi, Andrew Bai, and ChoJui Hsieh. 2024. Defending llms against jailbreaking attacks via backtranslation. ArXiv preprint , abs/2402.16459. - Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning large language models with human: A survey. ArXiv preprint , abs/2307.12966. - Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023a. Jailbroken: How does LLM safety training fail? ArXiv preprint , abs/2307.02483. - Zeming Wei, Yifei Wang, and Yisen Wang. 2023b. Jailbreak and guard aligned language models with only few in-context demonstrations. ArXiv preprint , abs/2310.06387. - Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. ArXiv preprint , abs/2112.04359. - Fangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, and Xing Xie. 2023a. Defending ChatGPT against jailbreak attack via self-reminder. - Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. 2023b. Multimodal large language models: A survey. In 2023 IEEE International Conference on Big Data (BigData) , pages 2247-2256. IEEE. - Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv preprint , abs/1609.08144. - Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. ArXiv preprint , abs/2309.10253. - Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. 2024. How Johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs. ArXiv preprint , abs/2401.06373. - Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023. Defending large language models against jailbreaking attacks through goal prioritization. ArXiv preprint , abs/2311.09096. - Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William Yang Wang. 2024. Weak-to-strong jailbreaking on large language models. ArXiv preprint , abs/2401.17256. - Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-judge with MT-Bench and chatbot arena. ArXiv preprint , abs/2306.05685. - Andy Zhou, Bo Li, and Haohan Wang. 2024. Robust prompt optimization for defending language models against jailbreaking attacks. ArXiv preprint , abs/2401.17263. - Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. ArXiv preprint , abs/2305.11206. - Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. 2023. Autodan: Automatic and interpretable adversarial attacks on large language models. ArXiv preprint , abs/2310.15140. - Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. ArXiv preprint , abs/2307.15043.",
          "distance": 0.3955347537994385
        },
        {
          "chunk_id": "goat_c04",
          "source_id": "goat",
          "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
          "section_title": "2 Related Works",
          "text": "Jailbreaking research has so far largely focused on discovering a single adversarial prompt that triggers a violating response by a model. Some research studies this problem in an 'open box' setting to employ gradient information along with various optimization methods to arrive at suffixes that can be appended to prompts soliciting harmful information to force the model to respond (Zou et al., 2023; Geiping et al., 2024; Zhu et al., 2023; Geisler et al., 2024; Thompson and Sklar, 2024; Hayase et al., 2024). Others pursue a similar goal in closed-box or semi-open-box settings by repeatedly querying the target model and refining an adversarial prompt directly (Chao et al., 2023; Andriushchenko et al., 2024) or by using an 'attacker' model specialized in adversarial prompting (Paulus et al., 2024; Mehrotra et al., 2023). Gradient-based approaches can be traced back to techniques for finding effective prompts for eliciting information from language models before they were were fine tuned to act as chat assistants (Shin et al., 2020; Guo et al., 2021; Diao et al., 2022). Yet another class of works employs human intuition and knowledge to reprogram the model to act as a harmful persona (Shah et al., 2023), convince it to drop its defenses through psychological manipulation (Zeng et al., 2024), or simply prime it to behave in an unsafe way through demonstration of unsafe behavior (Anil et al., 2024) exploiting in-context learning. Some works (Jiang et al., 2024; Bhatt et al., 2024) aggregate multiple jailbreaking techniques discovered by human red teamers into static datasets. In our work, we propose a method for dynamically making use of such adversarial prompting techniques in the course of multiple conversation turns with the model. The vulnerability of language models in multi-turn settings has previously been identified in other works. Russinovich et al. (2024) proposed an automated method named Crescendo that begins a benign conversation with the model being tested and leads it to produce violating responses through gradual escalation. Perez et al. (2022) proposed a language model fine tuned for adversarial prompting in a multi-turn fashion but the effectiveness of this method is unknown for modern LLMs. Additionally, (Li et al., 2024) introduced a static dataset of adversarial multi-turn conversations. We introduce an automated method that makes use of a new reasoning technique to combine different jailbreaking approaches and show that it can also improve the performance of Crescendo. The research community has also been exploring different methods for compiling datasets of prompts soliciting harmful information and different ways to judge whether a jailbreaking technique is successful. As one of the first datasets of this nature, AdvBench (Zou et al., 2023) introduced 500 instructions soliciting 'detrimental' responses. The prompts in this dataset were selected based on topics that models of the time would commonly refuse and success was judged by a simple heuristic. Subsequent efforts such as StrongReject (Souly et al., 2024) and HarmBench (Mazeika et al., 2024) have further filtered this dataset to select for prompts that are more unequivocally considered violating. They also introduced more sophisticated evaluators and studied agreement of those with human labeling. We report our results based on what we believe to be the latest iteration of this curation and its corresponding scoring method and report all results on the JailbreakBench dataset (Chao et al., 2024) with its judge.",
          "distance": 0.40174710750579834
        },
        {
          "chunk_id": "safedecoding_c30",
          "source_id": "safedecoding",
          "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
          "section_title": "References",
          "text": "2023. Catastrophic jailbreak of open-source llms via exploiting generation. ArXiv preprint , abs/2310.06987. - Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. ArXiv preprint , abs/2309.00614. - Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys , 55(12):1-38. - Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, and Radha Poovendran. 2023. Identifying and mitigating vulnerabilities in llm-integrated applications. ArXiv preprint , abs/2311.16153. - Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, and Radha Poovendran. 2024. Artprompt: Ascii art-based jailbreak attacks against aligned llms. ArXiv preprint , abs/2402.11753. - Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization. ArXiv preprint , abs/2303.04381. - Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. 2023a. Deepinception: Hypnotize large language model to be jailbreaker. ArXiv preprint , abs/2311.03191. - Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 2023b. Rain: Your language models can align themselves without finetuning. ArXiv preprint , abs/2309.07124. - Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023. The unlocking spell on base LLMs: Rethinking alignment via in-context learning. ArXiv preprint , abs/2312.01552. - Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023a. Autodan: Generating stealthy jailbreak prompts on aligned large language models. ArXiv preprint , abs/2310.04451. - Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 2023b. Jailbreaking ChatGPT via prompt engineering: An empirical study. ArXiv preprint , abs/2305.13860. - Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys , 56(2):1-40. - Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744. - Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only. ArXiv preprint , abs/2306.01116. - Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. 2020. BPE-dropout: Simple and effective subword regularization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1882-1892, Online. Association for Computational Linguistics. - Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2024. Finetuning aligned language models compromises safety, even when users do not intend to! In The Twelfth International Conference on Learning Representations . - Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023. SmoothLLM: Defending large language models against jailbreaking attacks. ArXiv preprint , abs/2310.03684. - Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. ArXiv preprint , abs/2312.11805.",
          "distance": 0.40266865491867065
        },
        {
          "chunk_id": "agentsmith_c35",
          "source_id": "agentsmith",
          "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
          "section_title": "A. Related Work (Full Version)",
          "text": "(Multimodal) LLM agents. For a long time, artificial intelligence has been actively engaged in creating intelligent agents that can mimic human thought processes and independently carry out complex tasks (Minsky, 1988; Wooldridge &amp; Jennings, 1995; Russell &amp; Norvig, 2010; Bubeck et al., 2023). Owing to the recent incredible development of large language models (LLMs) (Brown et al., 2020; Kaplan et al., 2020; Ouyang et al., 2022; Korbak et al., 2023), multimodal LLMs (MLLMs) such as GPT-4V (OpenAI, 2023) and Gemini (Team et al., 2023) have demonstrated impressive capabilities, especially in vision-language scenarios. By leveraging the power of LLMs, autonomous agents can make better decisions and perform actions with greater autonomy (Zhou et al., 2023). In an LLM-powered autonomous agent system, an (M)LLM serves as the agent's brain, supported by a number of key components: the planning module decomposes tasks and questions (Yao et al., 2022; 2023; Liu et al., 2023a; Shinn et al., 2023); the memory module stores both the internal log and the external interactions with a user (Sumers et al., 2023; Packer et al., 2023); and the ability to use tools that can call executable workflows or APIs (Schick et al., 2023; Shen et al., 2023; Li et al., 2023b). Recently, there has been a surge of interest in operating systems built around (M)LLMs, which receive screenshots as visual signals and perform subsequent actions. For examples, Liu et al. (2023d) introduce LLaVA-Plus, a general-purpose multimodal agent that learns to use tools based on LLaVA; Yang et al. (2023c) propose an LLM-based multimodal agent framework for operating smartphone applications; Hong et al. (2023b) develop a visual language model that focuses on GUI understanding and navigation. Multi-agent systems. A popular recent trend is to create multi-agent systems based on (M)LLMs for downstream applications. Park et al. (2023) propose simulating human behaviors based on multiple LLM agents and discuss the information diffusion phenomenon: as agents communicate, information can spread from agent to agent; Qian et al. (2023) create ChatDev to allow multiple agent roles to communicate and collaborate using conversations to complete the software development life cycle. Similarly, several efforts use multi-agent cooperation to improve performance on different tasks (Du et al., 2023; Wang et al., 2023; Zhang et al., 2023; Chan et al., 2023; Liang et al., 2023). Furthermore, to facilitate the development of multi-agent systems, various multi-agent frameworks have recently been proposed, including CAMEL (Li et al., 2023a), AutoGen (Wu et al., 2023), AgentVerse (Chen et al., 2023), MetaGPT (Hong et al., 2023a), just name a few. In particular, AutoGen provides a practical example of how to build a multi-agent system based on GPT-4V and LLaVA (Li, 2023). Jailbreaking LLMs. LLMs such as ChatGPT/GPT-4 (OpenAI, 2023) and LLaMA 2 (Touvron et al., 2023) are typically aligned to generate helpful and harmless responses to human queries, following the training pipeline of human/AI alignment (Ouyang et al., 2022; Ganguli et al., 2022; Bai et al., 2022; Korbak et al., 2023). However, red-teaming research has shown that LLMs can be jailbroken to generate objectionable content by either manually designed or automatically crafted prompts (Perez et al., 2022; Zou et al., 2023; Liu et al., 2023f; Rao et al., 2023; Li et al., 2023c; Zhu et al., 2023; Lapid et al., 2023; Liu et al., 2023e; Chao et al., 2023; Ruan et al., 2023; Toyer et al., 2023; Yuan et al., 2023; Deng et al., 2023). Moreover, Tian et al. (2023) investigate the safety issues of LLM-based agents; Greshake et al. (2023) propose indirect prompt injection to jailbreak LLM-integrated applications; Wei et al. (2023a) hypothesize that the vulnerability of aligned LLMs to jailbreaking is attributed to the competing objectives of capability and safety, as well as the mismatch between pretraining and safety training; Carlini et al. (2023) attribute the vulnerability to neural networks' fundamental weakness in dealing with adversarial examples.",
          "distance": 0.40615081787109375
        }
      ],
      "metadata_filters": {
        "year": 2024
      },
      "timestamp": "2026-02-24T09:39:37.380319",
      "latency_ms": 5921.983003616333
    }
  ]
}