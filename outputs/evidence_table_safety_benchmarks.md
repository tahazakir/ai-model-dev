# Evidence Table: LLM Safety Benchmarks

*Generated: 2026-03-01 15:15*

# Evidence Table: Evaluation Benchmarks for LLM Safety and Red Teaming

| Claim | Evidence | Citation | Confidence | Notes |
|---|---|---|---|---|
| HarmBench provides a standardized, large-scale red teaming evaluation framework encompassing 510 behaviors, 18 attack methods, and 33 LLMs | "HarmBench offers a standardized, large-scale evaluation framework for automated red teaming and robust refusal. It includes four functional categories with 510 carefully curated behaviors... The initial set of evaluations includes 18 red teaming methods and 33 closed-source and open-source LLMs." | [harmbench, harmbench_c03] | HIGH | Represents a significant scale increase over prior benchmarks which typically used <100 unique behaviors |
| Prior automated red teaming evaluations lack standardization, making cross-paper comparisons unreliable | "The choice of this parameter [number of tokens generated] can change ASR by up to 30%. Unfortunately, this parameter has not been standardized in prior work, rendering cross-paper comparisons effectively meaningless." | [harmbench, harmbench_c08] | HIGH | Specific to substring-matching ASR metrics; HarmBench addresses this by standardizing to N=512 tokens |
| No current LLM is robust against all evaluated attacks, even after alignment training | "Even after alignment training, no model is robust against all malicious attacks we evaluate against, emphasizing the need for sophisticated, multidimensional defense strategies." | [harmbench, harmbench_c21] | HIGH | Finding applies across all 33 LLMs evaluated; generalizability to future models unknown |
| LLM robustness to adversarial attacks is independent of model size | "No current attack or defense is uniformly effective, and robustness is independent of model size." | [harmbench, harmbench_c03] | HIGH | Counterintuitive finding with significant implications for safety scaling assumptions; requires replication |
| Fine-tuning small LLMs for safety evaluation achieves agreement with human judges comparable to much larger models | "Post fine-tuning, LLMs at a smaller scale (e.g., Llama-3-8b-instruct) can achieve comparably high agreements (over 80%) to the larger ones, with per-pass evaluation costing merely 10s on a single A100 GPU." | [sorrybench, sorrybench_c14] | HIGH | Fine-tuned GPT-3.5-turbo achieves highest agreement at 83.8% κ; efficiency gains are hardware-dependent |
| Existing safety benchmark classifiers (e.g., HarmBench Classifier, Llama-Guard-2, MD-Judge) show substantially lower human agreement than fine-tuned LLM judges | "All the baselines (bottom segment) are agreeing with human evaluators to a substantially lower degree." HarmBench Classifier achieves κ=52.5%, MD-Judge κ=36.0%, vs. fine-tuned models at ~81%. | [sorrybench, sorrybench_c13] | HIGH | Evaluation conducted on SORRY-Bench dataset specifically; results may not generalize across all safety benchmarks |
| Direct comparison of scores across different LLM safety benchmarks is inadvisable due to differing configurations | "These benchmarks may have adopted different configurations (e.g., system prompt, sampling hyperparameters, 'safety' metrics, different datasets and taxonomies)... we strongly caution against direct comparison across these benchmarks." | [sorrybench, sorrybench_c33] | HIGH | Despite this caveat, cross-benchmark trends are noted to be broadly consistent (e.g., Claude and Llama-2 consistently rank as "safest") |
| Multi-turn adversarial interactions represent an underexplored but important class of jailbreak attacks on LLMs | "Recent work by Li et al. (2024) demonstrates that human red teamers can circumvent LLM defenses more easily with multi-turn jailbreaks... Few [techniques] have been reliably automated and none have been applied in multi-turn settings." | [goat, goat_c02] | HIGH | GOAT and X-Teaming both address this gap; single-turn focus of most prior benchmarks limits their real-world applicability |
| Manual red teaming is insufficient at scale due to high cost, motivating automated alternatives | "Human red teaming effectively uncovers vulnerabilities... However, its high cost limits the large-scale deployment. Automated red teaming offers a scalable alternative, using adversarial attacks, jailbreak attacks, and benchmarks to probe defenses." | [fitd, fitd_c29] | HIGH | Consensus view across multiple papers; precise cost figures not provided |
| Existing agent safety benchmarks largely do not address direct malicious user misuse in multi-step tool-use scenarios | "Prior works have generally not used our synthetic tool and narrow scoring approach, instead using an LLM for function emulation and scoring... we believe there is a need to establish a reliable, standardized benchmark for measuring robustness of LLM agents against a broad spectrum of potential misuse scenarios." | [agentharm, agentharm_c05] | HIGH | AgentHarm is proposed to fill this gap; the claim reflects the state of the field at time of publication |