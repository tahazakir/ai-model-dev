{
  "model": "claude-haiku-4-5",
  "system_prompt_hash": "7c1ee11d93629999",
  "user_message_hash": "5943ee7dac3a89b7",
  "max_tokens": 2048,
  "response": "# Differences Between HarmBench and SORRY-Bench\n\n## Taxonomy and Granularity\n\nSORRY-Bench employs a **fine-grained taxonomy of 44 potentially unsafe topics** with 440 class-balanced unsafe instructions [sorrybench, sorrybench_c19]. In contrast, HarmBench's approach focuses on behaviors designed to violate laws or norms based on acceptable use policies from major AI companies (OpenAI, Anthropic, Meta, and Inflection AI) [harmbench, harmbench_c12], without specifying the granularity level of its taxonomy in the provided context.\n\n## Scale and Augmentation\n\nSORRY-Bench takes a more comprehensive augmentation approach, supplementing its base benchmark with **8.8K additional unsafe instructions and 20 diverse linguistic augmentations** [sorrybench, sorrybench_c19]. The context provided does not detail comparable augmentation strategies for HarmBench.\n\n## Design Considerations\n\nHarmBench emphasizes two specific design principles:\n\n1. **Differential harm**: Prioritizing behaviors that pose greater harm via LLMs than through search engines, with contextual and multimodal behaviors designed to make search-based solutions infeasible [harmbench, harmbench_c12].\n\n2. **Dual-intent filtering**: Actively removing behaviors that could have benign interpretations to avoid filtering out legitimate use cases [harmbench, harmbench_c12].\n\nSORRY-Bench's design considerations focus on balance and customizability rather than these specific harm-differential criteria [sorrybench, sorrybench_c19].\n\n## Evaluation Methodology\n\nSORRY-Bench uses a binary classification approach (fulfillment vs. refusal) with human annotation datasets of 7K+ annotations [sorrybench, sorrybench_c11, sorrybench_c19]. The provided HarmBench context does not specify its evaluation methodology.",
  "timestamp": "2026-03-01T19:50:01.747459+00:00"
}