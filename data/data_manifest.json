[
  {
    "source_id": "nist.ai.100-1",
    "filename": "NIST.AI.100-1.pdf",
    "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
    "authors": ["Elham Tabassi"],
    "year": 2023,
    "source_type": "report",
    "venue": null,
    "url_or_doi": "https://doi.org/10.6028/nist.ai.100-1",
    "raw_path": "data/raw/NIST.AI.100-1.pdf",
    "processed_path": "data/processed/nist.ai.100-1.json",
    "tags": ["risk-management", "framework", "safety-standards", "governance"],
    "relevance_note": "Provides foundational framework for identifying and managing AI risks, including safety evaluation standards that inform current benchmark limitations.",
    "type": "report",
    "doi": "10.6028/nist.ai.100-1",
    "doi_url": "https://doi.org/10.6028/nist.ai.100-1",
    "arxiv_id": null,
    "arxiv_url": null,
    "status": "ok"
  },
  {
    "source_id": "aegis2",
    "filename": "aegis2.pdf",
    "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails",
    "authors": ["Shaona Ghosh", "Prasoon Varshney", "Makesh Narsimhan Sreedhar", "Aishwarya Padmakumar", "Traian Rebedea", "J. Varghese", "Christopher Parisien"],
    "year": 2025,
    "source_type": "JournalArticle",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2501.09004",
    "raw_path": "data/raw/aegis2.pdf",
    "processed_path": "data/processed/aegis2.json",
    "tags": ["taxonomy", "safety-dataset", "guardrails", "alignment"],
    "relevance_note": "Presents comprehensive taxonomy of AI safety risks and diverse dataset for evaluating guardrail effectiveness against various attack types.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2501.09004",
    "doi_url": "https://doi.org/10.48550/arXiv.2501.09004",
    "arxiv_id": "2501.09004",
    "arxiv_url": "https://arxiv.org/abs/2501.09004",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "agentharm",
    "filename": "agentharm.pdf",
    "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
    "authors": ["Maksym Andriushchenko", "Alexandra Souly", "Mateusz Dziemian", "Derek Duenas", "Maxwell Lin", "Justin Wang", "Dan Hendrycks", "Andy Zou", "Zico Kolter", "Matt Fredrikson", "Eric Winsor", "Jerome Wynne", "Yarin Gal", "Xander Davies"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "International Conference on Learning Representations",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2410.09024",
    "raw_path": "data/raw/agentharm.pdf",
    "processed_path": "data/processed/agentharm.json",
    "tags": ["benchmark", "agent-safety", "harmful-behavior", "tool-use"],
    "relevance_note": "Benchmark specifically measuring harm in LLM agents, directly addressing how safety vulnerabilities amplify when models have tool-use capabilities.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2410.09024",
    "doi_url": "https://doi.org/10.48550/arXiv.2410.09024",
    "arxiv_id": "2410.09024",
    "arxiv_url": "https://arxiv.org/abs/2410.09024",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "agentsmith",
    "filename": "agentsmith.pdf",
    "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
    "authors": ["Xiangming Gu", "Xiaosen Zheng", "Tianyu Pang", "Chao Du", "Qian Liu", "Ye Wang", "Jing Jiang", "Min Lin"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "International Conference on Machine Learning",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2402.08567",
    "raw_path": "data/raw/agentsmith.pdf",
    "processed_path": "data/processed/agentsmith.json",
    "tags": ["multimodal", "jailbreak", "agent-propagation", "adversarial-image"],
    "relevance_note": "Demonstrates exponential propagation of multimodal jailbreaks across agent systems using a single adversarial image, revealing agent-specific vulnerabilities.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2402.08567",
    "doi_url": "https://doi.org/10.48550/arXiv.2402.08567",
    "arxiv_id": "2402.08567",
    "arxiv_url": "https://arxiv.org/abs/2402.08567",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "artprompt",
    "filename": "artprompt.pdf",
    "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
    "authors": ["Fengqing Jiang", "Zhangchen Xu", "Luyao Niu", "Zhen Xiang", "Bhaskar Ramasubramanian", "Bo Li", "R. Poovendran"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2402.11753",
    "raw_path": "data/raw/artprompt.pdf",
    "processed_path": "data/processed/artprompt.json",
    "tags": ["ascii-art", "jailbreak", "vision-language", "safety-filter-bypass"],
    "relevance_note": "Introduces ASCII art-based jailbreak technique that exploits vision-language model weaknesses to bypass text-based safety filters.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2402.11753",
    "doi_url": "https://doi.org/10.48550/arXiv.2402.11753",
    "arxiv_id": "2402.11753",
    "arxiv_url": "https://arxiv.org/abs/2402.11753",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "beavertails",
    "filename": "beavertails.pdf",
    "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
    "authors": ["Jiaming Ji", "Mickel Liu", "Juntao Dai", "Xuehai Pan", "Chi Zhang", "Ce Bian", "Ruiyang Sun", "Yizhou Wang", "Yaodong Yang"],
    "year": 2023,
    "source_type": "JournalArticle",
    "venue": "Neural Information Processing Systems",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2307.04657",
    "raw_path": "data/raw/beavertails.pdf",
    "processed_path": "data/processed/beavertails.json",
    "tags": ["safety-alignment", "human-preference", "dataset", "RLHF"],
    "relevance_note": "Human-preference dataset for safety alignment that reveals limitations in preference-based training approaches and evaluation metrics.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2307.04657",
    "doi_url": "https://doi.org/10.48550/arXiv.2307.04657",
    "arxiv_id": "2307.04657",
    "arxiv_url": "https://arxiv.org/abs/2307.04657",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "crescendo",
    "filename": "crescendo.pdf",
    "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
    "authors": ["M. Russinovich", "Ahmed Salem", "Ronen Eldan"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "USENIX Security Symposium",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2404.01833",
    "raw_path": "data/raw/crescendo.pdf",
    "processed_path": "data/processed/crescendo.json",
    "tags": ["multi-turn", "jailbreak", "escalation", "conversation-attack"],
    "relevance_note": "Multi-turn jailbreak attack that gradually escalates harmful requests across conversation turns, exploiting context-dependent safety failures.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2404.01833",
    "doi_url": "https://doi.org/10.48550/arXiv.2404.01833",
    "arxiv_id": "2404.01833",
    "arxiv_url": "https://arxiv.org/abs/2404.01833",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "fitd",
    "filename": "fitd.pdf",
    "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs",
    "authors": ["Zixuan Weng", "Xiaolong Jin", "Jinyuan Jia", "Xiangyu Zhang"],
    "year": 2025,
    "source_type": "JournalArticle",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2502.19820",
    "raw_path": "data/raw/fitd.pdf",
    "processed_path": "data/processed/fitd.json",
    "tags": ["multi-turn", "jailbreak", "psychological-persuasion", "incremental"],
    "relevance_note": "Multi-turn jailbreak using psychological persuasion techniques that exploit conversational context to incrementally bypass safety guardrails.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2502.19820",
    "doi_url": "https://doi.org/10.48550/arXiv.2502.19820",
    "arxiv_id": "2502.19820",
    "arxiv_url": "https://arxiv.org/abs/2502.19820",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "frontier_ai_risk_management",
    "filename": "frontier ai risk management.pdf",
    "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management",
    "authors": ["Sim\u00e9on Campos", "Henry Papadatos", "Fabien Roger", "Chlo'e Touzet", "Otter Quarks", "Malcolm Murray"],
    "year": 2025,
    "source_type": "JournalArticle",
    "venue": "arXiv.org",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2502.06656",
    "raw_path": "data/raw/frontier ai risk management.pdf",
    "processed_path": "data/processed/frontier_ai_risk_management.json",
    "tags": ["risk-management", "frontier-AI", "safety-practices", "governance"],
    "relevance_note": "Framework bridging traditional risk management with frontier AI systems, highlighting gaps between current safety practices and evaluation needs.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2502.06656",
    "doi_url": "https://doi.org/10.48550/arXiv.2502.06656",
    "arxiv_id": "2502.06656",
    "arxiv_url": "https://arxiv.org/abs/2502.06656",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "goat",
    "filename": "goat.pdf",
    "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "authors": ["Maya Pavlova", "Erik Brinkman", "Krithika Iyer", "V\u00edtor Albiero", "Joanna Bitton", "Hailey Nguyen", "Joe Li", "Cristian Canton-Ferrer", "Ivan Evtimov", "Aaron Grattafiori"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "arXiv.org",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2410.01606",
    "raw_path": "data/raw/goat.pdf",
    "processed_path": "data/processed/goat.json",
    "tags": ["red-teaming", "automated", "generative-agent", "offensive-testing"],
    "relevance_note": "Automated red teaming methodology using generative agents to systematically discover jailbreak vulnerabilities and safety weaknesses.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2410.01606",
    "doi_url": "https://doi.org/10.48550/arXiv.2410.01606",
    "arxiv_id": "2410.01606",
    "arxiv_url": "https://arxiv.org/abs/2410.01606",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "harmbench",
    "filename": "harmbench.pdf",
    "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "authors": ["Mantas Mazeika", "Long Phan", "Xuwang Yin", "Andy Zou", "Zifan Wang", "Norman Mu", "Elham Sakhaee", "Nathaniel Li", "Steven Basart", "Bo Li", "David Forsyth", "Dan Hendrycks"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "International Conference on Machine Learning",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2402.04249",
    "raw_path": "data/raw/harmbench.pdf",
    "processed_path": "data/processed/harmbench.json",
    "tags": ["benchmark", "red-teaming", "evaluation-framework", "refusal"],
    "relevance_note": "Standardized evaluation framework for automated red teaming and refusal testing, addressing limitations in current safety benchmarking approaches.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2402.04249",
    "doi_url": "https://doi.org/10.48550/arXiv.2402.04249",
    "arxiv_id": "2402.04249",
    "arxiv_url": "https://arxiv.org/abs/2402.04249",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "how_do_llms_fail",
    "filename": "how do llms fail.pdf",
    "title": "Jailbroken: How Does LLM Safety Training Fail?",
    "authors": ["Alexander Wei", "Nika Haghtalab", "J. Steinhardt"],
    "year": 2023,
    "source_type": "JournalArticle",
    "venue": "Neural Information Processing Systems",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2307.02483",
    "raw_path": "data/raw/how do llms fail.pdf",
    "processed_path": "data/processed/how_do_llms_fail.json",
    "tags": ["safety-training", "failure-modes", "competing-objectives", "mismatched-generalization"],
    "relevance_note": "Analyzes fundamental mechanisms of how safety training fails, identifying competing objectives and mismatched generalization as key architectural flaws.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2307.02483",
    "doi_url": "https://doi.org/10.48550/arXiv.2307.02483",
    "arxiv_id": "2307.02483",
    "arxiv_url": "https://arxiv.org/abs/2307.02483",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "pair",
    "filename": "pair.pdf",
    "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
    "authors": ["Patrick Chao", "Alexander Robey", "Edgar Dobriban", "Hamed Hassani", "George Pappas", "Eric Wong"],
    "year": 2023,
    "source_type": "JournalArticle",
    "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
    "url_or_doi": "https://doi.org/10.1109/SaTML64287.2025.00010",
    "raw_path": "data/raw/pair.pdf",
    "processed_path": "data/processed/pair.json",
    "tags": ["black-box", "jailbreak", "iterative-refinement", "attacker-LLM"],
    "relevance_note": "Black-box jailbreak technique using iterative prompt refinement with attacker LLM, demonstrating query-efficient adversarial methods.",
    "type": "JournalArticle",
    "doi": "10.1109/SaTML64287.2025.00010",
    "doi_url": "https://doi.org/10.1109/SaTML64287.2025.00010",
    "arxiv_id": "2310.08419",
    "arxiv_url": "https://arxiv.org/abs/2310.08419",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "safedecoding",
    "filename": "safedecoding.pdf",
    "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
    "authors": ["Zhangchen Xu", "Fengqing Jiang", "Luyao Niu", "Jinyuan Jia", "Bill Yuchen Lin", "R. Poovendran"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2402.08983",
    "raw_path": "data/raw/safedecoding.pdf",
    "processed_path": "data/processed/safedecoding.json",
    "tags": ["defense", "decoding", "inference-time", "safety-aware"],
    "relevance_note": "Defense mechanism using safety-aware decoding that reveals limitations of current input-filtering approaches and need for inference-time safeguards.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2402.08983",
    "doi_url": "https://doi.org/10.48550/arXiv.2402.08983",
    "arxiv_id": "2402.08983",
    "arxiv_url": "https://arxiv.org/abs/2402.08983",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "self_jailbreaking",
    "filename": "self jailbreaking.pdf",
    "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training",
    "authors": ["Zheng-Xin Yong", "Stephen H. Bach"],
    "year": 2025,
    "source_type": "JournalArticle",
    "venue": "arXiv.org",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2510.20956",
    "raw_path": "data/raw/self jailbreaking.pdf",
    "processed_path": "data/processed/self_jailbreaking.json",
    "tags": ["self-jailbreaking", "reasoning", "alignment-failure", "capability-safety-tension"],
    "relevance_note": "Demonstrates how reasoning training can inadvertently enable models to self-jailbreak, revealing tension between capability enhancement and safety alignment.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2510.20956",
    "doi_url": "https://doi.org/10.48550/arXiv.2510.20956",
    "arxiv_id": "2510.20956",
    "arxiv_url": "https://arxiv.org/abs/2510.20956",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "sorrybench",
    "filename": "sorrybench.pdf",
    "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors",
    "authors": ["Tinghao Xie", "Xiangyu Qi", "Yi Zeng", "Yangsibo Huang", "Udari Madhushani Sehwag", "Kaixuan Huang", "Luxi He", "Boyi Wei", "Dacheng Li", "Ying Sheng", "Ruoxi Jia", "Bo Li", "Kai Li", "Danqi Chen", "Peter Henderson", "Prateek Mittal"],
    "year": 2024,
    "source_type": "JournalArticle",
    "venue": "International Conference on Learning Representations",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2406.14598",
    "raw_path": "data/raw/sorrybench.pdf",
    "processed_path": "data/processed/sorrybench.json",
    "tags": ["benchmark", "refusal-evaluation", "safety-inconsistencies", "systematic"],
    "relevance_note": "Systematic evaluation of LLM safety refusal behaviors, exposing inconsistencies and limitations in current safety evaluation methodologies.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2406.14598",
    "doi_url": "https://doi.org/10.48550/arXiv.2406.14598",
    "arxiv_id": "2406.14598",
    "arxiv_url": "https://arxiv.org/abs/2406.14598",
    "status": "matched_by_arxiv"
  },
  {
    "source_id": "xteaming",
    "filename": "xteaming.pdf",
    "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
    "authors": ["Salman Rahman", "Liwei Jiang", "James Shiffer", "Genglin Liu", "Sheriff M. Issaka", "Md. Rizwan Parvez", "Hamid Palangi", "Kai-Wei Chang", "Yejin Choi", "Saadia Gabriel"],
    "year": 2025,
    "source_type": "JournalArticle",
    "venue": "arXiv.org",
    "url_or_doi": "https://doi.org/10.48550/arXiv.2504.13203",
    "raw_path": "data/raw/xteaming.pdf",
    "processed_path": "data/processed/xteaming.json",
    "tags": ["multi-agent", "multi-turn", "jailbreak", "adaptive-defense"],
    "relevance_note": "Multi-agent framework for both jailbreak attacks and defenses, demonstrating adaptive adversarial strategies in multi-turn conversations.",
    "type": "JournalArticle",
    "doi": "10.48550/arXiv.2504.13203",
    "doi_url": "https://doi.org/10.48550/arXiv.2504.13203",
    "arxiv_id": "2504.13203",
    "arxiv_url": "https://arxiv.org/abs/2504.13203",
    "status": "matched_by_arxiv"
  }
]
