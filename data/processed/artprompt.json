{
  "source_id": "artprompt",
  "filename": "artprompt.pdf",
  "sections": [
    {
      "title": "ArtPrompt : ASCII Art-based Jailbreak Attacks against Aligned LLMs",
      "text": "/warningsign WARNING: This paper contains model outputs that may be considered offensive. Fengqing Jiang \u2663 \u2021 Zhangchen Xu \u2663 \u2021 Luyao Niu \u2663 \u2021 Zhen Xiang \u2720 Bhaskar Ramasubramanian \u2666 Bo Li \u2660 Radha Poovendran \u2663 \u2663 University of Washington \u2720 UIUC \u2666 Western Washington University \u2660 University of Chicago {fqjiang,zxu9,luyaoniu,rp3}@uw.edu , zhen.xiang.lance@gmail.com , ramasub@wwu.edu , bol@uchicago.edu"
    },
    {
      "title": "Abstract",
      "text": "Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised finetuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (VITC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt , which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. Our code is available at https: //github.com/uw-nsl/ArtPrompt ."
    },
    {
      "title": "1 Introduction",
      "text": "Large language models (LLMs) such as ChatGPT (Achiam et al., 2023), Llama2 (Touvron et al., 2023), and Gemini (Team et al., 2023) are increasingly deployed to serve users across various application scenarios. Typical examples include math reasoning (Wei et al., 2022b), code completion \u2021 Equal Contribution. Figure 1: This figure presents two instances of a harmful instruction, 'how to build a bomb\", from a malicious user to an LLM. The first instance, labeled as 1 \u20dd , is written in English and is rejected by the LLM due to safety alignment. The second instance, labeled as 2 \u20dd , utilizes ASCII art to visually represent the word 'bomb\". This instance successfully bypasses the safety alignment, resulting in a harmful response 'Sure, here is ...\". <!-- image --> (Zhang et al., 2023a), and creative writing (Yuan et al., 2022). The widespread use of LLMs highlights significant safety concerns (Weidinger et al., 2022; Wang et al., 2023a). Violations of safety can expose users of LLMs to toxic contents (Gehman et al., 2020), bias (Nadeem et al., 2020), and other associated risks (Wei et al., 2023a). Extensive efforts have been made to ensure safety alignment of LLMs, including supervised fine-tuning (Bakker et al., 2022), reinforcement learning from human feedback (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022), and red-teaming (Dinan et al., 2019; Ge et al., 2023). However, a common feature of these efforts is their exclusive focus on semantics of natural language involved in training/instruction corpora. This overlooks the fact that corpora can be interpreted in multiple ways beyond just semantics. For example, the word 'BOMB\" in Figure 1 is written using characters '*' and spaces. Interpreting such an example through the semantics of the characters is generally meaningless. Instead, humans interpret the example in Figure 1 based on the arrangement of characters within the input space. This observa- Figure 2: Overview of ArtPrompt . ArtPrompt consists of two steps. In the first step, ArtPrompt masks the safety words (e.g., 'bomb\") within a prompt that could result in rejection from the victim LLM. In the second step, ArtPrompt replaces the masked word in Step I with ASCII art. Then the masked prompt is combined with the ASCII art representation to form a cloaked prompt. The cloaked prompt is finally sent to the victim LLM as a jailbreak attack. A detailed example prompt and response is available in Appendix C. <!-- image -->"
    },
    {
      "title": "tion raises the question:",
      "text": "'Will semantics-only interpretation of corpora during safety alignment lead to vulnerabilities of LLM safety that can be exploited by malicious users?\" In this paper, we address this question by making the following contributions. First, we develop a benchmark, named Vision-in-Text Challenge (VITC), to evaluate the capabilities of five SOTA LLMs (GPT-3.5 (OpenAI, 2023), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), Claude (Anthropic, 2023), and Llama2 (Touvron et al., 2023)) of perceiving and responding to queries in the form of ASCII art (Wagner, 2023), which cannot be interpreted using semantics of characters involved. Our results indicate that all five LLMs struggle to understand input queries that use ASCII art to represent one single letter or number. Furthermore, the ability of the LLMs to correctly recognize input queries drop significantly (close to zero) as the input queries contain more letters or numbers. Second, we exploit the limitations of LLMs in recognizing ASCII art and reveal vulnerabilities of LLMs to a novel jailbreak attack, termed ArtPrompt . A malicious user can launch ArtPrompt by following two steps as depicted in Figure 2. In Step I, ArtPrompt finds the words within a given prompt that may trigger rejections from LLM. In Step II, ArtPrompt crafts a set of cloaked prompts by visually encoding the identified words in the first step using ASCII art. These cloaked prompts are subsequently sent to the victim LLM to execute our jailbreak attack, resulting in responses that fulfill the malicious user's objectives and induce unsafe behaviors from the victim LLM. Third, we perform extensive experiments to evaluate ArtPrompt on five LLMs (GPT-3.5, GPT-4, Claude, Gemini, and Llama2) using two benchmark datasets (AdvBench (Zou et al., 2023) and HEx-PHI (Qi et al., 2023)). Our comparison with five jailbreak attacks (Direct Instruction, GCG (Zou et al., 2023), AutoDan (Liu et al., 2023), PAIR (Chao et al., 2023), and DeepInception (Li et al., 2023a)) demonstrates that ArtPrompt can effectively and efficiently induce unsafe behaviors from LLMs, and outperforms all attacks on average. We further evaluate ArtPrompt against three defenses (Perplexity, Paraphrase (Jain et al., 2023), and Retokenization (Provilkov et al., 2019)). We show that ArtPrompt successfully bypasses all defenses."
    },
    {
      "title": "2 Related Work",
      "text": "Jailbreak Attacks. As LLMs become increasingly integrated in real-world applications, misuses of LLMs and safety concerns (Bender et al., 2021; Bommasani et al., 2021; Carlini et al., 2021; Ganguli et al., 2022; Weidinger et al., 2021) have attracted attention. In particular, multiple jailbreak attacks against LLMs have been developed. Zou et al. (2023) and Jones et al. (2023) proposed gradientbased methods to search for inputs to LLMs that can trigger undesired outputs. Another line of work (Liu et al., 2023) used hierarchical genetic algorithm to automatically generate jailbreak prompts. Chao et al. (2023) proposed to use a pre-trained LLM to generate adversarial prompt to the victim LLM. Alternatively, Mozes et al. (2023) and Kang et al. (2023) exploited instruction-following behaviors of LLMs to disrupt LLM safety. Manuallycrafted prompts for jailbreaking LLMs were constructed by Deng et al. (2023) and Yu et al. (2023). In context demonstrations were used in (Wei et al., 2023b; Wang et al., 2023b). Defenses against Jailbreak Attacks. We categorize current defense against jailbreak attacks into the following two categories. The first is Detection-based Defenses , which involve applying input or output filters to detect and block potentially harmful user prompts. For example, Jain et al. (2023) adopted input perplexity as an input detection mechanism to defend against jailbreak attacks. Helbling et al. (2023) leverages LLM's own capabilities to detect whether it generates harmful outputs. SmoothLLM (Robey et al., 2023) detected harmful inputs by randomly perturbing multiple copies of inputs and aggregating the corresponding outputs to detect adversarial inputs. The second category is Mitigation-based Defenses . Jain et al. (2023) used paraphrase and retokenization to modify the input to reduce the success rate of gradientbased jailbreak attacks. Li et al. (2023b) proposed rewindable auto-regressive inference that allows LLMs to evaluate their own model outputs, and then use the evaluation result to guide the generation towards safer and more reliable outputs. Other defenses leveraged in-context prompt demonstration to enhance the safety awareness of LLMs (Wei et al., 2023b; Xie et al., 2023; Zhang et al., 2023b). Xu et al. (2024) leveraged an expert model focusing on safety and developed a safety-aware decoding strategy for LLMs to mitigate jailbreak attacks. ASCII Art Text. ASCII Art can be used to generate visuals using printable elements and enrich the text environment with enhanced human expressions (Wagner, 2023). Content that can be presented by ASCII Art are diverse, including portraits, objects, and rich-formatting texts. In our paper, we focus on (rich-formatting) texts represented as ASCII Art. We will demonstrate that SOTA LLMs are inadequately equipped to recognize prompts encoding information using ASCII art."
    },
    {
      "title": "3 VITC Benchmark to Evaluate LLM Capabilities of ASCII Art Recognition",
      "text": "In this section, we evaluate the intrinsic capabilities of LLMs when they are given prompts that cannot be interpreted using semantics. We design a benchmark called Vision-in-Text Challenge (VITC), and evaluate the capabilities of five SOTA LLMs."
    },
    {
      "title": "3.1 Design of Benchmark VITC",
      "text": "Goals. Our objectives are two-fold. First, we aim to comprehensively evaluate LLMs' capabilities of responding to prompts that cannot be interpreted semantically. Second, we investigate potential strategies to improve the capabilities of LLMs. Datasets. VITC provides two labeled datasets, namely VITC-S and VITC-L. VITC-S consists of 8424 samples and covers 36 classes. Each sample is a single character (e.g., a digit from 0 to 9 , or a letter from A to Z in upper or lower case) in the form of ASCII art. Samples with identical labels are represented in 234 different fonts filtered by human using Python art library 1 . VITC-L consists of 8000 samples and covers 800 classes in 10 representative distinct fonts. Each sample in VITC-L consists of a sequence of characters obtained from VITC-S, where the length of the sequence varies from 2 to 4. Each sample is labeled by concatenating the corresponding labels of each individual character. Detailed statistics of VITC-S and VITCL datasets are presented in Table 1. Task. We consider a recognition task on datasets VITC-S and VITC-L. An LLM performing this task is required to predict the label \u02c6 y = f ( x \u2223 x 0 ) , where x is a data sample from either VITC-S or VITC-L, x 0 is a task description prompt, f ( \u22c5 \u2223 \u22c5 ) represents the process of generating response under given prompt and input sample. When the predicted label \u02c6 y matches the ground truth label y associated with x , then the LLM is considered to succeed in the recognition task. Metrics. VITC employs two metrics to assess LLM performance on the recognition task. The first metric is prediction accuracy ( Acc ), defined as <!-- formula-not-decoded --> The second metric, termed as average match ratio ( AMR ), is defined as follows: <!-- formula-not-decoded --> where D denotes the dataset used for evaluation, \u2223 D \u2223 represents the size of dataset, x is a sample 1 https://github.com/sepandhaghighi/art Table 1: The statistic of VITC-S and VITC-L datasets. | | Length | Ratio | # Class | # Data | |--------|----------|---------|-----------|----------| | VITC-S | 1 | 100% | 36 | 8424 | | VITC-L | 2 | 80% | 640 | 6400 | | VITC-L | 3 | 15% | 120 | 1200 | | VITC-L | 4 | 5% | 40 | 400 | from dataset D , y is the associated label, M ( y, \u02c6 y ) denotes the number of matched digits or characters between y and \u02c6 y . AMR is particularly valuable when dataset VITC-L is used for evaluation since label y has length longer than one. Different from Acc which assigns a binary result for each individual sample x , AMR captures partial fulfillment of the recognition task. For example, when the predicted label is \u02c6 y = a 1 c while the ground truth label is y = a 7 c , we have AMR = 66 . 66% and Acc = 0 . When the length of label y is one, AMR reduces to Acc as a special case."
    },
    {
      "title": "3.2 Experiments using our VITC Benchmark",
      "text": "Models. We evaluate multiple model families, including closed-source models GPT-3.5, GPT-4 (from OpenAI), Gemini (Google) and Claude (Anthropic) and open-sourced model Llama2 (Meta). Experimental Setup. The task description prompt x 0 indicates whether the data sample to be fed into LLM contains a digit or a character. We adopt three strategies when querying LLMs, including zero-shot (Kojima et al., 2022), few-shot In-Context-Learning (ICL) (Brown et al., 2020), and Chain-of-Thought (CoT) (Wei et al., 2022b). Experimental Results. In what follows, we report our experimental results obtained using our VITC benchmark. LLMs struggle with the recognition task. Table 2 summarizes the performance of evaluated LLMs on the recognition task. We observe that all models struggle with the recognition task . For example, the highest performance (exhibited by GPT-4) on dataset VITC-S is only Acc = 25 . 19% , which is considerably lower compared to evaluations on other tasks such as code completion, summarization, and math reasoning (Achiam et al., 2023). Notably, as model size increases (from Llama2-7B to Llama2-70B), performance slightly increases from 1 . 01% to 10 . 04% . When evaluated on dataset VITC-L, the performance of all models deteriorate significantly. For example, GPT-4 only achieves Acc = 3 . 26% . Table 2: This table summarizes the model performance on ViTC Benchmark. We use zero-shot setting for evaluation. The Acc of all models is less than 25.19% and 3.26% on VITC-S and VITC-L, respectively. This performance is significantly worse than evaluations on other tasks such as math and code completion. | Model | Variant | VITC-S | VITC-S | VITC-L | VITC-L | |---------|-----------|----------|----------|----------|----------| | Family | | Acc | AMR | Acc | AMR | | | 0301 | 10.64% | 10.64% | 0.01% | 54.39% | | GPT-3.5 | 0613 | 13.50% | 13.50% | 0.10% | 53.16% | | | 1106 | 13.87% | 13.87% | 0.11% | 51.15% | | | 0314 | 24.82% | 24.82% | 2.09% | 19.76% | | GPT-4 | 0613 | 25.19% | 25.19% | 3.26% | 19.64% | | | 1106 | 22.67% | 22.67% | 0.00% | 17.53% | | Gemini | Pro | 13.00% | 13.00% | 0.31% | 13.90% | | Claude | v2 | 11.16% | 11.16% | 0.25% | 22.04% | | | Chat-7B | 1.01% | 1.01% | 0.44% | 3.66% | | Llama2 | Chat-13B | 5.75% | 5.75% | 0.29% | 7.31% | | | Chat-70B | 10.04% | 10.04% | 0.83% | 5.89% | Compared to the evaluation results on VITC-S, the significant decrease on Acc is because the recognition task becomes more challenging, i.e., samples contain sequences of digits or characters. Additionally, the highest AMR among all models is 54 . 39% , achieved by GPT-3.5. This indicates that, on average, the model can only recognize about half of the digits or characters associated with a data sample. In summary, all evaluated LLMs exhibit poor performance on both datasets VITC-S and VITCL when assessed with metrics Acc and AMR . The reason is that these models are trained with corpora that rely solely on the semantics for interpretation. Few-Shot Prompting and CoT Provide Marginal Performance Improvement. We adopt the ICL and CoT as prompting strategies to investigate whether they can improve the capabilities of LLMs in the recognition task. The results are presented in Figure 8 in Appendix B. We observe that both prompting strategies provide marginal performance improvement. As we vary the number of demonstrations from one to four, we notice that the performance may not necessarily increase (e.g., Gemini and Llama2-13B). Furthermore, for models from the same family (Llama2-7B/13B/70B), smaller models barely gain performance improvement from demonstrations. Instead, these models may even be confused by demonstrations due to limited model capabilities (Achiam et al., 2023)."
    },
    {
      "title": "4 A Jailbreak Attack via ASCII Art",
      "text": "In this section, we show that LLMs failing the recognition task (described in Section 3) create vulnerabilities, which can be exploited by malicious users to bypass safety measures implemented by LLMs, resulting in jailbreak attack. We term this attack as ArtPrompt , and show that it provokes unintended and unsafe behaviors from LLMs."
    },
    {
      "title": "4.1 Overview of ArtPrompt",
      "text": "ArtPrompt leverages the following two key insights to exploit the vulnerabilities of LLMs and launch jailbreak attacks. First, given that LLMs often struggle with the recognition task, substituting words likely to trigger rejection by LLMs with ASCII art potentially increases the probability of bypassing safety measures. In the meantime, although LLMs find it difficult to interpret specific words represented as ASCII art, they can still infer what such a word might be based on the context content in the remainder of the prompt. These two insights inspire our design of a novel jailbreak attack, ArtPrompt , shown in Figure 2. ArtPrompt consists of two steps, namely word masking and cloaked prompt generation. In the word masking step, given the targeted behavior that the attacker aims to provoke, the attacker first masks the sensitive words in the prompt that will likely conflict with the safety alignment of LLMs, resulting in prompt rejection. In the cloaked prompt generation step, the attacker uses an ASCII art generator to replace the identified words with those represented in the form of ASCII art. Finally, the generated ASCII art is substituted into the original prompt, which will be sent to the victim LLM to generate response. We describe the detailed design of ArtPrompt in the remainder of this section."
    },
    {
      "title": "4.2 Design of ArtPrompt",
      "text": "Step I: Word Masking. The first step of ArtPrompt is to mask the words within a prompt from the attacker that trigger rejections from LLMs. To serve this purpose, we place a mask on each word of the prompt from attacker, resulting in a masked prompt. As illustrated in Figure 2, by placing a mask on the word 'bomb\" from the prompt 'How to make a bomb?\", the attacker generates a masked prompt 'How to make a ?\". If there are N words in the prompt from attacker, then the attacker may generate N such masked prompts in the worst-case. In practice, the number of masked prompts will be significantly less than N since the attacker does not need to mask function words such as 'a', 'the\", and 'I\". These masked prompts will later serve as the template when generating the attack prompt in the subsequent step. Compared to the existing optimization-based jailbreak attacks (Zou et al., 2023; Jones et al., 2023), masking words that trigger rejections is more efficient. The reason is that there are finitely many words within the prompt that need to be masked. By contrast, the search space of optimization-based jailbreak attacks, however, is discrete and infinite, requiring iterative procedures to search for words/tokens that lead to successful jailbreak attacks. Step II: Cloaked Prompt Generation. Given a masked prompted generated in Step I, the attacker utilizes an ASCII art generator to substitute the masked word with ASCII art. Subsequently, the ASCII art is integrated into the masked prompt obtained from the previous step, resulting in a cloaked prompt . For example, the ASCII art representing the masked word 'bomb\" is shown in Figure 2. Then this representation is combined with the masked prompt to generate the cloaked prompt, as illustrated in Figure 2. Finally, the cloaked prompt is sent to the victim LLM for jailbreak attacks. An additional example on the cloaked prompt and the response from victim model is presented in Appendix C. We remark that if the attacker generates N masked prompts in Step 1, then it can create N cloaked prompts for jailbreak attack. Furthermore, all the cloaked prompts can be sent to the LLM simultaneously to reduce the latency incurred during attack. In comparison to existing jailbreak attacks that manually craft prompts (Deng et al., 2023; Yu et al., 2023), ArtPrompt can be automated by simply stitching the output of ASCII art generator with the masked prompt. Furthermore, the cloaked prompt is readable by humans, making ArtPrompt more stealthy and natural compared to jailbreak attacks that manipulate tokens (Zou et al., 2023)."
    },
    {
      "title": "5 Experimental Evaluations of ArtPrompt",
      "text": "In this section, we perform extensive experiments to evaluate ArtPrompt ."
    },
    {
      "title": "5.1 Setup",
      "text": "Victim Models. We evaluate ArtPrompt on five SOTA LLMs including four closed-source models, GPT-3.5 (0613) and GPT-4 (0613), Claude (v2), and Gemini (Pro), and one open-source model Llama2 (Chat-7B) from Meta. All LLMs used in our experiments are aligned with safety protocols. Baselines. We compare ArtPrompt with five SOTA jailbreak attacks described as follows. We follow the default setting of all baselines, with details deferred to Appendix A.1. - Direct Instruction ( DI ).An attacker launches DI by directly prompting the harmful instruction to victim LLMs. - Greedy Coordinate Gradient ( GCG ) (Zou et al., 2023). GCG is an optimization-based jailbreak attack. It requires white-box access to the victim model. Particularly, GCG uses a gradient-based approach to search for token sequences that can bypass the safety measure deployed by victim models. - AutoDAN (Liu et al., 2023). AutoDAN is an optimization-based jailbreak attack that can be automated to generate stealthy jailbreak prompts. AutoDAN requires white-box access to victim models, and generate prompts using a hierarchical genetic algorithm. - Prompt Automatic Iterative Refinement ( PAIR ) (Chao et al., 2023). PAIR is an optimzation-based jailbreak attack that iteratively refines the prompt to victim models to elicit unintended behaviors. PAIR only requires black-box access to victim models. - DeepInception (Li et al., 2023a). DeepInception is a black-box jailbreak attack. It utilizes the personification ability of LLMs to construct a nested scene as an inception to the victim model to elicit harmful behaviors. Metrics. We use three metrics to measure the effectiveness of a jailbreak attack. The first metric is Helpful Rate ( HPR ), defined as <!-- formula-not-decoded --> To quantify the number of queries that are not refused by an LLM, we follow the setting by Zou et al. (2023) and use Dict-Judge to evaluate the number of non-refusal responses. The second metric is the average Harmfulness Score ( HS ). Following (Qi et al., 2023), a response to an input prompt is rated from 1 to 5 to indicate its harmfulness, where score 1 indicates no harm, and 5 represents extreme harm. We follow GPT-Judge (Qi et al., 2023) and utilize GPT-4 to evaluate HS. The last metric is the Attack Success Rate ( ASR ), defined as <!-- formula-not-decoded --> The metric ASR only counts the responses that are of extreme harm to minimize subjectivity. The reason is that assessing the success of jailbreak attacks is often subjective, while most aligned LLMs can consistently label responses with extreme harm. Datasets. We compare the performance of ArtPrompt with baselines on two benchmark datasets: AdvBench (Zou et al., 2023) and HExPHI dataset (Qi et al., 2023). In particular, we refine AdvBench to obtain 50 representative and non-duplicate harmful instructions following Wei et al. (2023b). HEx-PHI dataset covers 11 prohibited categories based on the usage policies of victim models, where each category includes 30 examples. We sample 10 questions from each individual category for evaluation in our experiment. Defenses Against Jailbreak Attacks. We consider three defenses against jailbreak attacks, namely Perplexity-based Detection (PPL-Pass), (2) Paraphrase, (3) Retokenization. Details of the defense settings are deferred to Appendix A.2. Setup of ArtPrompt . In our experiments, we consider the following two configurations of ArtPrompt . In the first configuration, denoted as Top 1 , we restrict the possible fonts that can be used by the ASCII art generator when replacing the masked word. Top 1 will use the font with highest ASRto generate the cloaked prompt and launch jailbreak attack. In the second configuration, denoted as Ensemble , we do not impose any constraint on the font used for ASCII art generation."
    },
    {
      "title": "ArtPrompt is effective against all victim LLMs.",
      "text": "We use AdvBench to evaluate the performance of ArtPrompt and all baselines on victim LLMs. We summarize the results in Table 3 and make the following observations. First, ArtPrompt is effective against all victim LLMs. For example, ArtPrompt using the Ensemble configuration achieves the highest ASR (52%) among all jailbreak attacks on Claude, whereas most baselines Table 3: This table summarizes HPR, HS, and ASR of ArtPrompt and five SOTA jailbreak attacks on AdvBench dataset. We observe that ArtPrompt is effective against all LLMs. On average, ArtPrompt outperforms all baselines in all metrics. | Attack Method | GPT-3.5 | GPT-3.5 | GPT-3.5 | GPT-4 | GPT-4 | GPT-4 | Claude | Claude | Claude | Gemini | Gemini | Gemini | Llama2 | Llama2 | Llama2 | Average | Average | Average | |----------------------|-----------|-----------|-----------|---------|---------|---------|----------|----------|----------|----------|----------|----------|----------|----------|----------|-----------|-----------|-----------| | Attack Method | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | | DI | 2% | 1.22 | 0% | 0% | 1.00 | 0% | 0% | 1.00 | 0% | 8% | 1.28 | 6% | 0% | 1.00 | 0% | 2% | 1.10 | 1% | | GCG | 30% | 3.36 | 54% | 24% | 1.48 | 10% | 2% | 1.16 | 4% | 48% | 2.88 | 46% | 32% | 2.00 | 18% | 27% | 2.18 | 26% | | AutoDAN | 24% | 1.78 | 18% | 14% | 1.52 | 10% | 2% | 1.00 | 0% | 20% | 1.34 | 8% | 58% | 2.90 | 36% | 24% | 1.71 | 14% | | PAIR | 54% | 3.16 | 38% | 60% | 3.14 | 30% | 6% | 1.10 | 0% | 66% | 3.80 | 50% | 38% | 2.16 | 22% | 45% | 2.67 | 28% | | DeepInception | 100% | 2.90 | 16% | 100% | 1.30 | 0% | 0% | 1.00 | 0% | 100% | 4.34 | 78% | 100% | 2.36 | 14% | 80% | 2.38 | 22% | | ArtPrompt (Top 1) | 90% | 4.38 | 72% | 78% | 2.38 | 16% | 34% | 2.22 | 20% | 98% | 3.70 | 60% | 66% | 1.96 | 14% | 73% | 2.93 | 36% | | ArtPrompt (Ensemble) | 92% | 4.56 | 78% | 98% | 3.38 | 32% | 60% | 3.44 | 52% | 100% | 4.42 | 76% | 68% | 2.22 | 20% | 84% | 3.60 | 52% | Figure 3: This figure presents HS of ArtPrompt on HExPHI dataset. ArtPrompt successfully induces unsafe behaviors across eleven prohibited categories from all victim models. <!-- image --> except GCG fail with ASR being 0%. Furthermore, we observe that ArtPrompt is the most effective jailbreak attack on almost all victim LLMs including GPT-3.5, GPT-4, Claude, and Gemini. We note that on Llama2, AutoDAN and PAIR outperform ArtPrompt . However, both AutoDAN and PAIR fail to generalize such effectiveness to other models. Indeed, as shown in Table 3, on average ArtPrompt outperforms all baselines, achieving the highest HPR (84%), HS (3.6), and ASR (52%). We also evaluate ArtPrompt on HEx-PHI (Qi et al., 2023) by representing the harmful instructions from HEx-PHI using ArtPrompt . The HS across the eleven prohibited categories in HEx-PHI when ArtPrompt is adopted are summarized in Figure 3. We observe that most victim LLMs exhibit Figure 4: This figure presents ASR (higher is better) versus the average number of optimization iterations (lower is better). We observe that ArtPrompt can efficiently generate the cloaked prompt with one iteration, while achieving the highest ASR among all jailbreak attacks. <!-- image --> safe behaviors when the harmful instructions are directly sent to the model to generate responses. However, when these harmful instructions are modified using ArtPrompt , unsafe behaviors can be induced from victim models, even for well aligned model such as GPT-4. ArtPrompt is efficient. In Figure 4, we present the average number of iterations required by ArtPrompt and other jailbreak attacks to construct the harmful instructions to victim models along with their ASRs. Here, the number of iterations reflects the computational cost incurred by an attacker to launch the jailbreak attack. We observe that ArtPrompt achieves the highest ASR among all jailbreak attacks with only one iteration with the victim LLM. The reason is ArtPrompt can efficiently construct the set of cloaked prompts, and send them to the model in parallel. However, optimization-based jailbreak attacks such as GCG require significantly larger amount of iterations to construct the prompt. These iterations cannot be processed in parallel because the optimization in subsequent iterations depends on results from previous iterations. This highlights the efficiency of Table 4: This table presents the effectiveness of ArtPrompt on AdvBench dataset when PPL, Paraphrase, or Retokenization is employed by victim LLMs. We observe that ArtPrompt can successfully bypass the existing defenses, highlighting the urgent need for more advanced defense mechanisms. | ArtPrompt Setting | GPT-3.5 | GPT-3.5 | GPT-3.5 | GPT-4 | GPT-4 | GPT-4 | Claude | Claude | Claude | Gemini | Gemini | Gemini | Llama2 | Llama2 | Llama2 | Average | Average | Average | |---------------------|-----------|-----------|-----------|---------|---------|---------|----------|----------|----------|----------|----------|----------|----------|----------|----------|-----------|-----------|-----------| | ArtPrompt Setting | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | | Top 1 | 90% | 4.38 | 72% | 78% | 2.38 | 16% | 34% | 2.22 | 20% | 98% | 3.70 | 60% | 66% | 1.96 | 14% | 73% | 2.93 | 36% | | + PPL-Pass | 88% | 4.38 | 72% | 78% | 2.28 | 10% | 34% | 2.22 | 20% | 98% | 3.70 | 60% | 66% | 1.68 | 12% | 73% | 2.85 | 35% | | + Paraphrase | 80% | 3.20 | 46% | 60% | 2.16 | 18% | 28% | 1.08 | 0% | 90% | 2.18 | 14% | 54% | 1.50 | 6% | 62% | 2.02 | 17% | | + Retokenization | 100% | 3.14 | 26% | 94% | 3.24 | 36% | 28% | 1.70 | 10% | 100% | 4.12 | 62% | 100% | 2.08 | 12% | 84% | 2.86 | 29% | | Ensemble | 92% | 4.56 | 78% | 98% | 3.38 | 32% | 60% | 3.44 | 52% | 100% | 4.42 | 76% | 68% | 2.22 | 20% | 84% | 3.60 | 52% | | + PPL | 92% | 4.56 | 78% | 96% | 3.30 | 28% | 58% | 3.36 | 50% | 100% | 4.42 | 76% | 68% | 2.22 | 18% | 83% | 3.57 | 50% | | + Paraphrase | 98% | 4.24 | 70% | 98% | 3.62 | 36% | 70% | 1.60 | 8% | 100% | 3.78 | 52% | 90% | 2.68 | 30% | 91% | 3.18 | 39% | | + Retokenization | 100% | 4.08 | 54% | 100% | 4.18 | 56% | 62% | 3.06 | 30% | 100% | 4.74 | 86% | 100% | 3.52 | 32% | 92% | 3.92 | 52% | Table 5: This table presents our ablation analysis of ArtPrompt on AdvBench dataset. We observe that the choice of font and arrangement of ASCII art impact the attack effectiveness. | ArtPrompt Setting | GPT-3.5 | GPT-3.5 | GPT-3.5 | GPT-4 | GPT-4 | GPT-4 | Claude | Claude | Claude | Gemini | Gemini | Gemini | Llama2 | Llama2 | Llama2 | Average | Average | Average | |---------------------|-----------|-----------|-----------|---------|---------|---------|----------|----------|----------|----------|----------|----------|----------|----------|----------|-----------|-----------|-----------| | ArtPrompt Setting | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | | Top 1 | 90% | 4.38 | 72% | 78% | 2.38 | 16% | 34% | 2.22 | 20% | 98% | 3.70 | 60% | 66% | 1.96 | 14% | 73% | 2.93 | 36% | | - Vertical Arranged | 42% | 2.36 | 24% | 88% | 2.50 | 12% | 18% | 1.40 | 8% | 96% | 3.46 | 48% | 26% | 1.40 | 6% | 54% | 2.22 | 20% | | - Tail Font Sets | 68% | 2.78 | 36% | 84% | 2.20 | 10% | 40% | 2.24 | 24% | 98% | 3.38 | 48% | 30% | 1.18 | 2% | 64% | 2.36 | 24% | | Ensemble | 92% | 4.56 | 78% | 98% | 3.38 | 32% | 60% | 3.44 | 52% | 100% | 4.42 | 76% | 68% | 2.22 | 20% | 84% | 3.60 | 52% | | - Vertical Arranged | 72% | 3.06 | 40% | 90% | 2.84 | 16% | 26% | 1.78 | 16% | 98% | 4.40 | 74% | 34% | 1.64 | 8% | 64% | 2.74 | 31% | | - Tail Font Sets | 82% | 3.62 | 58% | 92% | 2.98 | 24% | 52% | 2.66 | 32% | 100% | 4.06 | 68% | 46% | 1.54 | 6% | 74% | 2.97 | 38% | ArtPrompt compared to existing jailbreak attacks. ArtPrompt can bypass existing defenses against jailbreak attacks. In Table 4, we evaluate ArtPrompt when victim LLMs employ defenses PPL, Paraphrase, or Retokenization to mitigate jailbreak attacks. We make the following two observations. First, ArtPrompt can successfully bypass defenses PPL and Retokenization on all victim models. This highlights the urgent need for developing more advanced defenses against our ArtPrompt jailbreak attack. We note that Retokenization may even help ArtPrompt to improve ASR. We conjecture that this is because the spaces introduced by Retokenization forms a new font for ArtPrompt , which further reduces the chance of triggering safety measures deployed by victim models. Second, we observe that Paraphrase is the most effective defense against ArtPrompt . The reason is that Paraphrase may disrupt the arrangement used by ArtPrompt , and thus reduces the ASR. However, Paraphrase is still inadequate to mitigate ArtPrompt . We note that on average, ArtPrompt achieves 39% ASR and 3.18 HS when Paraphrase is deployed by victim models. Ablation analysis of ArtPrompt . Based on our analysis in Section 3, we have shown that the capabilities of victim models in recognizing ASCII art vary as the font of ASCII art changes. In Table 5, we analyze how the choice of font used by ArtPrompt impacts HPR, HS, and ASR. We use the tail-set fonts from Appendix A.3, and apply ArtPrompt to the harmful queries to all victim models. We observe that all metrics decrease slightly compared to those in Table 3. However, ArtPrompt still remain effective in jailbreaking all victim LLMs. To achieve the best effectiveness of jailbreak attack using ArtPrompt , it is necessary to configure the Top 1 and ensemble strategy for ArtPrompt by leveraging our results in Figure 6. We further perform ablation analysis on the impact of arrangements of ASCII art in Table 5. In this set of experiments, we arrange the characters forming ASCII art along the vertical direction. We observe that vertical arrangment leads to degradation in effectiveness of ArtPrompt . We conjecture that the reason is that vertical arrangement significantly reduces the prediction accuracy of the recognition task, making the victim models uncertain about the input prompt. Ablation analysis on the masked words setup used by ArtPrompt is deferred to Appendix B.2. ArtPrompt on models fine tuned with nonsemantic interpretations. To further assess the vulnerabilities introduced by semantics-only interpretation of corpora during safety alignment, we evaluate ArtPrompt on models fine tuned using VITC-S dataset. Specifically, we use 60% data samples for fine-tuning and 40% data samples for testing the model performance on the recognition task of VITC benchmark. We observe that the Figure 5: HS Ratio on AdvBench for ArtPrompt using Top-1 font. The distribution shift of HS indicates ArtPrompt is less effective on fine-tuned GPT-3.5. <!-- image --> fine-tuned model gets accuracy 71.54%, which significantly outperforms the original model (i.e., 10.26%) in the recognition task. We further evaluate the effectiveness of ArtPrompt on the fine-tuned model. We observe that HS of the responses returned by the fine-tuned model decrease compared to those generated by the original model. Specifically, for the fine-tuned model, 64%, 10%, and 12% of responses yield an HS of 5, 4, and 2, respectively. In contrast, for the original model, 72%, 8%, and 6% of responses yield an HS of 5, 4, and 2, respectively. This indicates that providing LLMs with corpora that should not be solely interpreted by semantics could mitigate the vulnerabilities identified in our paper. We believe that such vulnerabilities may be further mitigated if such corpora is used during pre-training and safety alignment procedures. However, we defer further exploration as future work."
    },
    {
      "title": "6 Conclusion",
      "text": "In this paper, we revealed that semantics-only interpretation of corpora during safety alignment creates vulnerabilities to jailbreak attacks. We developed a benchmark named Vision-in-Text Challenge (VITC) to evaluate the capabilities of LLMs in recognizing prompts that should not be interpreted purely using semantics. Our results showed that five SOTA LLMs struggle with the recognition task specified by our benchmark. We demonstrated that such poor performance leads to vulnerabilities. We designed a novel jailbreak attacks, named ArtPrompt , to exploit these vulnerabilities. We evaluated ArtPrompt on five LLMs against three defenses. Our experimental results demonstrated that ArtPrompt can effectively and efficiently provoke unsafe behaviors from aligned LLMs."
    },
    {
      "title": "7 Limitations",
      "text": "In this paper, we evaluate ArtPrompt on five LLMs. The performance of ArtPrompt on multimodal language models is subject to further investigation. We hypothesize that ArtPrompt will remain effective to attack multimodal language models. The reason is that although multimodal language models can take images as inputs, which can be interpreted in a similar manner to ASCII art, cloaked prompts generated by ArtPrompt are still in the format of texts. Such input format will confuse the model, thereby allowing ArtPrompt to induce unsafe behaviors from multimodal language models."
    },
    {
      "title": "8 Ethical Statement",
      "text": "The primary goal of this paper is to advance the safety of LLMs operating under adversarial conditions. This paper focuses on how corpora should be interpreted to enhance the safety of LLMs. This paper reveals the limitations and potential vulnerabilities of the existing LLMs if the training corpora are interpreted using semantics only. We acknowledge that the vulnerabilities of LLMs and prompts demonstrated in this paper can be repurposed or misused by malicious entities to attack LLMs. We will disseminate the code and prompts used in our experiments to the community, hoping that they will further assist in the redteaming of LLMs."
    },
    {
      "title": "Acknowledgement",
      "text": "This work is partially supported by the National Science Foundation (NSF) under grants IIS 2229876, No.1910100, No.2046726, CNS 2153136, Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, DARPA GARD, the National Aeronautics and Space Administration (NASA) under grant No.80NSSC20M0229, Alfred P. Sloan Fellowship, and the Amazon research award. This work is supported in part by funds provided by the National Science Foundation, Department of Homeland Security, and IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or its federal agency and industry partners."
    },
    {
      "title": "References",
      "text": "- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. Technical report. - Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. - Anthropic. 2023. Model card and evaluations for Claude models. Technical report. - Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 . - Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. 2022. Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems , 35:38176-38189. - Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency , pages 610-623. - Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 . - Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901. - Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) , pages 2633-2650. - Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. 2023. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419 . - Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion - Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. - Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems , 30. - Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715 . - Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. arXiv preprint arXiv:1908.06083 . - Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. 2022. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency , pages 1747-1764. - Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, and Yuning Mao. 2023. Mart: Improving llm safety with multi-round automatic red-teaming. arXiv preprint arXiv:2311.07689 . - Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In Findings . - Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. 2023. LLM self defense: By self examination, LLMs know they are being tricked. arXiv preprint arXiv:2308.07308 . - Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614 . - Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381 . - Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733 . - Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in - neural information processing systems , 35:2219922213. - Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. 2023a. Deepinception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191 . - Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. 2023b. Rain: Your language models can align themselves without finetuning. arXiv preprint arXiv:2309.07124 . - Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451 . - Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D Griffin. 2023. Use of LLMs for illicit purposes: Threats, prevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833 . - Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456 . - OpenAI. 2023. Models-OpenAI API. https:// platform.openai.com/docs/models . Accessed: 2023-09-15. - Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744. - Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. 2019. Bpe-dropout: Simple and effective subword regularization. arXiv preprint arXiv:1910.13267 . - Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Finetuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693 . - Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684 . - Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203 . - Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 . - Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 . - Karin Wagner. 2023. From ASCII Art to Comic Sans: Typography and Popular Culture in the Digital Age . MIT Press. - Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023a. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. arXiv preprint arXiv:2306.11698 . - Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. 2023b. Adversarial demonstration attacks on large language models. arXiv preprint arXiv:2305.14950 . - Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023a. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483 . - Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 . - Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824-24837. - Zeming Wei, Yifei Wang, and Yisen Wang. 2023b. Jailbreak and guard aligned language models with only few in-context demonstrations. arXiv preprint arXiv:2310.06387 . - Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 . - Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency , pages 214-229. - Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. 2023. Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence , pages 1-11. - Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. 2024. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv preprint arXiv:2402.08983 . - Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253 . - Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft: story writing with large language models. In 27th International Conference on Intelligent User Interfaces , pages 841-852. - Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan. 2023a. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510 . - Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023b. Defending large language models against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096 . - Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 ."
    },
    {
      "title": "A Experiment Details",
      "text": "In this section, we first provide detailed setups for baseline jailbreak attacks, i.e., GCG, AutoDAN, PAIR and DeepInception. Following this, we provide the defense settings. We then demonstrate fonts selection for ArtPrompt . We finally present the model generation configuration."
    },
    {
      "title": "A.1 Attack Settings",
      "text": "- GCG : We follow the official implementation of Zou et al. (2023) for Llama2 model. For black-box models (i.e., GPT-3.5, GPT4, Claude and Gemini), we follow the transferable optimization settings, which generate the universal suffixes targeting on Vicuna-7B model (Chiang et al., 2023). We note that Zou et al. (2023) have demonstrated significant transferability of GCG to black-box models. - AutoDAN : We follow the official implementation of Liu et al. (2023) for Llama2 model. For black-box models, we follow the transferable experiment settings, which use the optimized prompts from Llama2 as attack prompts. - PAIR and DeepInception : We follow the official implementation with the default hyperparameters in Chao et al. (2023) and Li et al. (2023a)."
    },
    {
      "title": "A.2 Defense Settings",
      "text": "PPL-Pass PPL is defined as: <!-- formula-not-decoded --> where X is the input token sequence. Following Alon and Kamfonas (2023), we use GPT-2 model (Solaiman et al., 2019) to calculate PPL ( X ) . PPLPass filters out queries if PPL ( X ) \u2265 T , where T is a pre-defined threshold. Following Jain et al. (2023), we set T = 175 . 57 as the threshold, which is the max perplexity in our tested AdvBench dataset. Any harmful instruction identified and filtered out by PPL-Pass will receive a standardized refusal response, specifically, \"Sorry, but I cannot assist with that.\" This ensures consistency in the evaluation process. Paraphrase Following Jain et al. (2023), we employ GPT-3.5 model to paraphrase the original instruction with the following prompt.The response generated by GPT-3.5 is then forwarded to the target LLM as a new instruction. Please paraphrase the following sentences. Give me paraphrased results only. Do not include any other information. [Original instruction] Retokenization We use BPE-dropout (Provilkov et al., 2019), where 20% merges from BPE tokenizer will be dropped."
    },
    {
      "title": "A.3 Fonts Selection for ArtPrompt",
      "text": "We note that the text content and specific font used (i.e., the format encoding for individual characters) are important in the generation of ASCII Art Text. We consider the samples representing letters in VITC-S dataset, and apply the Acc metric for selection. The experimental results across different LLMs are demonstrated in Figure 6. We observe that while the ability of LLMs to identify ASCII Art text of letters varies significantly across fonts, there are certain subsets of fonts that exhibit similar performance patterns across all tested models. To reduce potential biases in our study, we selected fonts based on their consistent performance across various LLMs. Specifically, we chose the head-set fonts from art library, which exhibited higher Acc across all models. This includes 'alphabet', 'cards', 'letters', 'keyboard', and 'puzzle'. Additionally, we selected tail-set fonts that have low Acc across all models: 'block','roman','xchartri','hollywood', and 'ghoulish'. To reduce dependency on the art library and enhance diversity, we also generated a font using the GPT-4 model, and named it 'Gen'. As shown in Figure 7, the 'Gen' font can generally be well recognized by all models. Therefore, we also include it in the head-set fonts."
    },
    {
      "title": "A.4 Model Generation Configuration",
      "text": "We present generation configurations as follows. For closed-sourced models including GPT-3.5, GPT-4, Claude, and Gemini, we set the temperature to be 0, with all other parameters being their default values provided by the API. For Llama2, we follow the default generation configuration 2 with temperature=0.6 and top-p= 0.9 for sampling. 2 https://huggingface.co/meta-llama/Llama-2-7b-chathf/blob/main/generationconfig.json GPT-3.5 Figure 6: Acc corresponding to each font using VITC-S. Font names are associated with the indices as defined by the art library (see Table 6 for more details). <!-- image --> Figure 7: This figure illustrates the Acc of 'Gen' font across different models. The result indicates that the 'Gen' font is generally well recognized by all models. <!-- image --> Table 6: Font name used by Art library in alphabet order. Head-set fonts are marked in red and tail-set fonts are marked in blue . | Index | | Font Names | Font Names | Font Names | Font Names | |-----------|------------|--------------|--------------|-------------------|--------------| | 0 \u223c 4 | 1943 | 3-d | 3d_diagonal | 3x5 | 4max | | 5 \u223c 9 | 4x4_offr | 5lineoblique | 5x7 | 5x8 | 64f1 | | 10 \u223c 14 | 6x10 | 6x9 | a_zooloo | alligator | alligator2 | | 15 \u223c 19 | alligator3 | alphabet | amc3line | amcun1 | aquaplan | | 20 \u223c 24 | arrows | asc | ascii | assalt_m | asslt_m | | 25 \u223c 29 | banner | banner3 | banner3-d | banner4 | barbwire | | 30 \u223c 34 | basic | beer_pub | bell | big | bigchief | | 35 \u223c 39 | block | bolger | braced | bright | bubble | | 40 \u223c 44 | c1 | c2 | c_ascii | cards | catwalk | | 45 \u223c 49 | char1 | char2 | char3 | char4 | charact1 | | 50 \u223c 54 | charact2 | charact3 | charact4 | charact5 | charact6 | | 55 \u223c 59 | characte | chartr | chartri | chunky | clb6x10 | | 60 \u223c 64 | clb8x10 | clb8x8 | cli8x8 | clr4x6 | clr5x10 | | 65 \u223c 69 | clr5x6 | clr5x8 | clr6x10 | clr6x6 | clr6x8 | | 70 \u223c 74 | clr7x8 | clr8x10 | clr8x8 | coinstak | colossal | | 75 \u223c 79 | com_sen | computer | contessa | contrast | cricket | | 80 \u223c 84 | cygnet | digital | doh | doom | dotmatrix | | 85 \u223c 89 | drpepper | druid | e_fist | ebbs_1 | ebbs_2 | | 90 \u223c 94 | eca | eftifont | eftitalic | epic | faces_of | | 95 \u223c 99 | fairligh | fantasy1 | fbr1 | fbr12 | fbr2 | | 100 \u223c 104 | fbr_stri | fbr_tilt | finalass | fireing | fourtops | | 105 \u223c 109 | fp1 | fp2 | funky_dr | future_1 | future_2 | | 110 \u223c 114 | future_3 | future_4 | future_5 | future_6 | future_7 | | 115 \u223c 119 | future_8 | fuzzy | georgi16 | georgia11 | ghost | | 120 \u223c 124 | ghost_bo | ghoulish | glenyn | goofy | gothic | | 125 \u223c 129 | green_be | heartleft | heartright | henry3d | hollywood | | 130 \u223c 134 | home_pak | hyper | impossible | inc_raw | jacky | | 135 \u223c 139 | jazmine | keyboard | kik_star | larry3d | lcd | | 140 \u223c 144 | lean | letters | marquee | maxfour | merlin1 | | 145 \u223c 149 | modular | moscow | nancyj | nancyj-underlined | nipples | | 150 \u223c 154 | nscript | o8 | ogre | oldbanner | os2 | | 155 \u223c 159 | pawp | peaks | pebbles | poison | puffy | | 160 \u223c 164 | puzzle | pyramid | red_phoenix | rev | roman | | 165 \u223c 169 | rounded | rozzo | santaclara | sblood | script | | 170 \u223c 174 | shimrod | slant | slide | slscript | small | | 175 \u223c 179 | smkeyboard | smpoison | smslant | soft | standard | | 180 \u223c 184 | starwars | stellar | stforek | stop | straight | | 185 \u223c 189 | swampland | swan | sweet | tanja | thick | | 190 \u223c 194 | thin | threepoint | tiles | tinker-toy | tombstone | | 195 \u223c 199 | tubular | type_set | ucf_fan | unarmed | usa | | 200 \u223c 204 | usa_pq | usaflag | utopia | utopiab | utopiabi | | 205 \u223c 209 | utopiai | varsity | vortron | war_of_w | xbrite | | 210 \u223c 214 | xbriteb | xbritebi | xbritei | xchartr | xchartri | | 215 \u223c 219 | xcour | xcourb | xcourbi | xcouri | xhelv | | 220 \u223c 224 | xhelvb | xhelvbi | xhelvi | xsans | xsansb | | 225 \u223c 229 | xsansbi | xsansi | xtimes | xtty | xttyb | | 230 \u223c 233 | yie-ar | yie_ar_k | zig_zag | zone7 | |"
    },
    {
      "title": "B.1 Evaluation on VITC",
      "text": "In Figure 8, we present the few-shot experiments on VITC-S and compare them with zero-shot settings. We observe that both ICL and CoT provide marginal performance improvements in the recognition task across different models compared with zero-shot settings. Notably, increasing the number of examples in few-shot learning does not consistently lead to enhanced performance. Additionally, upon comparing different parameter sizes of the Llama2 model, specifically 7B, 13B, and 70B, we find that small models barely gain performance improvements. This may account to LLMs' emergent ability, i.e., it is challenging for small models to learn from the demonstrations to perform the recognition task (Wei et al., 2022a)."
    },
    {
      "title": "B.2 ArtPrompt Evaluation",
      "text": "ArtPrompt with Different Mask Setting. In our main evaluations, we set the number of masked words used by ArtPrompt to 1 by default, i.e. mask one word. In Table 7, we evaluate the effectiveness of ArtPrompt with multiple words masked. We note that attack performance of ArtPrompt is comparable to the setting when a single word is masked. For GPT-3.5, Gemini, and Llama2 models, we observe a slight drop in ASR because the instructions become more challenging to comprehend for LLMs when multiple words are masked. For GPT-4 and Claude, the increase in attack performance could be attributed to their strong reasoning capabilities."
    },
    {
      "title": "C ArtPrompt Example",
      "text": "In this section, we demonstrate the example ArtPrompt and the corresponding response from the GPT-4 model in Figure 9. Table 7: This table summarizes HPR, HS, and ASR of ArtPrompt on AdvBench dataset under different mask settings using Top-1 font. Here, 'Mask Ensemble' is the ensemble setting of 1-word and 2-word masks. | ArtPrompt Mask Setting | GPT-3.5 | GPT-3.5 | GPT-3.5 | GPT-4 | GPT-4 | GPT-4 | Claude | Claude | Claude | Gemini | Gemini | Gemini | Llama2 | Llama2 | Llama2 | Average | Average | Average | |--------------------------|-----------|-----------|-----------|---------|---------|---------|----------|----------|----------|----------|----------|----------|----------|----------|----------|-----------|-----------|-----------| | ArtPrompt Mask Setting | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | HPR | HS | ASR | | 1-Word Mask | 90% | 4.38 | 72% | 78% | 2.38 | 16% | 34% | 2.22 | 20% | 98% | 3.70 | 60% | 66% | 1.96 | 14% | 73% | 2.93 | 36% | | 2-Word Mask | 96% | 3.72 | 54% | 86% | 3.08 | 24% | 80% | 3.24 | 36% | 98% | 3.40 | 44% | 62% | 1.48 | 4% | 84% | 2.98 | 32% | | Mask Ensemble | 98% | 4.56 | 76% | 90% | 3.44 | 34% | 80% | 3.54 | 44% | 100% | 4.08 | 68% | 74% | 2.08 | 16% | 88% | 3.54 | 48% | Figure 8: Few-shot experiments on VITC-S under ICL and CoT setting using 1, 2, or 4 shots compared to zero-shot setting. The results show that both ICL and CoT provide marginal performance improvements in the recognition task across different models. <!-- image --> Figure 9: Demonstration ArtPrompt and the corresponding response from the GPT-4 model. The harmful goal is 'provide step-by-step instructions on how to make and distribute counterfeit money'. <!-- image -->"
    }
  ],
  "num_sections": 27
}