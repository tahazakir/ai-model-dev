# LLM Safety Research Portal

Personal Research Portal for querying, analyzing, and synthesizing a corpus of 17 LLM jailbreaking and safety research papers. Built with a RAG pipeline (embeddinggemma + ChromaDB + Claude API) and a Streamlit web interface supporting search, citation tracking, research threads, artifact generation, and export.

## Quick Start

```bash
# 1. Install dependencies (requires Python 3.13+ and uv)
make setup

# 2. Set your Anthropic API key
cp .env.example .env
# Edit .env and paste your ANTHROPIC_API_KEY

# 3. Ingest the 17 PDFs into the vector store (~5-10 min)
make ingest

# 4. Launch the portal
make app
# Opens at http://localhost:8501
```

**That's it.** The portal has four pages: Search & Ask, Research Threads, Artifact Generator, and Evaluation.

## Makefile Targets

| Command | Description |
|---|---|
| `make setup` | Install all dependencies via uv |
| `make ingest` | Parse PDFs, chunk, embed, and store in ChromaDB |
| `make app` | Launch the Streamlit portal |
| `make evaluate` | Run the 20-query evaluation suite |
| `make query TEXT="..."` | Ask a single question via CLI |
| `make query-filtered TEXT="..." YEAR=2024` | CLI query with metadata filters |
| `make clean` | Remove vector store, logs, and generated outputs |

## Project Structure

```
├── data/
│   ├── raw/                  # 17 source PDFs
│   ├── processed/            # Parsed text JSON per paper (generated by ingest)
│   ├── vector_store/         # ChromaDB persistent storage (generated by ingest)
│   └── data_manifest.json    # Paper metadata (title, authors, year, DOI, arXiv)
├── src/
│   ├── config.py             # Shared configuration (paths, models, keys)
│   ├── ingest/               # PDF parsing, chunking, embedding
│   │   ├── parse.py          # Docling-based section-aware PDF extraction
│   │   ├── chunk.py          # Sentence-boundary chunking (1024 tokens, 100 overlap)
│   │   ├── embed.py          # embeddinggemma-300m embedding + ChromaDB storage
│   │   └── pipeline.py       # Orchestrator: parse -> chunk -> embed -> store
│   ├── rag/                  # Retrieval and generation
│   │   ├── retrieve.py       # Source-diversified retrieval with metadata filtering
│   │   ├── generate.py       # Claude API generation (Haiku for queries, Sonnet for artifacts)
│   │   ├── prompts.py        # System prompts and artifact templates
│   │   └── pipeline.py       # Full RAG pipeline with JSONL logging
│   ├── eval/                 # Evaluation
│   │   ├── queries.json      # 20 queries (10 direct, 5 synthesis, 5 edge-case)
│   │   ├── metrics.py        # Citation precision, groundedness, source recall
│   │   └── runner.py         # Evaluation runner (CLI and programmatic)
│   └── app/                  # Streamlit web portal
│       ├── main.py           # App entry point with navigation
│       ├── pages/            # Search, Threads, Artifacts, Evaluation
│       └── components/       # Citation rendering, Markdown/CSV/PDF export
├── outputs/                  # Generated artifacts (evidence tables, synthesis memos)
│   └── threads/              # Saved research threads (JSON)
├── logs/                     # Machine-readable run logs (JSONL)
├── Makefile                  # One-command build targets
├── pyproject.toml            # Pinned dependencies (resolved in uv.lock)
└── .env                      # ANTHROPIC_API_KEY (gitignored)
```

## Portal Pages

### Search & Ask
Query the corpus with natural language. Optionally filter by year, author, or document type in the sidebar. Every answer includes `[source_id, chunk_id]` citations. Retrieved chunks are shown in expandable sections with distance scores. Answers can be saved to a research thread.

### Research Threads
Persistent file-based threads that accumulate query/answer/evidence across sessions. Browse saved threads, view full history, rename or delete.

### Artifact Generator
Two artifact types, each exportable as Markdown, CSV, or PDF:
- **Evidence Table**: Claim | Evidence Snippet | Citation | Confidence | Notes
- **Synthesis Memo**: 800-1200 word research synthesis with inline citations and reference list

### Evaluation Dashboard
Run the 20-query evaluation suite from the UI. Displays aggregate metrics (citation precision, groundedness, source recall), per-query-type breakdown, and per-query drill-down with full answers.

## Architecture

| Component | Technology |
|---|---|
| Embedding | google/embeddinggemma-300m (768-dim, task-prompted) |
| Vector Store | ChromaDB with cosine similarity (HNSW index) |
| LLM (queries) | Claude Haiku 4.5 |
| LLM (artifacts) | Claude Sonnet 4.6 |
| Chunking | Section-aware, 1024 tokens target, 100 token overlap |
| Retrieval | Source-diversified top-8 (max 3 chunks per paper) |
| UI | Streamlit |

## Trust Behaviors

- Every answer includes citations to specific chunks from the corpus
- Missing evidence is explicitly flagged with a suggested next retrieval step
- No fabricated citations -- the system only cites chunk IDs present in retrieved context
- Conflicting evidence across sources is identified and flagged

## Logging and Reproducibility

- All dependencies pinned in `pyproject.toml` and resolved in `uv.lock`
- Every query logs: timestamp, prompt template version, model ID, retrieved chunk IDs, and full response to `logs/run_logs.jsonl`
- Generated artifacts are saved to `outputs/` with timestamps
- Evaluation results are persisted to `src/eval/eval_results.json` for the dashboard
