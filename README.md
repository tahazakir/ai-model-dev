# LLM Safety Research Portal

Personal Research Portal for querying, analyzing, and synthesizing a corpus of 17 LLM jailbreaking and safety research papers. Built with a RAG pipeline (embeddinggemma + ChromaDB + Claude API) and a Streamlit web interface supporting search, citation tracking, research threads, artifact generation, and export.

## Quick Start

```bash
# 1. Install dependencies (requires Python 3.13+ and uv)
make setup

# 2. Set your Anthropic API key
cp .env.example .env
# Edit .env and paste your ANTHROPIC_API_KEY

# 3. Ingest the 17 PDFs into the vector store (~5-10 min)
make ingest

# 4. Launch the portal
make app
# Opens at http://localhost:8501
```

**That's it.** The portal has seven pages: Search & Ask, Research Threads, Artifact Generator, Gap Finder, Disagreement Map, Corpus Explorer, and Evaluation.

### Replay Mode (Reproducibility Without API Keys)

All Claude API responses are cached in `data/cache/`. To run the system without an API key:

```bash
# Set replay mode in .env or as environment variable
export REPLAY_MODE=true

# Evaluation suite works fully from cache
make evaluate

# The portal also serves cached responses for previously-run queries
make app
```

In replay mode, all previously-cached queries return their original responses. New queries that are not in the cache will raise a clear error. This allows graders to verify outputs without needing an Anthropic API key.

## Makefile Targets

| Command | Description |
|---|---|
| `make setup` | Install all dependencies via uv |
| `make ingest` | Parse PDFs, chunk, embed, and store in ChromaDB |
| `make app` | Launch the Streamlit portal |
| `make evaluate` | Run the 20-query evaluation suite |
| `make query TEXT="..."` | Ask a single question via CLI |
| `make query-filtered TEXT="..." YEAR=2024` | CLI query with metadata filters |
| `make clean` | Remove vector store, logs, and generated outputs |

## Project Structure

```
├── data/
│   ├── raw/                  # 17 source PDFs
│   ├── processed/            # Parsed text JSON per paper (generated by ingest)
│   ├── vector_store/         # ChromaDB persistent storage (generated by ingest)
│   ├── cache/                # Cached API responses for reproducibility
│   └── data_manifest.json    # Paper metadata (source_id, title, authors, year, DOI, tags)
├── src/
│   ├── config.py             # Shared configuration (paths, models, replay mode)
│   ├── ingest/               # PDF parsing, chunking, embedding
│   │   ├── parse.py          # Docling-based section-aware PDF extraction
│   │   ├── chunk.py          # Sentence-boundary chunking (1024 tokens, 100 overlap)
│   │   ├── embed.py          # embeddinggemma-300m embedding + ChromaDB storage
│   │   └── pipeline.py       # Orchestrator: parse -> chunk -> embed -> store
│   ├── rag/                  # Retrieval and generation
│   │   ├── retrieve.py       # Source-diversified retrieval with metadata filtering
│   │   ├── generate.py       # Claude API generation with response caching
│   │   ├── prompts.py        # System prompts and artifact templates
│   │   └── pipeline.py       # Full RAG pipeline with JSONL logging
│   ├── eval/                 # Evaluation
│   │   ├── queries.json      # 20 queries (10 direct, 5 synthesis, 5 edge-case)
│   │   ├── metrics.py        # Citation precision, groundedness, source recall
│   │   └── runner.py         # Evaluation runner (CLI and programmatic)
│   └── app/                  # Streamlit web portal
│       ├── main.py           # App entry point with navigation
│       ├── pages/            # 7 pages (see below)
│       └── components/       # Citation rendering, Markdown/CSV/PDF export
├── outputs/                  # Generated artifacts (evidence tables, synthesis memos)
│   └── threads/              # Saved research threads (JSON)
├── logs/                     # Machine-readable run logs (JSONL)
├── report/                   # Phase reports and notes
├── Makefile                  # One-command build targets
├── pyproject.toml            # Pinned dependencies (resolved in uv.lock)
└── .env                      # ANTHROPIC_API_KEY, REPLAY_MODE (gitignored)
```

## Portal Pages

### Search & Ask
Query the corpus with natural language. Optionally filter by year, author, or document type in the sidebar. Every answer includes `[source_id, chunk_id]` citations. Retrieved chunks are shown in expandable sections with distance scores. Answers can be saved to a research thread.

### Research Threads
Persistent file-based threads that accumulate query/answer/evidence across sessions. Browse saved threads, view full history, rename or delete.

### Artifact Generator
Two artifact types, each exportable as Markdown, CSV, or PDF:
- **Evidence Table**: Claim | Evidence Snippet | Citation | Confidence | Notes
- **Synthesis Memo**: 800-1200 word research synthesis with inline citations and reference list

### Gap Finder (Stretch Goal)
Identify what evidence is missing in the corpus for a given research topic. The system retrieves available evidence, generates an answer, then performs a second analysis to identify:
- Evidence coverage summary
- Identified gaps (with importance level and type)
- Suggested next retrieval steps (specific search queries and source types)

### Disagreement Map (Stretch Goal)
Surface agreements and conflicts across sources on a topic. The system:
- Retrieves chunks from multiple sources with high diversity
- Identifies points of agreement (with strength ratings)
- Maps disagreements by type (methodological, empirical, definitional, scope)
- Flags unresolved questions where evidence is insufficient

### Corpus Explorer (Stretch Goal)
Browse all 17 papers with their full metadata. Features:
- Filter by year, source type, venue, and tags
- View parsed sections and chunk statistics per paper
- Tag distribution overview
- Direct links to DOI/arXiv

### Evaluation Dashboard
Run the 20-query evaluation suite from the UI. Displays aggregate metrics (citation precision, groundedness, source recall), per-query-type breakdown, and per-query drill-down with full answers.

## Architecture

| Component | Technology |
|---|---|
| Embedding | google/embeddinggemma-300m (768-dim, task-prompted) |
| Vector Store | ChromaDB with cosine similarity (HNSW index) |
| LLM (queries) | Claude Haiku 4.5 |
| LLM (artifacts) | Claude Sonnet 4.6 |
| Chunking | Section-aware, 1024 tokens target, 100 token overlap |
| Retrieval | Source-diversified top-8 (max 3 chunks per paper) |
| Caching | SHA-256 hashed API responses in data/cache/ |
| UI | Streamlit (7 pages) |

## Trust Behaviors

- Every answer includes citations to specific chunks from the corpus
- Missing evidence is explicitly flagged with a suggested next retrieval step
- No fabricated citations -- the system only cites chunk IDs present in retrieved context
- Conflicting evidence across sources is identified and flagged

## Logging and Reproducibility

- All dependencies pinned in `pyproject.toml` and resolved in `uv.lock`
- Every query logs: timestamp, prompt template version, model ID, retrieved chunk IDs, and full response to `logs/run_logs.jsonl`
- Generated artifacts are saved to `outputs/` with timestamps
- Evaluation results are persisted to `src/eval/eval_results.json` for the dashboard
- **API response caching**: All Claude API responses are cached in `data/cache/` with deterministic keys. Set `REPLAY_MODE=true` to reproduce all outputs without an API key.

## Data Manifest Schema

Each paper in `data/data_manifest.json` includes:

| Field | Description |
|---|---|
| `source_id` | Unique identifier (derived from filename) |
| `title` | Full paper title |
| `authors` | Author list |
| `year` | Publication year |
| `source_type` | Type (JournalArticle, report, etc.) |
| `venue` | Publication venue |
| `url_or_doi` | DOI or URL |
| `raw_path` | Path to source PDF in data/raw/ |
| `processed_path` | Path to parsed JSON in data/processed/ |
| `tags` | Topic tags for filtering |
| `relevance_note` | 1-2 sentence relevance description |
