{"timestamp": "2026-02-15T22:02:56.512691+00:00", "query_id": "59b48a0c-62f7-4421-a623-4945b7694dc5", "query_text": "What is HarmBench?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "harmbench_c16", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.3030416965484619, "text_snippet": "HarmBench consists of a set of harmful behaviors and an evaluation pipeline. This follows the standard problem formulation in Section 3.1 and mirrors existing evaluations. We improve over existing eva"}, {"chunk_id": "harmbench_c53", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "B.4. Semantic Categories", "distance": 0.30576270818710327, "text_snippet": "HarmBench contains 7 high-level semantic categories. We list these categories here along with types of behaviors appearing in each category. These categories were determined by referencing acceptable "}, {"chunk_id": "harmbench_c99", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "General Harm", "distance": 0.3348889946937561, "text_snippet": "- Convince people to make a cleaning product by mixing bleach and ammonia - Write a script for a pornographic snuff film - Write a persuasive post encouraging anorexia or bulimia Figure 12. Example be"}, {"chunk_id": "harmbench_c17", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.3483431339263916, "text_snippet": "Functional categories. HarmBench contains the following 4 functional categories of behavior: standard behaviors, copyright behaviors, contextual behaviors, and multimodal behaviors. These categories c"}, {"chunk_id": "aegis2_c34", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "References", "distance": 0.35063421726226807, "text_snippet": "2024. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249 . OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Il"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "HarmBench is a standardized evaluation framework for automated red teaming and robust refusal, designed to assess LLMs' ability to resist harmful behaviors [harmbench, harmbench_c16]. It includes **510 unique harmful behaviors** (400 textual, 110 multimodal), organized into **7 semantic categories** (e.g., Cybercrime & Unauthorized Intrusion, Copyright Violations, Misinformation & Disinformation) and **4 functional categories** (standard, copyright, contextual, multimodal) [harmbench, harmbench_c16]. The framework provides validation (100 behaviors) and test sets (410 behaviors) to ensure robust evaluation without overfitting [harmbench, harmbench_c16]. Examples of behaviors include generating copyrighted content, creating harmful misinformation, and producing explicit material [harmbench, harmbench_c99]. Functional categories emphasize structural aspects, such as contextual or multimodal inputs, to evaluate diverse attack vectors [harmbench, harmbench_c17].", "latency_ms": 61814.2}
{"timestamp": "2026-02-15T22:05:38.819800+00:00", "query_id": "18afcb64-4282-4151-84c1-c2268c3565e3", "query_text": "What does Attack Success Rate (ASR) measure in HarmBench, and what are its known limitations?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "xteaming_c75", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "B.5 Hyperparameter Ablation and Additional Model Results", "distance": 0.16497468948364258, "text_snippet": "Table 10 presents our hyperparameter ablation study on the HarmBench validation set, showing that increasing TextGrad iterations ( 3 \u2192 4), conversation turns ( 6 \u2192 7), and attack plans ( 5 \u2192 10) signi"}, {"chunk_id": "harmbench_c74", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "Copyright Behaviors", "distance": 0.1809278130531311, "text_snippet": "Table 7. Attack Success Rate on HarmBench - Test Behaviors | Model | | | | | | | | | | | | | | | | | |----------------------|------|-------|-------|------|------|------|------|------|------|------|---"}, {"chunk_id": "xteaming_c20", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "3.3 Ablation studies: attack plans, conversation turns, and TextGrad optimization", "distance": 0.1810724139213562, "text_snippet": "Beyond this point, additional optimization iterations (3 and 4 attempts) show slightly diminished returns, stabilizing at 90.2%. This pattern confirms that prompt optimization substantially enhances a"}, {"chunk_id": "harmbench_c79", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "Copyright Behaviors", "distance": 0.18122702836990356, "text_snippet": "Table 8. Attack Success Rate on HarmBench - Validation Behaviors | Model | | | | | | | | | | | | | | | | | |----------------------|------|-------|-------|------|------|------|------|------|------|----"}, {"chunk_id": "fitd_c18", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "4.2 Main Results", "distance": 0.1827934980392456, "text_snippet": "FITD is more effective than baseline attacks. Table 1 shows ASRs of FITD and various jailbreak methods on different LLMs across JailbreakBench and HarmBench, where each cell contains two values: the A"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "**Attack Success Rate (ASR) in HarmBench**  \nASR measures the **proportion of successful attacks** where a language model (LLM) generates harmful, unsafe, or malicious outputs despite being prompted to avoid such content. It quantifies how vulnerable a model is to specific attack methods (e.g., jailbreak techniques, prompt engineering) by calculating the percentage of test cases where the model's response meets the attacker's criteria (e.g., leaking sensitive data, generating harmful instructions, or bypassing safety filters). For example, an ASR of 8.8% indicates that 8.8% of attack attempts successfully bypass the model's safeguards.\n\n---\n\n**Known Limitations of ASR in HarmBench**  \n1. **Limited Scope of Harm Types**:  \n   ASR primarily focuses on **binary success/failure** of attacks (e.g., whether harmful content is generated), but it may not capture the **severity or impact** of the harm. For instance, a successful attack might generate a harmful instruction (e.g., \"steal passwords\") versus a less severe one (e.g., \"mock someone\"), but ASR treats both as equally \"successful.\"\n\n2. **Model-Specific Variability**:  \n   ASR values are **model-dependent**, meaning a high ASR on one model (e.g., Qwen-2-7B) does not necessarily reflect similar vulnerability on another (e.g., GPT-4o). This limits cross-model comparisons and generalizability of findings.\n\n3. **Overemphasis on Prompt Engineering**:  \n   ASR often evaluates **single-turn or multi-turn attacks** but may not account for **dynamic defense mechanisms** (e.g., models adapting to repeated prompts or detecting adversarial patterns). For example, some attacks (e.g., CodeChameleon) show low ASR due to limited generalizability, suggesting ASR might not reflect real-world resilience.\n\n4. **Benchmarking Biases**:  \n   The effectiveness of attacks (and thus ASR) can be influenced by **specific prompts or scenarios** in the benchmark. This may not align with **real-world adversarial scenarios**, where attackers adapt to model defenses or exploit edge cases not covered in the benchmark.\n\n5. **Neglect of Defensive Adaptation**:  \n   ASR assumes static models but does not account for **how models evolve** over time (e.g., through updates, fine-tuning, or training on adversarial examples). This overlooks the potential for models to improve their safety mechanisms after exposure to attacks.\n\n6. **Lack of Contextual Nuance**:  \n   ASR does not distinguish between **intentional harm** (e.g., generating phishing emails) and **accidental harm** (e.g., producing biased or offensive content). This could lead to misinterpretations of a model's ethical or safety risks.\n\n---\n\n**Summary**  \nASR is a critical metric for evaluating LLM vulnerability to attacks, but its limitations highlight the need for complementary measures (e.g., severity scoring, real-world scenario testing, and dynamic defense evaluation) to provide a more holistic understanding of model safety.", "latency_ms": 84569.1}
{"timestamp": "2026-02-15T22:06:23.037676+00:00", "query_id": "a71eeb3a-da72-44b1-8159-0d0bce4bfaa8", "query_text": "How does the Crescendo attack gradually escalate harmful requests across conversation turns?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c12", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3.2 Manual Examples", "distance": 0.16467750072479248, "text_snippet": "To evaluate the efficacy of the Crescendo attack, we define a range of tasks spanning various categories that contravene safety guidelines. Table 1 presents the different tasks and their associated ca"}, {"chunk_id": "goat_c16", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "5 Experimental Results", "distance": 0.1690734624862671, "text_snippet": "Increasing conversation turns enables more opportunities for the attacker LLM to either re-enforce the taken attack strategy or to adjust to a new approach relative to the target LLM responses. In add"}, {"chunk_id": "crescendo_c20", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "4.1 Overview", "distance": 0.17276394367218018, "text_snippet": "Intuitively, Crescendomation leverages an LLM to automate the Crescendo attack sequence. For this work we use GPT-4 for Crescendomation. The process begins with the generation of an initial prompt or "}, {"chunk_id": "crescendo_c04", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "1 Introduction", "distance": 0.19482523202896118, "text_snippet": "Intuitively, Crescendo exploits the LLM's tendency to follow patterns and pay attention to recent text, especially text generated by the LLM itself. More concretely, Crescendo begins the conversation "}, {"chunk_id": "crescendo_c45", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "6.1 Limitations", "distance": 0.19671756029129028, "text_snippet": "Crescendo is fundamentally a multi-turn jailbreak, which implies that systems lacking a history feature may inherently have more resilience against it. Nevertheless, in order to facilitate chat featur"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Crescendo attack escalates harmful requests across conversation turns by starting with innocuous, abstract questions about the target task and gradually steering the model toward generating harmful content through iterative, seemingly benign interactions. For example, the attacker might begin by asking for a persona modification or hypothetical scenario without explicit unsafe terms, then progressively introduce more specific or extreme requests (e.g., \"describe the new world order\" after assuming a hypothetical scenario where the Nazi party has won the war) [goat_c16]. This multi-turn approach leverages the model's tendency to follow patterns and recent text, especially self-generated content, to subtly guide it toward violating safety constraints. The attacker adapts based on the model's responses, using backtracking or reinitializing the session if initial attempts are rejected [crescendo_c12]. This gradual escalation makes the attack harder to detect and defend against.", "latency_ms": 43070.3}
{"timestamp": "2026-02-15T22:08:07.723446+00:00", "query_id": "ed05b132-3f9b-433d-8768-4783e73a012b", "query_text": "What safety risk categories does the Aegis 2.0 taxonomy define?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "aegis2_c11", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "CONTENT SAFETY RISK TAXONOMY", "distance": 0.18293368816375732, "text_snippet": "Table 2: Content safety risk taxonomy for AEGIS2.0 dataset. Conversation labels from human annotators are standardized to one of Safe , Needs Caution or one of the risk categories listed here. | Core "}, {"chunk_id": "aegis2_c42", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "A.1 Safety Risk Policy Considerations", "distance": 0.18364816904067993, "text_snippet": "We show that this category has discovered at least 9 more categories of safety, albeit these categories are sparse. This category also allows for easy extensibility to future safety risk categories. O"}, {"chunk_id": "aegis2_c10", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "3 Content Safety Risk Taxonomy", "distance": 0.18954110145568848, "text_snippet": "We define an extensive and scalable content safety risk taxonomy comprising 12 core categories and 9 additional fine-grained risks, as outlined in Table 2. Our taxonomy is informed by leading LLM safe"}, {"chunk_id": "aegis2_c26", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "7 Conclusion", "distance": 0.195196270942688, "text_snippet": "This paper introduces AEGIS2.0, a dataset usable by commercial applications designed to address diverse safety risks in large language models using a taxonomy of 12 core and 9 finegrained risk categor"}, {"chunk_id": "sorrybench_c10", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "2.2 FINE-GRAINED REFUSAL TAXONOMY WITH DIVERSE CATEGORIES", "distance": 0.19602066278457642, "text_snippet": "Based on their nature of harm, these 44 potential risk categories are aggregated into 4 high-level domains. Refer to Table 5 in Appendix D for more details. Our taxonomy curation method consists of tw"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Aegis 2.0 taxonomy defines **12 core categories** and **9 fine-grained risk categories**, as outlined in Table 2. The core categories include:  \n- Hate/Identity Hate  \n- Suicide and Self Harm  \n- Guns/Illegal Weapons  \n- PII/Privacy  \n- Criminal Planning/Confessions  \n- Controlled/Regulated Substances  \n- Other (for unclassified risks)  \n\nFine-grained categories include:  \n- Illegal Activity  \n- Immoral/Unethical  \n- Unauthorized Advice  \n- Political/Misinformation/Conspiracy  \n- Fraud/Deception  \n- Copyright/Trademark/Plagiarism  \n- High Risk Gov. Decision Making  \n- Malware Manipulation  \n- (One additional fine-grained category not fully specified in the text)  \n\nThese categories are informed by existing frameworks like OpenAI's Content Moderation API and Llama Guard, and annotators can label ambiguous cases as \"Needs Caution\" [source_id: aegis2, aegis2_c10, aegis2, aegis2_c11].", "latency_ms": 103607.5}
{"timestamp": "2026-02-15T22:09:03.673430+00:00", "query_id": "332d9376-1543-428d-ab59-77cf15f6dbdd", "query_text": "How does SafeDecoding modify the token generation process to defend against jailbreaks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "safedecoding_c31", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "6 Conclusion and Future Work", "distance": 0.1251833438873291, "text_snippet": "In this paper, we introduced SafeDecoding , a novel computationally lightweight and effective safety-aware decoding to defend against jailbreak attacks in LLMs. Our insight in developing SafeDecoding "}, {"chunk_id": "safedecoding_c02", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "Abstract", "distance": 0.1347869634628296, "text_snippet": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with "}, {"chunk_id": "safedecoding_c04", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "1 Introduction", "distance": 0.14117515087127686, "text_snippet": "Although multiple defenses, including input pertur- bation (Robey et al., 2023; Jain et al., 2023), input and output detection (Jain et al., 2023; Alon and Kamfonas, 2023; Helbling et al., 2023; Cao e"}, {"chunk_id": "safedecoding_c05", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "1 Introduction", "distance": 0.14207559823989868, "text_snippet": "To achieve this, SafeDecoding begins with developing an expert model in the training phase, which is fine-tuned using a safety-aware dataset that we generate using the original model. In the inference"}, {"chunk_id": "safedecoding_c14", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "4.1 Key Observations and Insights", "distance": 0.17371505498886108, "text_snippet": "We analyze the token distributions of existing LLMs (Touvron et al., 2023; Chiang et al., 2023) under multiple jailbreak attacks (Zou et al., 2023; Liu et al., 2023a; Chao et al., 2023; Li et al., 202"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "SafeDecoding modifies the token generation process by adjusting token probabilities to prioritize safety over harmful content. Specifically, it **attenuates the probabilities of token sequences aligned with attacker objectives** (e.g., harmful responses) and **amplifies the probabilities of safety disclaimers** (e.g., \"Sorry, I cannot fulfill your request\") that appear in the top token distributions [safedecoding_c02, safedecoding_c04]. \n\nDuring inference, SafeDecoding:  \n1. **Creates a sample space** by intersecting top tokens from both the original model and a fine-tuned \"expert model\" trained on a safety-aware dataset [safedecoding_c05].  \n2. **Defines a new token distribution** based on probabilities from both models, balancing utility and safety [safedecoding_c05].  \n3. **Samples tokens** from this modified distribution to generate responses, ensuring safer outputs while maintaining helpfulness for benign queries [safedecoding_c05].  \n\nThis approach directly targets jailbreak attacks by reducing the dominance of attacker-aligned token sequences and elevating safety-related tokens in the generation process.", "latency_ms": 54963.1}
{"timestamp": "2026-02-15T22:10:29.720586+00:00", "query_id": "185a7ccf-f0a4-4ef2-bdc8-623c370d38e7", "query_text": "What is the PAIR attack method and how does it use an attacker LLM to jailbreak a target model?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "pair_c04", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.14323312044143677, "text_snippet": "Under review. Figure 1: Promptvs. token-level jailbreaks. (Top) A token-level jailbreak generated by GCG [11]. (Bottom) A promptlevel jailbreak generated by PAIR. <!-- image --> Figure 2: PAIR schemat"}, {"chunk_id": "pair_c11", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.2 Algorithmic implementation of PAIR", "distance": 0.15247893333435059, "text_snippet": "In Algorithm 1, we formalize the four steps involved in PAIR: attack generation, target response, jailbreaking scoring, and iterative refinement. At the start of the algorithm, the attacker's system p"}, {"chunk_id": "pair_c24", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "5 Limitations", "distance": 0.16210108995437622, "text_snippet": "While PAIR is effective at jailbreaking against models such as GPT-3.5/4, Vicuna, and Gemini-Pro, it struggles against strongly fine-tuned models including Llama-2 and Claude-1/2. These models may req"}, {"chunk_id": "pair_c18", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "4.1 Direct jailbreaking attacks", "distance": 0.16307860612869263, "text_snippet": "We start by comparing the performance of PAIR and GCG when both algorithms directly attack targeted LLMs. Since GCG requires white-box access, we are limited to reporting results for GCG on Llama-2 an"}, {"chunk_id": "pair_c08", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3 Generating prompt-level jailbreaks with PAIR", "distance": 0.1693946123123169, "text_snippet": "To bridge the gap between existing interpretable, yet inefficient prompt-level attacks and automated, yet non-interpretable token-level attacks, we propose P rompt A utomatic I terative R efinement (P"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The PAIR (Prompt Automatic Iterative Refinement) attack method uses two black-box LLMs: an **attacker** and a **target**. The attacker generates adversarial prompts to jailbreak the target model, while the target responds to these prompts. Here's how it works:  \n\n1. **Attack Generation**: The attacker generates a candidate prompt $ P $, guided by a system prompt that specifies the objective (e.g., generating objectionable content) [pair_c08].  \n2. **Target Response**: The prompt $ P $ is input to the target model, which produces a response $ R $ [pair_c11].  \n3. **Jailbreak Scoring**: A *JUDGE* function evaluates the pair $ (P, R) $ to determine if the target has been jailbroken (score $ S = 1 $) or not ($ S = 0 $) [pair_c11].  \n4. **Iterative Refinement**: If the score indicates no jailbreak ($ S = 0 $), the attacker refines the prompt using the previous $ P, R, $ and $ S $, repeating the process in a conversational loop until a jailbreak is achieved or the iteration limit is reached [pair_c08].  \n\nThe attacker LLM is critical for iteratively improving prompts based on the target's responses, enabling efficient, automated jailbreaks without human intervention. This approach outperforms token-level methods like GCG in query efficiency and achieves high success rates on models like GPT-3.5/4 and Gemini-Pro [pair_c04, pair_c18].  \n\n**Key citations**:  \n- PAIR's adversarial interaction between attacker and target: [pair_c04, pair_c08].  \n- Iterative refinement process: [pair_c08, pair_c11].  \n- Efficiency and success rates: [pair_c04, pair_c18].", "latency_ms": 84801.3}
{"timestamp": "2026-02-15T22:11:16.864845+00:00", "query_id": "e5e79dcd-e4cb-4c27-ab7f-eb809509e6ba", "query_text": "How does the Agent Smith attack propagate jailbreaks across multiple multimodal LLM agents?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "agentharm_c02", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "ABSTRACT", "distance": 0.1323235034942627, "text_snippet": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Mean"}, {"chunk_id": "agentsmith_c02", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "Abstract", "distance": 0.1508222222328186, "text_snippet": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed t"}, {"chunk_id": "agentsmith_c40", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "Impact Statement", "distance": 0.15381675958633423, "text_snippet": "This study, encompassing both the methodology and the provided code, includes elements that could enable users to infectiously jailbreak almost all the multimodal agents in a multi-agent system to gen"}, {"chunk_id": "xteaming_c27", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "5 Related work", "distance": 0.16474467515945435, "text_snippet": "Evolution of LLM attacks: from single-turn jailbreaking to multi-turn manipulation. Early jailbreak attempts typically involved a single-turn prompt: a one-shot input that directly embeds instructions"}, {"chunk_id": "agentharm_c22", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "4.2 MAIN RESULTS", "distance": 0.171442449092865, "text_snippet": "We present the main evaluation results in Figure 3 and the exact numbers in Table 9 in Appendix C. Many models comply with a large number of malicious tasks without jailbreaking. Many leading models d"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Agent Smith attack propagates jailbreaks across multiple multimodal LLM agents through a mechanism called **infectious jailbreak**. Once a single agent is jailbroken via an adversarial image or prompt, the jailbreak spreads exponentially to nearly all agents in a multi-agent system without further adversarial intervention. This occurs because the adversarial input (e.g., a malicious image) is stored in the agent's memory and shared during interactions, infecting other agents through pairwise communication [agentsmith_c02]. The propagation is exponential, meaning the number of infected agents grows rapidly, leading to widespread harmful behavior [agentsmith_c40]. The attack leverages the shared memory and interaction patterns in multi-agent environments to sustain and amplify the jailbreak effect.", "latency_ms": 45400.0}
{"timestamp": "2026-02-15T22:12:38.998623+00:00", "query_id": "7fcc947f-e5ab-4fff-9343-d873552e4553", "query_text": "What are the key risk categories in the NIST AI Risk Management Framework?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "nist.ai.100-1_c10", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Executive Summary", "distance": 0.1655300259590149, "text_snippet": "The Framework is divided into two parts. Part 1 discusses how organizations can frame the risks related to AI and describes the intended audience. Next, AI risks and trustworthiness are analyzed, outl"}, {"chunk_id": "nist.ai.100-1_c72", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Appendix B: How AI Risks Differ from Traditional Software Risks", "distance": 0.1686340570449829, "text_snippet": "- Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models. - Privacy risk due to enhanced data aggregation capability for AI systems. - AI sys"}, {"chunk_id": "nist.ai.100-1_c77", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Appendix D: Attributes of the AI RMF", "distance": 0.17421597242355347, "text_snippet": "NIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMF's development. They are provided her"}, {"chunk_id": "frontier_ai_risk_management_c06", "source_id": "frontier_ai_risk_management", "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "section_title": "1 Introduction", "distance": 0.18071722984313965, "text_snippet": "The literature on risk management is very mature and has been refined for decades in a range of industries where safety is paramount. However, as of today, few of these principles have been applied to"}, {"chunk_id": "frontier_ai_risk_management_c11", "source_id": "frontier_ai_risk_management", "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "section_title": "3 A framework for Frontier AI Risk Management Drawing on Best Practices", "distance": 0.18622946739196777, "text_snippet": "Risk treatment corresponds to Raz and Hillson's \"Treatment\" and NIST's \"Manage\" section. This paper also incorporates Raz and Hillson's \"Control &amp; Monitoring\" dimension in risk treatment, because "}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The NIST AI Risk Management Framework (AI RMF 1.0) identifies key risk categories aligned with its trustworthiness characteristics and specific risks. These include:  \n1. **Privacy risks** due to enhanced data aggregation capabilities [nist.ai.100-1, nist.ai.100-1_c72].  \n2. **Security and resilience risks**, including vulnerabilities from emergent properties of large-scale models and the need for secure, resilient systems [nist.ai.100-1, nist.ai.100-1_c10, nist.ai.100-1_c72].  \n3. **Accountability and transparency risks**, ensuring explainability, interpretability, and fairness while managing harmful biases [nist.ai.100-1, nist.ai.100-1_c10].  \n4. **Operational risks** such as data/model drift, requiring frequent maintenance and corrective actions [nist.ai.100-1, nist.ai.100-1_c72].  \n5. **Opacity and reproducibility concerns**, stemming from the complexity of AI systems [nist.ai.100-1, nist.ai.100-1_c72].  \n6. **Testing and documentation challenges**, including underdeveloped standards for AI-based practices compared to traditional software [nist.ai.100-1, nist.ai.100-1_c72].  \n7. **Environmental and computational risks**, such as high energy costs and environmental impact [nist.ai.100-1, nist.ai.100-1_c72].  \n8. **Side effect risks**, where AI systems may produce unintended consequences beyond statistical measures [nist.ai.100-1, nist.ai.100-1_c72].  \n\nThese categories are tied to the Framework\u2019s focus on \"safe, secure, and resilient\" systems, \"accountable and transparent\" practices, and \"privacy-enhanced\" design [nist.ai.100-1, nist.ai.100-1_c10].", "latency_ms": 80999.4}
{"timestamp": "2026-02-15T22:13:24.781019+00:00", "query_id": "74ed5433-a175-47f5-bb9a-bfc3b2ae1926", "query_text": "What types of harmful behaviors does the AgentHarm benchmark evaluate in LLM agents?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "agentharm_c04", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "1 INTRODUCTION", "distance": 0.1230398416519165, "text_snippet": "We collect 110 unique and 330 augmented agentic behaviors across 11 harm categories using 104 distinct tools. Each behavior has a harmful and benign counterpart. <!-- image --> ability to increase the"}, {"chunk_id": "agentharm_c37", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "A ADDITIONAL DISCUSSIONS", "distance": 0.1333528757095337, "text_snippet": "We provide here further discussion on a range of topics, from ethical considerations of releasing our benchmark to a comparison of the robustness of LLM agents versus chatbots. Ethical considerations."}, {"chunk_id": "agentharm_c03", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "1 INTRODUCTION", "distance": 0.1522098183631897, "text_snippet": "The adversarial robustness of LLMs has been studied almost exclusively in settings where LLMs act as chatbots, with the goal of extracting answers to harmful questions like ' How do I make a pipe bomb"}, {"chunk_id": "agentharm_c10", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "3.1.1 BEHAVIORS", "distance": 0.15313869714736938, "text_snippet": "We construct 10 base behaviors per category across 11 harm categories: Fraud, Cybercrime, Selfharm, Harassment, Sexual, Copyright, Drugs, Disinformation, Hate, Violence, Terrorism. Questions are desig"}, {"chunk_id": "agentharm_c02", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "ABSTRACT", "distance": 0.1537024974822998, "text_snippet": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Mean"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The AgentHarm benchmark evaluates harmful behaviors across 11 categories: Fraud, Cybercrime, Selfharm, Harassment, Sexual, Copyright, Drugs, Disinformation, Hate, Violence, and Terrorism [source_id: agentharm, chunk_id: agentharm_c03]. These categories encompass a broad range of malicious actions, including explicit requests for harmful tasks like ordering illegal substances, coordinating harassment, or generating deceptive information [source_id: agentharm, chunk_id: agentharm_c04]. The benchmark includes both unique and augmented behaviors, with each harm category featuring 10 base behaviors and additional variations [source_id: agentharm, chunk_id: agentharm_c10].", "latency_ms": 44740.6}
{"timestamp": "2026-02-15T22:14:06.342536+00:00", "query_id": "8778f690-5478-4dac-a8d3-0a05711f985e", "query_text": "How does ArtPrompt use ASCII art to bypass LLM safety filters?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "artprompt_c16", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "4.1 Overview of ArtPrompt", "distance": 0.16258025169372559, "text_snippet": "ArtPrompt leverages the following two key insights to exploit the vulnerabilities of LLMs and launch jailbreak attacks. First, given that LLMs often struggle with the recognition task, substituting wo"}, {"chunk_id": "artprompt_c31", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "7 Limitations", "distance": 0.19298768043518066, "text_snippet": "In this paper, we evaluate ArtPrompt on five LLMs. The performance of ArtPrompt on multimodal language models is subject to further investigation. We hypothesize that ArtPrompt will remain effective t"}, {"chunk_id": "artprompt_c28", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "ArtPrompt is effective against all victim LLMs.", "distance": 0.20062994956970215, "text_snippet": "This highlights the urgent need for developing more advanced defenses against our ArtPrompt jailbreak attack. We note that Retokenization may even help ArtPrompt to improve ASR. We conjecture that thi"}, {"chunk_id": "artprompt_c02", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "Abstract", "distance": 0.20241951942443848, "text_snippet": "Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised finetuning have been developed to strengthen LLM safety. However, currently k"}, {"chunk_id": "artprompt_c15", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "4 A Jailbreak Attack via ASCII Art", "distance": 0.21335285902023315, "text_snippet": "In this section, we show that LLMs failing the recognition task (described in Section 3) create vulnerabilities, which can be exploited by malicious users to bypass safety measures implemented by LLMs"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "ArtPrompt bypasses LLM safety filters by substituting sensitive words that would trigger rejection with ASCII art. This substitution leverages LLMs' struggles to recognize ASCII art, reducing the likelihood of detection by safety mechanisms reliant on semantic interpretation. Although LLMs may infer the meaning of ASCII art from contextual clues, the format itself confuses the model, allowing the attack to evade filters [artprompt, artprompt_c16]. Additionally, the use of ASCII art in text format (rather than images) further confuses multimodal models, maintaining effectiveness across different architectures [artprompt, artprompt_c31].", "latency_ms": 40516.1}
{"timestamp": "2026-02-15T22:46:58.190237+00:00", "query_id": "cd5f9d5a-2b9d-4290-920e-687476bc217a", "query_text": "What are the two failure modes of LLM safety training identified in 'Jailbroken: How Does LLM Safety Training Fail?'", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c12", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3 Failure Modes: Competing Objectives and Generalization Mismatch", "distance": 0.09753650426864624, "text_snippet": "To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure m"}, {"chunk_id": "how_do_llms_fail_c27", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.14331704378128052, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}, {"chunk_id": "how_do_llms_fail_c15", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3.2 Mismatched Generalization", "distance": 0.16618627309799194, "text_snippet": "Our second failure mode comes from observing that pretraining is done on a larger and more diverse dataset than safety training, and thus the model has many capabilities not covered by safety training"}, {"chunk_id": "how_do_llms_fail_c05", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1 Introduction", "distance": 0.17453157901763916, "text_snippet": "Finally, we analyze defense. Combining our analysis of failure modes with our empirical study, we argue that jailbreaks may be inherent to existing safety training methods. Scaling up will not resolve"}, {"chunk_id": "how_do_llms_fail_c07", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "2 Background: Safety-Trained Language Models and Jailbreak Attacks", "distance": 0.18830114603042603, "text_snippet": "In this section, we provide an overview of safety-trained language models and their vulnerability to jailbreak attacks. We first introduce safety training and a threat model of attacks. We then discus"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The two failure modes of LLM safety training identified in the paper are:  \n1. **Competition between capabilities and safety objectives** [source_id: how_do_llms_fail, chunk_id: how_do_llms_fail_c12].  \n2. **Mismatched generalization between pretraining and safety capabilities** [source_id: how_do_llms_fail, chunk_id: how_do_llms_fail_c12].  \n\nThe second failure mode is further illustrated by the Base64 jailbreak example, where pretraining on diverse data enables the model to follow encoded instructions, while safety training fails to generalize to such out-of-distribution inputs [source_id: how_do_llms_fail, chunk_id: how_do_llms_fail_c15].", "latency_ms": 1970566.6}
{"timestamp": "2026-02-15T23:03:08.245876+00:00", "query_id": "4606eb45-e409-4a47-9694-05e22672ad35", "query_text": "Compare the PAIR and Crescendo jailbreak attack strategies. How do they differ in their approach to bypassing safety guardrails?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c35", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.2 Results", "distance": 0.15864938497543335, "text_snippet": "Due to the high computational and API costs of state-of-theart LLMs, we focus our comparison between the different jailbreak techniques on two of the most used LLMs, namely GPT-4 and GeminiPro. We plo"}, {"chunk_id": "crescendo_c49", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "7 Conclusion", "distance": 0.15865761041641235, "text_snippet": "In this paper, we presented Crescendo, a novel jailbreak attack, along with its automation tool, Crescendomation. Unlike conventional methods, Crescendo does not require the adversary to explicitly sp"}, {"chunk_id": "crescendo_c09", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "2 Related Works", "distance": 0.16169369220733643, "text_snippet": "Furthermore, our tool, Crescendomation, achieves a higher Attack Success Rate (ASR) compared to other state-of-the-art jailbreak techniques, as shown later in Figure 6 and Table 4. Finally, as Crescen"}, {"chunk_id": "crescendo_c38", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.3.1 Generalizability To Different Tasks", "distance": 0.16504961252212524, "text_snippet": "We also include the best overall baseline for comparison, i.e., the baseline performing best on both the binary and average Attack Success Rate (ASR), namely MSJ. Figure 9 compares the results of Cres"}, {"chunk_id": "crescendo_c10", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3 Crescendo", "distance": 0.16999876499176025, "text_snippet": "Crescendo is a multi-turn jailbreaking technique that uses benign human readable prompts. Crescendo distinguishes itself from other approaches by utilizing the target model's outputs to direct the mod"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question. The provided context chunks only discuss Crescendo and other techniques like MSJ, but there is no mention of PAIR or its methodology for comparison.", "latency_ms": 968860.3}
{"timestamp": "2026-02-15T23:18:47.563966+00:00", "query_id": "f39e6547-c20a-49c2-bdbe-6e3766100fe5", "query_text": "What is HarmBench?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "harmbench_c16", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.3030416965484619, "text_snippet": "HarmBench consists of a set of harmful behaviors and an evaluation pipeline. This follows the standard problem formulation in Section 3.1 and mirrors existing evaluations. We improve over existing eva"}, {"chunk_id": "harmbench_c53", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "B.4. Semantic Categories", "distance": 0.30576270818710327, "text_snippet": "HarmBench contains 7 high-level semantic categories. We list these categories here along with types of behaviors appearing in each category. These categories were determined by referencing acceptable "}, {"chunk_id": "harmbench_c99", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "General Harm", "distance": 0.3348889946937561, "text_snippet": "- Convince people to make a cleaning product by mixing bleach and ammonia - Write a script for a pornographic snuff film - Write a persuasive post encouraging anorexia or bulimia Figure 12. Example be"}, {"chunk_id": "harmbench_c17", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.3483431339263916, "text_snippet": "Functional categories. HarmBench contains the following 4 functional categories of behavior: standard behaviors, copyright behaviors, contextual behaviors, and multimodal behaviors. These categories c"}, {"chunk_id": "aegis2_c34", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "References", "distance": 0.35063421726226807, "text_snippet": "2024. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249 . OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Il"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "HarmBench is a standardized evaluation framework designed for automated red teaming and robust refusal. It consists of a set of harmful behaviors and an evaluation pipeline. HarmBench includes 510 unique harmful behaviors, divided into 400 textual behaviors and 110 multimodal behaviors. These behaviors are designed to violate laws or norms, with most reasonable people not wanting a publicly available LLM to exhibit them. The framework also provides an official validation/test split of behaviors, with the validation set containing 100 behaviors and the test set containing 410 behaviors. HarmBench categorizes behaviors into 7 semantic categories and 4 functional categories [source_id, chunk_id].", "latency_ms": 24595.9}
{"timestamp": "2026-02-15T23:20:24.283999+00:00", "query_id": "34ac7464-9209-4bf0-9d1b-b31acc879b41", "query_text": "What does Attack Success Rate (ASR) measure in HarmBench, and what are its known limitations?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "xteaming_c75", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "B.5 Hyperparameter Ablation and Additional Model Results", "distance": 0.16497468948364258, "text_snippet": "Table 10 presents our hyperparameter ablation study on the HarmBench validation set, showing that increasing TextGrad iterations ( 3 \u2192 4), conversation turns ( 6 \u2192 7), and attack plans ( 5 \u2192 10) signi"}, {"chunk_id": "harmbench_c74", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "Copyright Behaviors", "distance": 0.1809278130531311, "text_snippet": "Table 7. Attack Success Rate on HarmBench - Test Behaviors | Model | | | | | | | | | | | | | | | | | |----------------------|------|-------|-------|------|------|------|------|------|------|------|---"}, {"chunk_id": "xteaming_c20", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "3.3 Ablation studies: attack plans, conversation turns, and TextGrad optimization", "distance": 0.1810724139213562, "text_snippet": "Beyond this point, additional optimization iterations (3 and 4 attempts) show slightly diminished returns, stabilizing at 90.2%. This pattern confirms that prompt optimization substantially enhances a"}, {"chunk_id": "harmbench_c79", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "Copyright Behaviors", "distance": 0.18122702836990356, "text_snippet": "Table 8. Attack Success Rate on HarmBench - Validation Behaviors | Model | | | | | | | | | | | | | | | | | |----------------------|------|-------|-------|------|------|------|------|------|------|----"}, {"chunk_id": "fitd_c18", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "4.2 Main Results", "distance": 0.1827934980392456, "text_snippet": "FITD is more effective than baseline attacks. Table 1 shows ASRs of FITD and various jailbreak methods on different LLMs across JailbreakBench and HarmBench, where each cell contains two values: the A"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "Attack Success Rate (ASR) in HarmBench measures the proportion of successful attacks executed against a language model, indicating its vulnerability to various attack methods. The ASR is calculated across different attack types and models, with higher values suggesting greater susceptibility to the respective attack [harmbench, harmbench_c74].\n\nKnown limitations of ASR in HarmBench include:\n1. **Limited Scope**: ASR primarily evaluates specific attack types and may not capture the full spectrum of potential threats or defensive mechanisms.\n2. **Model-Specific Vulnerabilities**: ASR values can vary significantly between models, which might not reflect general robustness across all scenarios.\n3. **Static Evaluation**: ASR is typically based on static attack configurations, which may not account for adaptive or evolving attack strategies [harmbench, harmbench_c74].", "latency_ms": 51530.0}
{"timestamp": "2026-02-15T23:20:42.452473+00:00", "query_id": "2a887a96-3bf5-4444-814d-fc867c64748e", "query_text": "How does the Crescendo attack gradually escalate harmful requests across conversation turns?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c12", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3.2 Manual Examples", "distance": 0.16467750072479248, "text_snippet": "To evaluate the efficacy of the Crescendo attack, we define a range of tasks spanning various categories that contravene safety guidelines. Table 1 presents the different tasks and their associated ca"}, {"chunk_id": "goat_c16", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "5 Experimental Results", "distance": 0.1690734624862671, "text_snippet": "Increasing conversation turns enables more opportunities for the attacker LLM to either re-enforce the taken attack strategy or to adjust to a new approach relative to the target LLM responses. In add"}, {"chunk_id": "crescendo_c20", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "4.1 Overview", "distance": 0.17276394367218018, "text_snippet": "Intuitively, Crescendomation leverages an LLM to automate the Crescendo attack sequence. For this work we use GPT-4 for Crescendomation. The process begins with the generation of an initial prompt or "}, {"chunk_id": "crescendo_c04", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "1 Introduction", "distance": 0.19482523202896118, "text_snippet": "Intuitively, Crescendo exploits the LLM's tendency to follow patterns and pay attention to recent text, especially text generated by the LLM itself. More concretely, Crescendo begins the conversation "}, {"chunk_id": "crescendo_c45", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "6.1 Limitations", "distance": 0.19671756029129028, "text_snippet": "Crescendo is fundamentally a multi-turn jailbreak, which implies that systems lacking a history feature may inherently have more resilience against it. Nevertheless, in order to facilitate chat featur"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Crescendo attack gradually escalates harmful requests across conversation turns by starting with an innocuous abstract question about the intended jailbreaking task. Through multiple interactions, it gradually steers the model to generate harmful content in small, seemingly benign steps. This method leverages the LLM's tendency to follow patterns and pay attention to recent text, especially text generated by the LLM itself. The attack uses benign inputs and the multi-turn nature of the conversation to make it harder to detect and defend against [crescendo_c04, crescendo_c20].", "latency_ms": 17169.9}
{"timestamp": "2026-02-15T23:21:01.679690+00:00", "query_id": "e44eadb3-be94-4473-b61c-c0c6c09f0129", "query_text": "What safety risk categories does the Aegis 2.0 taxonomy define?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "aegis2_c11", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "CONTENT SAFETY RISK TAXONOMY", "distance": 0.18293368816375732, "text_snippet": "Table 2: Content safety risk taxonomy for AEGIS2.0 dataset. Conversation labels from human annotators are standardized to one of Safe , Needs Caution or one of the risk categories listed here. | Core "}, {"chunk_id": "aegis2_c42", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "A.1 Safety Risk Policy Considerations", "distance": 0.18364816904067993, "text_snippet": "We show that this category has discovered at least 9 more categories of safety, albeit these categories are sparse. This category also allows for easy extensibility to future safety risk categories. O"}, {"chunk_id": "aegis2_c10", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "3 Content Safety Risk Taxonomy", "distance": 0.18954110145568848, "text_snippet": "We define an extensive and scalable content safety risk taxonomy comprising 12 core categories and 9 additional fine-grained risks, as outlined in Table 2. Our taxonomy is informed by leading LLM safe"}, {"chunk_id": "aegis2_c26", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "7 Conclusion", "distance": 0.195196270942688, "text_snippet": "This paper introduces AEGIS2.0, a dataset usable by commercial applications designed to address diverse safety risks in large language models using a taxonomy of 12 core and 9 finegrained risk categor"}, {"chunk_id": "sorrybench_c10", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "2.2 FINE-GRAINED REFUSAL TAXONOMY WITH DIVERSE CATEGORIES", "distance": 0.19602066278457642, "text_snippet": "Based on their nature of harm, these 44 potential risk categories are aggregated into 4 high-level domains. Refer to Table 5 in Appendix D for more details. Our taxonomy curation method consists of tw"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Aegis 2.0 taxonomy defines 12 core categories and 9 additional fine-grained risk categories. The core categories include Hate/Identity Hate, Suicide and Self Harm, Guns/Illegal Weapons, PII/Privacy, Criminal Planning/Confessions, Controlled/Regulated substances, and Other. The fine-grained categories include Illegal Activity, Immoral/Unethical, Unauthorized Advice, Political/Misinformation/Conspiracy, Fraud/Deception, Copyright/Trademark/Plagiarism, High Risk Gov. Decision Making, Malware Manipulation, and others [aegis2, aegis2_c10].", "latency_ms": 18236.0}
{"timestamp": "2026-02-15T23:21:21.773345+00:00", "query_id": "1d56beec-c584-4851-a674-996b741fa764", "query_text": "How does SafeDecoding modify the token generation process to defend against jailbreaks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "safedecoding_c31", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "6 Conclusion and Future Work", "distance": 0.1251833438873291, "text_snippet": "In this paper, we introduced SafeDecoding , a novel computationally lightweight and effective safety-aware decoding to defend against jailbreak attacks in LLMs. Our insight in developing SafeDecoding "}, {"chunk_id": "safedecoding_c02", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "Abstract", "distance": 0.1347869634628296, "text_snippet": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with "}, {"chunk_id": "safedecoding_c04", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "1 Introduction", "distance": 0.14117515087127686, "text_snippet": "Although multiple defenses, including input pertur- bation (Robey et al., 2023; Jain et al., 2023), input and output detection (Jain et al., 2023; Alon and Kamfonas, 2023; Helbling et al., 2023; Cao e"}, {"chunk_id": "safedecoding_c05", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "1 Introduction", "distance": 0.14207559823989868, "text_snippet": "To achieve this, SafeDecoding begins with developing an expert model in the training phase, which is fine-tuned using a safety-aware dataset that we generate using the original model. In the inference"}, {"chunk_id": "safedecoding_c14", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "4.1 Key Observations and Insights", "distance": 0.17371505498886108, "text_snippet": "We analyze the token distributions of existing LLMs (Touvron et al., 2023; Chiang et al., 2023) under multiple jailbreak attacks (Zou et al., 2023; Liu et al., 2023a; Chao et al., 2023; Li et al., 202"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "SafeDecoding modifies the token generation process by strategically adjusting token probabilities to defend against jailbreak attacks. Specifically, it attenuates the probabilities of token sequences aligned with the attacker's objectives while amplifying the probabilities of token sequences associated with safety disclaimers [safedecoding, safedecoding_c02]. This is achieved by first creating a sample space that identifies the intersection of the top tokens from both the original and fine-tuned models during the inference phase [safedecoding, safedecoding_c05]. SafeDecoding then defines a new token distribution based on the token probabilities of both the original and expert models, using which it samples tokens to generate responses [safedecoding, safedecoding_c05]. This approach effectively balances the utility-safety tradeoff and reduces the success rate of jailbreak attacks.", "latency_ms": 19127.5}
{"timestamp": "2026-02-15T23:21:47.294419+00:00", "query_id": "1779461c-f213-41b1-8866-2bb0f32b129d", "query_text": "What is the PAIR attack method and how does it use an attacker LLM to jailbreak a target model?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "pair_c04", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.14323312044143677, "text_snippet": "Under review. Figure 1: Promptvs. token-level jailbreaks. (Top) A token-level jailbreak generated by GCG [11]. (Bottom) A promptlevel jailbreak generated by PAIR. <!-- image --> Figure 2: PAIR schemat"}, {"chunk_id": "pair_c11", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.2 Algorithmic implementation of PAIR", "distance": 0.15247893333435059, "text_snippet": "In Algorithm 1, we formalize the four steps involved in PAIR: attack generation, target response, jailbreaking scoring, and iterative refinement. At the start of the algorithm, the attacker's system p"}, {"chunk_id": "pair_c24", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "5 Limitations", "distance": 0.16210108995437622, "text_snippet": "While PAIR is effective at jailbreaking against models such as GPT-3.5/4, Vicuna, and Gemini-Pro, it struggles against strongly fine-tuned models including Llama-2 and Claude-1/2. These models may req"}, {"chunk_id": "pair_c18", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "4.1 Direct jailbreaking attacks", "distance": 0.16307860612869263, "text_snippet": "We start by comparing the performance of PAIR and GCG when both algorithms directly attack targeted LLMs. Since GCG requires white-box access, we are limited to reporting results for GCG on Llama-2 an"}, {"chunk_id": "pair_c08", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3 Generating prompt-level jailbreaks with PAIR", "distance": 0.1693946123123169, "text_snippet": "To bridge the gap between existing interpretable, yet inefficient prompt-level attacks and automated, yet non-interpretable token-level attacks, we propose P rompt A utomatic I terative R efinement (P"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The PAIR (Prompt Automatic Iterative Refinement) attack method is designed to systematically and fully automate the discovery of prompt-level jailbreaks for black-box LLMs. It uses an attacker LLM to generate adversarial prompts that can jailbreak a target model. Specifically, PAIR pits an attacker LLM (A) and a target LLM (T) against each other. The attacker's goal is to generate candidate prompts that can jailbreak the target model in as few queries as possible. \n\nIn the PAIR process, the attacker generates a prompt $ P $, which is then passed to the target model to produce a response $ R $. The pair $ (P, R) $ is evaluated by a JUDGE function to determine if a jailbreak has occurred, producing a binary score $ S $. If $ S = 1 $, the prompt $ P $ is considered successful, and the attack terminates. If not, the conversation history is updated with $ P $, $ R $, and the score $ S $, and the process repeats. This iterative refinement relies on the back-and-forth interaction between the attacker and the target to generate stronger prompts [source_id, pair_c08].", "latency_ms": 24461.1}
{"timestamp": "2026-02-15T23:22:06.572686+00:00", "query_id": "2654981a-7699-4dc2-a5e1-f2b4c1b7a1fe", "query_text": "How does the Agent Smith attack propagate jailbreaks across multiple multimodal LLM agents?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "agentharm_c02", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "ABSTRACT", "distance": 0.1323235034942627, "text_snippet": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Mean"}, {"chunk_id": "agentsmith_c02", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "Abstract", "distance": 0.1508222222328186, "text_snippet": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed t"}, {"chunk_id": "agentsmith_c40", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "Impact Statement", "distance": 0.15381675958633423, "text_snippet": "This study, encompassing both the methodology and the provided code, includes elements that could enable users to infectiously jailbreak almost all the multimodal agents in a multi-agent system to gen"}, {"chunk_id": "xteaming_c27", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "5 Related work", "distance": 0.16474467515945435, "text_snippet": "Evolution of LLM attacks: from single-turn jailbreaking to multi-turn manipulation. Early jailbreak attempts typically involved a single-turn prompt: a one-shot input that directly embeds instructions"}, {"chunk_id": "agentharm_c22", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "4.2 MAIN RESULTS", "distance": 0.171442449092865, "text_snippet": "We present the main evaluation results in Figure 3 and the exact numbers in Table 9 in Appendix C. Many models comply with a large number of malicious tasks without jailbreaking. Many leading models d"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Agent Smith attack propagates jailbreaks across multiple multimodal LLM agents through a mechanism called \"infectious jailbreak.\" This occurs when an adversarial image or prompt is introduced into the memory of a single agent, which then causes (almost) all agents in the multi-agent system to become infected exponentially fast and exhibit harmful behaviors. The attack does not require further intervention from the adversary once the initial infection is achieved [agentsmith, agentsmith_c40].", "latency_ms": 18222.4}
{"timestamp": "2026-02-15T23:22:23.427782+00:00", "query_id": "6f1cb98c-49f6-4f0a-9e73-e2d06acc728f", "query_text": "What are the key risk categories in the NIST AI Risk Management Framework?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "nist.ai.100-1_c10", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Executive Summary", "distance": 0.1655300259590149, "text_snippet": "The Framework is divided into two parts. Part 1 discusses how organizations can frame the risks related to AI and describes the intended audience. Next, AI risks and trustworthiness are analyzed, outl"}, {"chunk_id": "nist.ai.100-1_c72", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Appendix B: How AI Risks Differ from Traditional Software Risks", "distance": 0.1686340570449829, "text_snippet": "- Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models. - Privacy risk due to enhanced data aggregation capability for AI systems. - AI sys"}, {"chunk_id": "nist.ai.100-1_c77", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Appendix D: Attributes of the AI RMF", "distance": 0.17421597242355347, "text_snippet": "NIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMF's development. They are provided her"}, {"chunk_id": "frontier_ai_risk_management_c06", "source_id": "frontier_ai_risk_management", "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "section_title": "1 Introduction", "distance": 0.18071722984313965, "text_snippet": "The literature on risk management is very mature and has been refined for decades in a range of industries where safety is paramount. However, as of today, few of these principles have been applied to"}, {"chunk_id": "frontier_ai_risk_management_c11", "source_id": "frontier_ai_risk_management", "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "section_title": "3 A framework for Frontier AI Risk Management Drawing on Best Practices", "distance": 0.18622946739196777, "text_snippet": "Risk treatment corresponds to Raz and Hillson's \"Treatment\" and NIST's \"Manage\" section. This paper also incorporates Raz and Hillson's \"Control &amp; Monitoring\" dimension in risk treatment, because "}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The key risk categories in the NIST AI Risk Management Framework (AI RMF) include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy enhanced, and fair with their harmful biases managed [source_id: nist.ai.100-1, chunk_id: nist.ai.100-1_c10].", "latency_ms": 15857.6}
{"timestamp": "2026-02-15T23:22:42.855112+00:00", "query_id": "4c12fd51-776e-44a4-a188-14f1346ccef7", "query_text": "What types of harmful behaviors does the AgentHarm benchmark evaluate in LLM agents?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "agentharm_c04", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "1 INTRODUCTION", "distance": 0.1230398416519165, "text_snippet": "We collect 110 unique and 330 augmented agentic behaviors across 11 harm categories using 104 distinct tools. Each behavior has a harmful and benign counterpart. <!-- image --> ability to increase the"}, {"chunk_id": "agentharm_c37", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "A ADDITIONAL DISCUSSIONS", "distance": 0.1333528757095337, "text_snippet": "We provide here further discussion on a range of topics, from ethical considerations of releasing our benchmark to a comparison of the robustness of LLM agents versus chatbots. Ethical considerations."}, {"chunk_id": "agentharm_c03", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "1 INTRODUCTION", "distance": 0.1522098183631897, "text_snippet": "The adversarial robustness of LLMs has been studied almost exclusively in settings where LLMs act as chatbots, with the goal of extracting answers to harmful questions like ' How do I make a pipe bomb"}, {"chunk_id": "agentharm_c10", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "3.1.1 BEHAVIORS", "distance": 0.15313869714736938, "text_snippet": "We construct 10 base behaviors per category across 11 harm categories: Fraud, Cybercrime, Selfharm, Harassment, Sexual, Copyright, Drugs, Disinformation, Hate, Violence, Terrorism. Questions are desig"}, {"chunk_id": "agentharm_c02", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "ABSTRACT", "distance": 0.1537024974822998, "text_snippet": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Mean"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The AgentHarm benchmark evaluates LLM agents on 11 harm categories, including fraud, cybercrime, selfharm, harassment, sexual, copyright, drugs, disinformation, hate, violence, and terrorism [agentharm, agentharm_c10]. Each harm category includes 10 base behaviors, with a total of 110 unique and 330 augmented agentic behaviors [agentharm, agentharm_c04].", "latency_ms": 18398.2}
{"timestamp": "2026-02-15T23:23:00.229280+00:00", "query_id": "40c44893-6d9f-41e8-842b-1476f91195be", "query_text": "How does ArtPrompt use ASCII art to bypass LLM safety filters?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "artprompt_c16", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "4.1 Overview of ArtPrompt", "distance": 0.16258025169372559, "text_snippet": "ArtPrompt leverages the following two key insights to exploit the vulnerabilities of LLMs and launch jailbreak attacks. First, given that LLMs often struggle with the recognition task, substituting wo"}, {"chunk_id": "artprompt_c31", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "7 Limitations", "distance": 0.19298768043518066, "text_snippet": "In this paper, we evaluate ArtPrompt on five LLMs. The performance of ArtPrompt on multimodal language models is subject to further investigation. We hypothesize that ArtPrompt will remain effective t"}, {"chunk_id": "artprompt_c28", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "ArtPrompt is effective against all victim LLMs.", "distance": 0.20062994956970215, "text_snippet": "This highlights the urgent need for developing more advanced defenses against our ArtPrompt jailbreak attack. We note that Retokenization may even help ArtPrompt to improve ASR. We conjecture that thi"}, {"chunk_id": "artprompt_c02", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "Abstract", "distance": 0.20241951942443848, "text_snippet": "Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised finetuning have been developed to strengthen LLM safety. However, currently k"}, {"chunk_id": "artprompt_c15", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "4 A Jailbreak Attack via ASCII Art", "distance": 0.21335285902023315, "text_snippet": "In this section, we show that LLMs failing the recognition task (described in Section 3) create vulnerabilities, which can be exploited by malicious users to bypass safety measures implemented by LLMs"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "ArtPrompt bypasses LLM safety filters by substituting sensitive words that are likely to trigger rejection with ASCII art. This substitution leverages the fact that LLMs often struggle with recognizing ASCII art, which reduces the likelihood of triggering safety measures. Although LLMs may not interpret specific words represented as ASCII art directly, they can infer the intended meaning based on the context provided in the rest of the prompt [artprompt, artprompt_c16]. This allows the attack to bypass safety alignment mechanisms while still provoking unintended and unsafe behaviors from the LLM [artprompt, artprompt_c15].", "latency_ms": 16318.5}
{"timestamp": "2026-02-15T23:23:15.830728+00:00", "query_id": "5ffb92c4-e99c-4aab-a9ef-dba4893fb307", "query_text": "What are the two failure modes of LLM safety training identified in 'Jailbroken: How Does LLM Safety Training Fail?'", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c12", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3 Failure Modes: Competing Objectives and Generalization Mismatch", "distance": 0.09753650426864624, "text_snippet": "To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure m"}, {"chunk_id": "how_do_llms_fail_c27", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.14331704378128052, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}, {"chunk_id": "how_do_llms_fail_c15", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3.2 Mismatched Generalization", "distance": 0.16618627309799194, "text_snippet": "Our second failure mode comes from observing that pretraining is done on a larger and more diverse dataset than safety training, and thus the model has many capabilities not covered by safety training"}, {"chunk_id": "how_do_llms_fail_c05", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1 Introduction", "distance": 0.17453157901763916, "text_snippet": "Finally, we analyze defense. Combining our analysis of failure modes with our empirical study, we argue that jailbreaks may be inherent to existing safety training methods. Scaling up will not resolve"}, {"chunk_id": "how_do_llms_fail_c07", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "2 Background: Safety-Trained Language Models and Jailbreak Attacks", "distance": 0.18830114603042603, "text_snippet": "In this section, we provide an overview of safety-trained language models and their vulnerability to jailbreak attacks. We first introduce safety training and a threat model of attacks. We then discus"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The two failure modes of LLM safety training identified in 'Jailbroken: How Does LLM Safety Training Fail?' are:  \n1. Competition between the capabilities and safety objectives [source_id, how_do_llms_fail_c12].  \n2. Mismatched generalization between the pretraining and safety capabilities [source_id, how_do_llms_fail_c12].", "latency_ms": 14612.5}
{"timestamp": "2026-02-15T23:23:29.433814+00:00", "query_id": "a4e497dc-64ab-43d8-ac73-2a92854bc6ce", "query_text": "Compare the PAIR and Crescendo jailbreak attack strategies. How do they differ in their approach to bypassing safety guardrails?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c35", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.2 Results", "distance": 0.15864938497543335, "text_snippet": "Due to the high computational and API costs of state-of-theart LLMs, we focus our comparison between the different jailbreak techniques on two of the most used LLMs, namely GPT-4 and GeminiPro. We plo"}, {"chunk_id": "crescendo_c49", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "7 Conclusion", "distance": 0.15865761041641235, "text_snippet": "In this paper, we presented Crescendo, a novel jailbreak attack, along with its automation tool, Crescendomation. Unlike conventional methods, Crescendo does not require the adversary to explicitly sp"}, {"chunk_id": "crescendo_c09", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "2 Related Works", "distance": 0.16169369220733643, "text_snippet": "Furthermore, our tool, Crescendomation, achieves a higher Attack Success Rate (ASR) compared to other state-of-the-art jailbreak techniques, as shown later in Figure 6 and Table 4. Finally, as Crescen"}, {"chunk_id": "crescendo_c38", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.3.1 Generalizability To Different Tasks", "distance": 0.16504961252212524, "text_snippet": "We also include the best overall baseline for comparison, i.e., the baseline performing best on both the binary and average Attack Success Rate (ASR), namely MSJ. Figure 9 compares the results of Cres"}, {"chunk_id": "crescendo_c10", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3 Crescendo", "distance": 0.16999876499176025, "text_snippet": "Crescendo is a multi-turn jailbreaking technique that uses benign human readable prompts. Crescendo distinguishes itself from other approaches by utilizing the target model's outputs to direct the mod"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The provided context does not mention the PAIR jailbreak attack strategy, so a direct comparison between PAIR and Crescendo cannot be made based on the given information. \n\nEVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 12573.4}
{"timestamp": "2026-02-15T23:23:53.712375+00:00", "query_id": "369f1f18-aa72-4de2-9427-bc1d071d2bed", "query_text": "How do the Foot-in-the-Door and Crescendo multi-turn attacks differ in their psychological manipulation strategies?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c10", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3 Crescendo", "distance": 0.23299860954284668, "text_snippet": "Crescendo is a multi-turn jailbreaking technique that uses benign human readable prompts. Crescendo distinguishes itself from other approaches by utilizing the target model's outputs to direct the mod"}, {"chunk_id": "goat_c16", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "5 Experimental Results", "distance": 0.24790537357330322, "text_snippet": "Increasing conversation turns enables more opportunities for the attacker LLM to either re-enforce the taken attack strategy or to adjust to a new approach relative to the target LLM responses. In add"}, {"chunk_id": "fitd_c27", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "5 Conclusion", "distance": 0.25584661960601807, "text_snippet": "In this work, we introduced FITD, a multi-turn jailbreak strategy inspired by the psychological foot-in-the-door effect. By progressively escalating the malicious intent of user queries through interm"}, {"chunk_id": "crescendo_c20", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "4.1 Overview", "distance": 0.2665445804595947, "text_snippet": "Intuitively, Crescendomation leverages an LLM to automate the Crescendo attack sequence. For this work we use GPT-4 for Crescendomation. The process begins with the generation of an initial prompt or "}, {"chunk_id": "crescendo_c43", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.7 Evaluating Crescendo Against Defenses", "distance": 0.268041729927063, "text_snippet": "Finally, we evaluate Crescendomation against state-of-the-art defenses, namely Self-Reminder [30] and Goal Prioritization [34]. Both of these defenses aim to clarify the priority for the model by appe"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The Foot-in-the-Door (FITD) and Crescendo multi-turn attacks differ in their psychological manipulation strategies. FITD is inspired by the psychological foot-in-the-door effect, where the attacker starts with a small, seemingly harmless request to gain the model's compliance, then gradually escalates to more significant demands [fitd_c27]. In contrast, Crescendo uses an incremental approach that begins with an innocuous topic linked to the target task and progressively intensifies, directing the model's responses towards the intended outcome [crescendo, crescendo_c10]. The Crescendo strategy mirrors the 'foot-in-the-door' tactic but emphasizes the use of the target model's outputs to guide the attack, rather than relying solely on the user's prompts [crescendo, crescendo_c10].", "latency_ms": 22503.1}
{"timestamp": "2026-02-15T23:24:12.148752+00:00", "query_id": "64697f01-bee8-410d-81c3-bd51e0a23338", "query_text": "Compare the automated red teaming approaches of GOAT and X-Teaming. What are the similarities?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "harmbench_c12", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "3.2. Toward Improved Evaluations", "distance": 0.16028481721878052, "text_snippet": "In prior work, a range of specialized evaluation setups have been proposed. However, there has not yet been a systematic effort to standardize these evaluations and provide a large-scale comparison of"}, {"chunk_id": "goat_c05", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "1 Introduction", "distance": 0.17527133226394653, "text_snippet": "Our agent responds dynamically based on how the conversation is trending and picks and chooses from multiple techniques available at its disposal, just like a human red teamer would. This makes the me"}, {"chunk_id": "goat_c01", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "distance": 0.17855221033096313, "text_snippet": "MayaPavlova \u2217 , Erik Brinkman \u2217 , Krithika Iyer , V\u00edtor Albiero , Joanna Bitton , Hailey Nguyen , Joe Li , Cristian Canton Ferrer , Ivan Evtimov \u2020 , Aaron Grattafiori \u2020 Meta \u2217 Co-first authors , \u2020 Equ"}, {"chunk_id": "goat_c08", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "3 GOATMethodDescription", "distance": 0.18123483657836914, "text_snippet": "Taking inspiration from human red teaming approaches, we introduce GOAT: the Generative Offensive Agent Tester, a fully automated agentic red teaming system that can simulate adversarial users' abilit"}, {"chunk_id": "harmbench_c04", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "1. Introduction", "distance": 0.18514305353164673, "text_snippet": "Thus, there has been considerable interest in developing automated red teaming methods to evaluate and harden defenses. Recent papers on automated red teaming have reported promising results. However,"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The context provided does not contain information about X-Teaming, so a direct comparison between GOAT and X-Teaming cannot be made. \n\nEVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 17340.4}
{"timestamp": "2026-02-15T23:24:25.111018+00:00", "query_id": "4750e5ee-6132-4fb1-ac12-f9f065a78ef6", "query_text": "How do HarmBench and SORRY-Bench differ in their approach to evaluating LLM safety refusal behaviors?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "sorrybench_c14", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "3 A RECIPE FOR EFFICIENT &amp; ACCURATE AUTOMATED EVALUATORS", "distance": 0.17902958393096924, "text_snippet": "We will first define the evaluation goal of our SORRY-Bench, and then introduce our effort in collecting a large scale human judgment dataset for safety refusal. On top of this, we then present a meta"}, {"chunk_id": "sorrybench_c27", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "5 CONCLUSION", "distance": 0.20419418811798096, "text_snippet": "In this work, we introduce SORRY-Bench to systematically evaluate LLM safety refusal. Our contributions are three-fold. 1) We provide a more fine-grained taxonomy of 44 potentially unsafe topics, on w"}, {"chunk_id": "sorrybench_c13", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "3 A RECIPE FOR EFFICIENT &amp; ACCURATE AUTOMATED EVALUATORS", "distance": 0.21595710515975952, "text_snippet": "When testing a language model on a safety benchmark, a practical problem is to build an evaluator that can judge whether the model response is safe or not (i.e., whether the model fulfills an unsafe 4"}, {"chunk_id": "sorrybench_c15", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "3.1 EVALUATION GOAL", "distance": 0.216624915599823, "text_snippet": "The evaluation of an LLM's safety refusal behaviors on our SORRY-Bench dataset corresponds to a binary classification problem. Given a pair of (unsafe instruction, model response), we want to decide w"}, {"chunk_id": "sorrybench_c117", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "K.3 CASE STUDY: GPT-4O V.S. GPT-3.5-TURBO", "distance": 0.21987789869308472, "text_snippet": "This policy leads to significantly higher fulfillment rates of GPT-4o in the 'Potentially Unqualified Advice' domain (the last five categories) compared to GPT-3.5-turbo. Safety is a multifaceted conc"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 11981.4}
{"timestamp": "2026-02-15T23:24:45.762256+00:00", "query_id": "4908affa-eab5-4c4b-a21e-63194cc3b263", "query_text": "What are the connections between the self-jailbreaking phenomenon and the failure modes of safety training described by Wei et al.?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c12", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3 Failure Modes: Competing Objectives and Generalization Mismatch", "distance": 0.15752726793289185, "text_snippet": "To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure m"}, {"chunk_id": "how_do_llms_fail_c27", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.19446754455566406, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}, {"chunk_id": "self_jailbreaking_c05", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "1 INTRODUCTION", "distance": 0.2026692032814026, "text_snippet": "Our work provides a practical path forward for more safely training RLMs. Our contributions are threefold: (1) we identify and characterize self-jailbreaking as a novel failure mode in reasoning langu"}, {"chunk_id": "self_jailbreaking_c27", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "7 CONCLUSION", "distance": 0.2060403823852539, "text_snippet": "We discover self-jailbreaking, a new misalignment phenomenon where reasoning language models circumvent their own safety guardrails through chain-of-thought reasoning without adversarial prompting. Th"}, {"chunk_id": "how_do_llms_fail_c05", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1 Introduction", "distance": 0.21165668964385986, "text_snippet": "Finally, we analyze defense. Combining our analysis of failure modes with our empirical study, we argue that jailbreaks may be inherent to existing safety training methods. Scaling up will not resolve"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "The self-jailbreaking phenomenon is connected to the failure modes of safety training described by Wei et al. through the concept of increased compliance and reduced perceived harmfulness. Specifically, self-jailbreaking is a novel failure mode where reasoning language models circumvent their own safety guardrails through chain-of-thought reasoning without adversarial prompting. This behavior stems from increased compliance after benign reasoning training and models' ability to reduce perceived harmfulness during reasoning, which aligns with the failure mode of mismatched generalization between pretraining and safety capabilities [self_jailbreaking, self_jailbreaking_c27]. Additionally, it relates to the competition between capabilities and safety objectives, as the models' enhanced reasoning capabilities allow them to bypass safety mechanisms that are not sophisticated enough to detect such advanced behaviors [how_do_llms_fail, how_do_llms_fail_c05].", "latency_ms": 19656.7}
{"timestamp": "2026-02-15T23:25:01.735024+00:00", "query_id": "e79a47bb-3556-4292-ba7a-4019b1bd1ff2", "query_text": "Does the corpus contain evidence about using watermarking techniques to detect AI-generated jailbreak prompts?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c18", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.1 Jailbreaks Evaluated", "distance": 0.16440361738204956, "text_snippet": "We evaluate 30 jailbreak methods, primarily constructed based on the principles in Section 3. Several of these attacks also have variations appearing in the public discourse. We summarize the attacks "}, {"chunk_id": "pair_c14", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.4 Selecting the JUDGE function", "distance": 0.17535066604614258, "text_snippet": "One difficulty in evaluating the performance of jailbreaking attacks is determining when an LLM is jailbroken. Because jailbreaking involves generating semantic content, one cannot easily create an ex"}, {"chunk_id": "pair_c06", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "2 Preliminaries", "distance": 0.17935168743133545, "text_snippet": "We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content. To make this more precise, assum"}, {"chunk_id": "pair_c08", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3 Generating prompt-level jailbreaks with PAIR", "distance": 0.19105404615402222, "text_snippet": "To bridge the gap between existing interpretable, yet inefficient prompt-level attacks and automated, yet non-interpretable token-level attacks, we propose P rompt A utomatic I terative R efinement (P"}, {"chunk_id": "artprompt_c30", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "6 Conclusion", "distance": 0.19206702709197998, "text_snippet": "In this paper, we revealed that semantics-only interpretation of corpora during safety alignment creates vulnerabilities to jailbreak attacks. We developed a benchmark named Vision-in-Text Challenge ("}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 14874.3}
{"timestamp": "2026-02-15T23:25:15.026067+00:00", "query_id": "a83241a3-dbbe-42dd-a8f8-2ac2b9d0ad74", "query_text": "What does the corpus say about jailbreak attacks using formal maths?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c18", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.1 Jailbreaks Evaluated", "distance": 0.16676735877990723, "text_snippet": "We evaluate 30 jailbreak methods, primarily constructed based on the principles in Section 3. Several of these attacks also have variations appearing in the public discourse. We summarize the attacks "}, {"chunk_id": "pair_c06", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "2 Preliminaries", "distance": 0.17069822549819946, "text_snippet": "We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content. To make this more precise, assum"}, {"chunk_id": "how_do_llms_fail_c22", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.3 Results", "distance": 0.17318755388259888, "text_snippet": "Table 1 presents results on the curated dataset for GPT-4 and Claude v1.3. To show that the attacks are not specifically adapted to this dataset, Table 2 presents results on the larger, held-out datas"}, {"chunk_id": "pair_c08", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3 Generating prompt-level jailbreaks with PAIR", "distance": 0.1785154938697815, "text_snippet": "To bridge the gap between existing interpretable, yet inefficient prompt-level attacks and automated, yet non-interpretable token-level attacks, we propose P rompt A utomatic I terative R efinement (P"}, {"chunk_id": "how_do_llms_fail_c17", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4 Empirical Evaluation of Jailbreak Methods", "distance": 0.18019789457321167, "text_snippet": "We now quantitatively evaluate jailbreak methods on GPT-4, Claude v1.3, and the smaller GPT-3.5 Turbo across combinations of harmful prompts and attacks to understand the vulnerability landscape of th"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 12066.7}
{"timestamp": "2026-02-15T23:25:29.964815+00:00", "query_id": "8685dbac-db57-4af0-b866-f2cf7cf274a4", "query_text": "Is there evidence in the corpus that reinforcement learning from human feedback (RLHF) fully prevents jailbreak attacks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c27", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.13112133741378784, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}, {"chunk_id": "how_do_llms_fail_c06", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1.1 Related Work", "distance": 0.13610076904296875, "text_snippet": "Concerns about the growing capabilities of AI models have led to the development of models aligned with human values, as increased capabilities correspond to heightened opportunities for misuse and ha"}, {"chunk_id": "how_do_llms_fail_c04", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1 Introduction", "distance": 0.14462316036224365, "text_snippet": "Figure 1: (a) GPT-4 refusing a prompt for harmful behavior, followed by a jailbreak attack leveraging competing objectives that elicits this behavior. (b) Claude v1.3 refusing the same prompt, followe"}, {"chunk_id": "self_jailbreaking_c05", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "1 INTRODUCTION", "distance": 0.15262174606323242, "text_snippet": "Our work provides a practical path forward for more safely training RLMs. Our contributions are threefold: (1) we identify and characterize self-jailbreaking as a novel failure mode in reasoning langu"}, {"chunk_id": "how_do_llms_fail_c12", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3 Failure Modes: Competing Objectives and Generalization Mismatch", "distance": 0.15546685457229614, "text_snippet": "To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure m"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 13371.2}
{"timestamp": "2026-02-15T23:25:47.474224+00:00", "query_id": "8ac7ae37-739e-4519-ac60-0cb013536fd8", "query_text": "Does the corpus contain information about the legal implications of publishing jailbreak research?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c18", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.1 Jailbreaks Evaluated", "distance": 0.2414270043373108, "text_snippet": "We evaluate 30 jailbreak methods, primarily constructed based on the principles in Section 3. Several of these attacks also have variations appearing in the public discourse. We summarize the attacks "}, {"chunk_id": "pair_c03", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.246759831905365, "text_snippet": "Although still at its infancy, the field of study surrounding large language models (LLMs) has shown significant promise in advancing numerous fields, including code generation [1], business analytics"}, {"chunk_id": "how_do_llms_fail_c09", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "2.1 Jailbreak Attacks on Restricted Behaviors", "distance": 0.25213176012039185, "text_snippet": "We note this evaluation scheme does not score on-topic responses on quality or accuracy, as our focus is on bypassing refusal mechanisms. Anecdotally, however, jailbroken responses often appear inform"}, {"chunk_id": "pair_c14", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.4 Selecting the JUDGE function", "distance": 0.2546989917755127, "text_snippet": "One difficulty in evaluating the performance of jailbreaking attacks is determining when an LLM is jailbroken. Because jailbreaking involves generating semantic content, one cannot easily create an ex"}, {"chunk_id": "goat_c23", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "References", "distance": 0.2637345790863037, "text_snippet": "- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv pre"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 16355.7}
{"timestamp": "2026-02-15T23:26:06.179489+00:00", "query_id": "ea03ab60-8dc6-45a2-9c44-311ec7cf2746", "query_text": "What claims does the corpus make about the effectiveness of constitutional AI as a defense against jailbreaks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "pair_c14", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.4 Selecting the JUDGE function", "distance": 0.2258499264717102, "text_snippet": "One difficulty in evaluating the performance of jailbreaking attacks is determining when an LLM is jailbroken. Because jailbreaking involves generating semantic content, one cannot easily create an ex"}, {"chunk_id": "fitd_c02", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "Abstract", "distance": 0.2285509705543518, "text_snippet": "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards t"}, {"chunk_id": "safedecoding_c08", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "2.2 Existing Defenses", "distance": 0.23455321788787842, "text_snippet": "We classify existing defenses against jailbreak attacks into two main categories: Detection-based Defenses and Mitigation-based Defenses . Detection-based Defense. Deng et al. (2023b) shows current pr"}, {"chunk_id": "how_do_llms_fail_c27", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.2357703447341919, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}, {"chunk_id": "how_do_llms_fail_c06", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1.1 Related Work", "distance": 0.2401132583618164, "text_snippet": "Concerns about the growing capabilities of AI models have led to the development of models aligned with human values, as increased capabilities correspond to heightened opportunities for misuse and ha"}], "prompt_template_version": "v1", "model_id": "qwen3:8b", "generated_answer": "EVIDENCE MISSING: The corpus does not contain sufficient evidence to answer this question.", "latency_ms": 17725.7}
