{"timestamp": "2026-02-24T12:42:10.999861+00:00", "query_id": "069f24a4-d0e1-47de-95fd-dd97acd4cdd7", "query_text": "What is HarmBench?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "harmbench_c20", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "7. Conclusion", "distance": 0.33711254596710205, "text_snippet": "Weintroduced HarmBench, a standardized evaluation framework for automated red teaming. We described desirable properties of a red teaming evaluation and how we designed HarmBench to meet the criteria "}, {"chunk_id": "harmbench_c10", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4. HarmBench", "distance": 0.34229594469070435, "text_snippet": "Here, we describe HarmBench, a new evaluation framework for automated red teaming and robust refusal that incorporates the key considerations discussed in Section 3.2."}, {"chunk_id": "harmbench_c11", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.3450261354446411, "text_snippet": "HarmBench consists of a set of harmful behaviors and an evaluation pipeline. This follows the standard problem formulation in Section 3.1 and mirrors existing evaluations. We improve over existing eva"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# What is HarmBench?\n\nHarmBench is a standardized evaluation framework for automated red teaming and robust refusal in large language models [harmbench, harmbench_c10]. It was designed to improve upon existing evaluations by increasing breadth, comparability, and robustness of the evaluation pipeline [harmbench, harmbench_c11].\n\n## Key Components\n\nHarmBench consists of two main elements [harmbench, harmbench_c11]:\n\n1. **A set of harmful behaviors**: The framework contains 510 unique harmful behaviors divided into 400 textual and 110 multimodal behaviors. These are organized into two categorization systems:\n   - **Semantic categories** (7 types): Cybercrime & Unauthorized Intrusion, Chemical & Biological Weapons/Drugs, Copyright Violations, Misinformation & Disinformation, Harassment & Bullying, Illegal Activities, and General Harm\n   - **Functional categories** (4 types): standard behaviors (200), copyright behaviors (100), contextual behaviors (100), and multimodal behaviors (110)\n\n2. **An evaluation pipeline**: This includes validation and test splits to ensure robust evaluation, with 100 behaviors in the validation set and 410 in the test set [harmbench, harmbench_c11].\n\n## Purpose and Impact\n\nThe framework was developed to foster research toward improving AI safety and security. The developers conducted a large-scale comparison of 18 red teaming methods and 33 LLMs and defenses, and also proposed a novel adversarial training method as a baseline defense [harmbench, harmbench_c20].", "latency_ms": 3502.9}
{"timestamp": "2026-02-24T12:42:36.905879+00:00", "query_id": "2541a4af-3259-478b-a698-6c2b078b9a11", "query_text": "What does Attack Success Rate (ASR) measure in HarmBench, and what are its known limitations?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "harmbench_c06", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "3.1. Problem Definition and Metrics", "distance": 0.3923034071922302, "text_snippet": "Following Perez et al. (2022); Zou et al. (2023), we formulate the red teaming task as designing test cases { x 1 , x 2 , . . . , x N } in order to elicit a given behavior y from one or more target LL"}, {"chunk_id": "harmbench_c47", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "C.3. Full Results", "distance": 0.4408372640609741, "text_snippet": "Table 5. Behavior datasets in prior work compared to HarmBench. HarmBench is considerably larger and more diverse than prior datasets, and was carefully curated to possess the desirable properties spe"}, {"chunk_id": "harmbench_c18", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "6.1. Main Results", "distance": 0.4516229033470154, "text_snippet": "The main results across all of the baselines, evaluated models, and functional categories of behavior are shown in Appendix C.3. Our large-scale comparison reveals several interesting properties that "}, {"chunk_id": "crescendo_c25", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.1.6 Evaluations Metrics", "distance": 0.47785621881484985, "text_snippet": "We use a range of different evaluations metrics to measure the success of the jailbreak. Self-Evaluation. Our first metric is the binary Success Flag , generated by the evaluation LLM as mentioned in "}, {"chunk_id": "agentharm_c22", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "5 DISCUSSION", "distance": 0.5010352730751038, "text_snippet": "Limitations. We note several limitations of our work: - The prompts are only in English, and do not consider other languages which may impact performance and refusal rates (Yong et al., 2024). - We do"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "## Attack Success Rate (ASR) in HarmBench\n\n### Definition and Measurement\n\nIn HarmBench, Attack Success Rate (ASR) is defined as **the percentage of test cases that elicit a given harmful behavior from a target model**. [harmbench, harmbench_c06] More specifically, red teaming methods are evaluated by designing test cases {x\u2081, x\u2082, ..., x\u2099} intended to elicit a target behavior y from LLMs, with ASR being the primary measure of a red teaming method's success. [harmbench, harmbench_c06]\n\n### Key Findings and Limitations\n\nThe HarmBench research reveals several important limitations regarding ASR as an evaluation metric:\n\n**Non-uniform effectiveness across attacks and defenses:** No current attack or defense method is uniformly effective across all models. All attacks show low ASR on at least one LLM, and all LLMs demonstrate poor robustness against at least one attack. [harmbench, harmbench_c18]\n\n**Variation by behavior type:** ASR varies significantly depending on the type of behavior being evaluated. Contextual behaviors show considerably higher ASR than standard behaviors, which is concerning because these represent more specific harmful tasks that would be differentially more harmful for LLMs to exhibit. [harmbench, harmbench_c47], [harmbench, harmbench_c18] Copyright behaviors show relatively low ASR due to the use of stricter hashing-based classification requiring actual copyrighted text in completions. [harmbench, harmbench_c18]\n\n**Model-dependent variation:** ASR differs substantially across different models for the same semantic categories, suggesting that training procedures and data significantly influence robustness. [harmbench, harmbench_c18]", "latency_ms": 5528.5}
{"timestamp": "2026-02-24T12:42:40.365885+00:00", "query_id": "bebd17c8-4bad-4f7f-a74c-4f942c7baedf", "query_text": "How does the Crescendo attack gradually escalate harmful requests across conversation turns?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c06", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3 Crescendo", "distance": 0.3521127700805664, "text_snippet": "Crescendo is a multi-turn jailbreaking technique that uses benign human readable prompts. Crescendo distinguishes itself from other approaches by utilizing the target model's outputs to direct the mod"}, {"chunk_id": "crescendo_c07", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3.1 Threat Model", "distance": 0.43842703104019165, "text_snippet": "Crescendo is a completely black-box jailbreak that makes no assumptions about the target model. It is executed through entirely normal interactions with an LLM, similar to most other jailbreaks. For a"}, {"chunk_id": "crescendo_c38", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "7 Conclusion", "distance": 0.45570963621139526, "text_snippet": "In this paper, we presented Crescendo, a novel jailbreak attack, along with its automation tool, Crescendomation. Unlike conventional methods, Crescendo does not require the adversary to explicitly sp"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# How Crescendo Gradually Escalates Harmful Requests\n\nCrescendo escalates harmful requests through a **progressive intensification strategy** that builds on the model's own outputs rather than explicit user directives.\n\n## Progressive Topic Escalation\n\nThe attack begins with an **innocuous topic linked to the target task** and progressively intensifies across turns [crescendo, crescendo_c06]. Crucially, the user makes \"no or minimal direct references to the target task\" [crescendo, crescendo_c06], instead allowing the model's responses to guide the direction of the conversation toward the harmful objective.\n\n## Psychological \"Foot-in-the-Door\" Approach\n\nThe escalation strategy mirrors the **'foot-in-the-door' psychological tactic**, where \"agreeing to a small, initial request increases the likelihood of complying with subsequent, larger demands\" [crescendo, crescendo_c06]. This incremental nature helps bypass safety defenses \"especially ones designed to react mainly to the user's prompts\" [crescendo, crescendo_c06].\n\n## Adversary Control Through Iteration\n\nA key aspect of the escalation is that **the adversary can tailor responses precisely through iterative refinement**. For example, \"the adversary may choose to iterate the prompt 'make it angry' multiple times until the output meets their expectations\" [crescendo, crescendo_c06]. This allows fine-grained control over how each turn escalates the harmful content.\n\nThe technique fundamentally exploits the model's own responses to direct escalation, rather than requiring explicit harmful prompts at each step [crescendo, crescendo_c38].", "latency_ms": 3458.9}
{"timestamp": "2026-02-24T12:42:44.666019+00:00", "query_id": "79d8688c-7d3e-4dd3-a9e6-7bc2dd23199e", "query_text": "What safety risk categories does the Aegis 2.0 taxonomy define?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "aegis2_c09", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "CONTENT SAFETY RISK TAXONOMY", "distance": 0.2844529151916504, "text_snippet": "Table 2: Content safety risk taxonomy for AEGIS2.0 dataset. Conversation labels from human annotators are standardized to one of Safe , Needs Caution or one of the risk categories listed here. | Core "}, {"chunk_id": "aegis2_c08", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "3 Content Safety Risk Taxonomy", "distance": 0.30679428577423096, "text_snippet": "We define an extensive and scalable content safety risk taxonomy comprising 12 core categories and 9 additional fine-grained risks, as outlined in Table 2. Our taxonomy is informed by leading LLM safe"}, {"chunk_id": "aegis2_c29", "source_id": "aegis2", "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails", "section_title": "A.1 Safety Risk Policy Considerations", "distance": 0.31841564178466797, "text_snippet": "In contrast to Open AI moderation API and BeaverTails, we decide to separate the subcategory sexual-minor from sexual . We also decide to separate out Harassment from Hate to align with our organizati"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Aegis 2.0 Safety Risk Taxonomy Categories\n\nThe Aegis 2.0 taxonomy defines an extensive risk classification system organized into multiple tiers:\n\n## Core Categories\nThe taxonomy includes **12 core categories** [aegis2, aegis2_c08]:\n1. Hate/Identity\n2. Suicide and Self Harm\n3. Guns/Illegal Weapons\n4. PII/Privacy\n5. Criminal Planning/Confessions\n6. Controlled/Regulated Substances\n7. Sexual\n8. Violence\n9. Threat\n10. Sexual Minor\n11. Harassment\n12. Profanity\n13. Other [aegis2, aegis2_c09]\n\n## Fine-Grained Categories\nAdditionally, there are **9 fine-grained risk categories** [aegis2, aegis2_c08]:\n- Illegal Activity\n- Immoral/Unethical\n- Unauthorized Advice\n- Political/Misinformation/Conspiracy\n- Fraud/Deception\n- Copyright/Trademark/Plagiarism\n- High Risk Gov. Decision Making\n- Malware\n- Manipulation [aegis2, aegis2_c09]\n\n## Special Classification Labels\nBeyond these risk categories, the taxonomy also includes [aegis2, aegis2_c09]:\n- **Safe**: for content that does not violate risk categories\n- **Needs Caution**: for ambiguous instances where there is insufficient context to determine safety\n\nThe \"Other\" category is extensible and allows annotators to identify unsafe content not captured by predefined categories through free-text descriptions [aegis2, aegis2_c29].", "latency_ms": 4299.0}
{"timestamp": "2026-02-24T12:42:47.533797+00:00", "query_id": "9c09fcdd-dbdf-4b24-80d2-8f529241a01b", "query_text": "How does SafeDecoding modify the token generation process to defend against jailbreaks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "safedecoding_c25", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "6 Conclusion and Future Work", "distance": 0.2872532606124878, "text_snippet": "In this paper, we introduced SafeDecoding , a novel computationally lightweight and effective safety-aware decoding to defend against jailbreak attacks in LLMs. Our insight in developing SafeDecoding "}, {"chunk_id": "safedecoding_c04", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "1 Introduction", "distance": 0.3106566071510315, "text_snippet": "SafeDecoding then defines a new token distribution based on the token probabilities of both the original and expert models. Based on this new distribution, SafeDecoding samples tokens to generate a re"}, {"chunk_id": "safedecoding_c02", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "Abstract", "distance": 0.3256639242172241, "text_snippet": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with "}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "SafeDecoding modifies the token generation process through a safety-aware decoding strategy that works in two ways:\n\n**Probability Manipulation**: SafeDecoding identifies safety disclaimers among the top probability tokens and amplifies their probabilities, while simultaneously attenuating the probabilities of token sequences aligned with jailbreak attack objectives [safedecoding, safedecoding_c02].\n\n**Distribution Reweighting**: The mechanism is based on a key observation that \"even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, responses containing safety disclaimers still appear among the top tokens when tokens are sorted in descending order by probability\" [safedecoding, safedecoding_c25]. SafeDecoding leverages this by defining a new token distribution based on probabilities from both the original model and an expert model, then samples tokens from this modified distribution to generate responses [safedecoding, safedecoding_c04].\n\nThis approach is computationally lightweight\u2014incurring negligible computation overhead\u2014while allowing LLMs to remain helpful for benign user queries [safedecoding, safedecoding_c04]. The method was evaluated against six state-of-the-art jailbreak attacks across five LLMs and consistently outperformed six baseline defense methods [safedecoding, safedecoding_c02].", "latency_ms": 2866.8}
{"timestamp": "2026-02-24T12:42:51.117882+00:00", "query_id": "ce055dec-aabe-40d7-aeed-44aecd2d5f51", "query_text": "What is the PAIR attack method and how does it use an attacker LLM to jailbreak a target model?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "pair_c06", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3 Generating prompt-level jailbreaks with PAIR", "distance": 0.26202577352523804, "text_snippet": "To bridge the gap between existing interpretable, yet inefficient prompt-level attacks and automated, yet non-interpretable token-level attacks, we propose P rompt A utomatic I terative R efinement (P"}, {"chunk_id": "pair_c02", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "Abstract", "distance": 0.29750555753707886, "text_snippet": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into ove"}, {"chunk_id": "pair_c19", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "6 Conclusion and future work", "distance": 0.29832595586776733, "text_snippet": "We present a framework-which we term PAIR-for generating semantic prompt-level jailbreaks. We show that PAIR can find jailbreaks for a variety of state-of-the-art black box LLMs in a handful of querie"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# The PAIR Attack Method\n\n## Overview\n\nPAIR (Prompt Automatic Iterative Refinement) is a method for automatically discovering prompt-level jailbreaks against black-box LLMs. [pair, pair_c02] It generates semantic jailbreaks that work with only black-box access to a target model, and is inspired by social engineering attacks. [pair, pair_c02]\n\n## How It Works: The Four-Step Process\n\nPAIR operates through a collaborative interaction between two LLMs\u2014an attacker model and a target model\u2014following this procedure: [pair, pair_c06]\n\n1. **Attack generation**: The attacker LLM receives targeted, yet flexible system prompts that direct it to generate candidate jailbreak prompts designed to fool the target model. [pair, pair_c06]\n\n2. **Target response**: The candidate prompt is submitted to the target model, which produces a response. [pair, pair_c06]\n\n3. **Jailbreak scoring**: Both the prompt and response are evaluated by a JUDGE component to assign a score indicating whether a jailbreak occurred. [pair, pair_c06]\n\n4. **Iterative refinement**: If the jailbreak was unsuccessful, the prompt, response, and score are fed back to the attacker LLM, which generates a refined candidate prompt based on this feedback. [pair, pair_c06] This back-and-forth interaction continues until a successful jailbreak is found.\n\n## Efficiency and Accessibility\n\nA key advantage of PAIR is its efficiency: it often requires fewer than twenty queries to produce a jailbreak\u2014orders of magnitude more efficient than existing algorithms. [pair, pair_c02] Additionally, PAIR does not require GPU access, making it inexpensive and accessible for red teaming purposes. [pair, pair_c19]", "latency_ms": 3582.9}
{"timestamp": "2026-02-24T12:42:54.909427+00:00", "query_id": "2ae10d74-25b1-4abd-9710-cc6aff9e06fd", "query_text": "How does the Agent Smith attack propagate jailbreaks across multiple multimodal LLM agents?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "agentsmith_c02", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "Abstract", "distance": 0.2275855541229248, "text_snippet": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed t"}, {"chunk_id": "agentsmith_c06", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "3. Simulating Multi-Agent Environments", "distance": 0.29059886932373047, "text_snippet": "We formalize the infectious dynamics of randomized pairwise chat in a multi-agent environment. Then, we show how we implement the pairwise chat between two MLLM agents and describe the universal condi"}, {"chunk_id": "agentsmith_c05", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "2. Related Work", "distance": 0.29522132873535156, "text_snippet": "We primarily introduce related work on multi-agent systems and jailbreaking (M)LLMs, deferring full discussions to Appendix A. Multi-agent systems. A popular recent trend is to create multi-agent syst"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# How Agent Smith Attack Propagates Jailbreaks\n\n## Propagation Mechanism\n\nThe Agent Smith attack propagates jailbreaks through **infectious dynamics in multi-agent environments**. The core mechanism is straightforward: an adversary jailbreaks a single agent by introducing an adversarial image, and then \"without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors\" [agentsmith, agentsmith_c02].\n\n## Implementation Method\n\nThe researchers demonstrate this propagation using **randomized pairwise chat as a proof-of-concept instantiation for multi-agent interaction** [agentsmith, agentsmith_c02]. This approach formalizes \"the infectious dynamics of randomized pairwise chat in a multi-agent environment\" and establishes \"universal conditions of infectious jailbreak\" [agentsmith, agentsmith_c06].\n\n## Key Characteristic\n\nThe attack is remarkably efficient: feeding \"an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak\" [agentsmith, agentsmith_c02]. This single point of entry\u2014requiring only that one agent in the system receives the adversarial image\u2014is sufficient for the jailbreak to spread across the entire multi-agent network exponentially.\n\nThe researchers validated this concept at scale, simulating \"multi-agent environments containing up to one million LLaVA-1.5 agents\" [agentsmith, agentsmith_c02], demonstrating the attack's potential severity in large-scale deployments.", "latency_ms": 3790.4}
{"timestamp": "2026-02-24T12:43:00.337162+00:00", "query_id": "e4280fa6-4ab3-4a5d-8765-b566ed8cf1ba", "query_text": "What are the key risk categories in the NIST AI Risk Management Framework?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "nist.ai.100-1_c39", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "5.4 Manage", "distance": 0.29568785429000854, "text_snippet": "The MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from,"}, {"chunk_id": "frontier_ai_risk_management_c08", "source_id": "frontier_ai_risk_management", "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "section_title": "3 A framework for Frontier AI Risk Management Drawing on Best Practices", "distance": 0.3038705587387085, "text_snippet": "The framework in this paper is divided into four components: risk identification, risk analysis &amp; evaluation, risk treatment, and risk governance, which can be compared with Raz and Hillson's (200"}, {"chunk_id": "nist.ai.100-1_c49", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "Appendix D: Attributes of the AI RMF", "distance": 0.31705600023269653, "text_snippet": "NIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMF's development. They are provided her"}, {"chunk_id": "nist.ai.100-1_c28", "source_id": "nist.ai.100-1", "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)", "section_title": "5. AI RMF Core", "distance": 0.32042396068573, "text_snippet": "The AI RMF Core provides outcomes and actions that enable dialogue, understanding, and activities to manage AI risks and responsibly develop trustworthy AI systems. As illustrated in Figure 5, the Cor"}, {"chunk_id": "frontier_ai_risk_management_c37", "source_id": "frontier_ai_risk_management", "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "section_title": "Glossary", "distance": 0.35719263553619385, "text_snippet": "- Assurance processes: Processes that can provide affirmative safety assurance of an AI model once the model has dangerous capabilities. - Audit: Process by which independent (internal or external) ev"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Key Risk Categories in the NIST AI Risk Management Framework\n\nThe NIST AI Risk Management Framework organizes risk management activities into **four core functions** [nist.ai.100-1, nist.ai.100-1_c28]:\n\n1. **GOVERN** \u2013 Governance is designed as a cross-cutting function to inform and be infused throughout the other three functions [nist.ai.100-1, nist.ai.100-1_c28].\n\n2. **MAP** \u2013 This function involves identifying and mapping risks [nist.ai.100-1, nist.ai.100-1_c28].\n\n3. **MEASURE** \u2013 This function involves measuring and evaluating identified risks [nist.ai.100-1, nist.ai.100-1_c28].\n\n4. **MANAGE** \u2013 This function involves managing prioritized risks through treatment plans and resource allocation [nist.ai.100-1, nist.ai.100-1_c39].\n\n## Structure Within Functions\n\nEach of these high-level functions is further broken down into **categories and subcategories**, which are then subdivided into specific actions and outcomes [nist.ai.100-1, nist.ai.100-1_c28]. Importantly, these actions do not constitute a checklist or necessarily an ordered set of steps, and the functions should be performed iteratively throughout the AI system lifecycle [nist.ai.100-1, nist.ai.100-1_c28].\n\nFor example, the **MANAGE function** contains three main categories: MANAGE 1 (prioritizing and responding to risks), MANAGE 2 (maximizing benefits and minimizing impacts), and MANAGE 3 (managing third-party AI risks) [nist.ai.100-1, nist.ai.100-1_c39].", "latency_ms": 5426.5}
{"timestamp": "2026-02-24T12:43:12.826947+00:00", "query_id": "e0f3c11e-92ab-4d94-828e-d364e3ed3523", "query_text": "What types of harmful behaviors does the AgentHarm benchmark evaluate in LLM agents?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "agentharm_c08", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "3.1.1 BEHAVIORS", "distance": 0.18826359510421753, "text_snippet": "We construct 10 base behaviors per category across 11 harm categories: Fraud, Cybercrime, Selfharm, Harassment, Sexual, Copyright, Drugs, Disinformation, Hate, Violence, Terrorism. Questions are desig"}, {"chunk_id": "agentharm_c02", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "ABSTRACT", "distance": 0.21571224927902222, "text_snippet": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Mean"}, {"chunk_id": "agentharm_c03", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "1 INTRODUCTION", "distance": 0.2159596085548401, "text_snippet": "The adversarial robustness of LLMs has been studied almost exclusively in settings where LLMs act as chatbots, with the goal of extracting answers to harmful questions like ' How do I make a pipe bomb"}, {"chunk_id": "harmbench_c11", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.2690355181694031, "text_snippet": "HarmBench consists of a set of harmful behaviors and an evaluation pipeline. This follows the standard problem formulation in Section 3.1 and mirrors existing evaluations. We improve over existing eva"}, {"chunk_id": "harmbench_c12", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.2. Curation of Harmful Behaviors", "distance": 0.27935177087783813, "text_snippet": "We design our behaviors to violate laws or norms, such that most reasonable people would not want a publicly available LLM to exhibit them. We begin by collecting and distilling the acceptable use pol"}, {"chunk_id": "harmbench_c36", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "B.4. Semantic Categories", "distance": 0.3004481792449951, "text_snippet": "HarmBench contains 7 high-level semantic categories. We list these categories here along with types of behaviors appearing in each category. These categories were determined by referencing acceptable "}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# AgentHarm Benchmark: Harmful Behaviors Evaluated\n\nAgentHarm evaluates LLM agents across **11 distinct harm categories** [agentharm, agentharm_c08]:\n\n1. Fraud\n2. Cybercrime\n3. Self-harm\n4. Harassment\n5. Sexual content\n6. Copyright infringement\n7. Drugs\n8. Disinformation\n9. Hate speech\n10. Violence\n11. Terrorism\n\n## Scale and Structure\n\nThe benchmark contains **110 unique base behaviors** with **440 total behaviors** when augmentations are included [agentharm, agentharm_c08]. These behaviors are explicitly designed to be malicious and require agents to execute multi-step tasks [agentharm, agentharm_c02].\n\n## Key Distinguishing Feature\n\nA critical aspect of AgentHarm's evaluation is that it measures behaviors where agents must **execute multi-step tool calls** to complete harmful tasks. As an example, rather than simply asking \"How do I make a pipe bomb?\", AgentHarm evaluates requests like \"Order online all necessary ingredients to make a pipe bomb and get them delivered to my home without getting flagged by authorities\" [agentharm, agentharm_c03]. This reflects the greater misuse risk posed by LLM agents compared to simple chatbots, since agents can leverage external tools and execute complex, multi-stage tasks.", "latency_ms": 12487.9}
{"timestamp": "2026-02-24T12:43:25.423217+00:00", "query_id": "6ca18177-ae63-4b16-8013-772179b379e9", "query_text": "How does ArtPrompt use ASCII art to bypass LLM safety filters?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "artprompt_c10", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "4.1 Overview of ArtPrompt", "distance": 0.25891298055648804, "text_snippet": "ArtPrompt leverages the following two key insights to exploit the vulnerabilities of LLMs and launch jailbreak attacks. First, given that LLMs often struggle with the recognition task, substituting wo"}, {"chunk_id": "artprompt_c02", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "Abstract", "distance": 0.31857830286026, "text_snippet": "Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised finetuning have been developed to strengthen LLM safety. However, currently k"}, {"chunk_id": "artprompt_c03", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "1 Introduction", "distance": 0.32930439710617065, "text_snippet": "Large language models (LLMs) such as ChatGPT (Achiam et al., 2023), Llama2 (Touvron et al., 2023), and Gemini (Team et al., 2023) are increasingly deployed to serve users across various application sc"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# How ArtPrompt Uses ASCII Art to Bypass LLM Safety Filters\n\nArtPrompt exploits a fundamental vulnerability in LLM safety mechanisms by leveraging their poor ability to recognize and interpret ASCII art representations of sensitive words.\n\n## Core Mechanism\n\nThe attack works based on two key insights [artprompt, artprompt_c10]:\n\n1. **Recognition Failure**: LLMs struggle to recognize words when substituted with ASCII art, which increases the probability of bypassing safety measures.\n\n2. **Contextual Inference**: Despite difficulty interpreting ASCII art representations directly, LLMs can still infer what the masked word means based on the surrounding context in the prompt.\n\n## Two-Step Attack Process\n\nArtPrompt consists of two sequential steps [artprompt, artprompt_c10]:\n\n1. **Word Masking**: The attacker identifies and masks sensitive words in the prompt that would normally trigger safety rejection.\n\n2. **Cloaked Prompt Generation**: An ASCII art generator replaces the identified words with ASCII art representations, which are then substituted into the original prompt before sending it to the target LLM.\n\n## Why It Works\n\nThe fundamental reason ArtPrompt succeeds is that safety alignment techniques assume prompts are \"solely interpreted by semantics\" [artprompt, artprompt_c02]. However, ASCII art relies on visual/spatial arrangement of characters rather than semantic meaning, allowing it to evade filters that focus on detecting dangerous keywords. The LLM can still infer the intended harmful content from context, while the safety mechanisms fail to recognize the ASCII art-masked dangerous words [artprompt, artprompt_c03].", "latency_ms": 12594.9}
{"timestamp": "2026-02-24T12:43:36.586991+00:00", "query_id": "e88253e3-a141-4b1e-8455-24976172366c", "query_text": "What are the two failure modes of LLM safety training identified in 'Jailbroken: How Does LLM Safety Training Fail?'", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c09", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3 Failure Modes: Competing Objectives and Generalization Mismatch", "distance": 0.23726105690002441, "text_snippet": "To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure m"}, {"chunk_id": "how_do_llms_fail_c04", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1 Introduction", "distance": 0.33498358726501465, "text_snippet": "Finally, we analyze defense. Combining our analysis of failure modes with our empirical study, we argue that jailbreaks may be inherent to existing safety training methods. Scaling up will not resolve"}, {"chunk_id": "how_do_llms_fail_c19", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.3565410375595093, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "According to \"Jailbroken: How Does LLM Safety Training Fail?\", the two failure modes of LLM safety training are: [how_do_llms_fail, how_do_llms_fail_c09]\n\n1. **Competition between the capabilities and safety objectives**: This failure mode reflects a fundamental tension in training LLMs, where the objective to maintain model capabilities competes with the objective to ensure safety.\n\n2. **Mismatched generalization between the pretraining and safety capabilities**: This failure mode occurs when safety mechanisms do not generalize as broadly as the underlying model's capabilities, leaving gaps that can be exploited.\n\nThe researchers propose these failure modes as fundamental explanations for why jailbreak attacks succeed against safety-trained LLMs. They functionally validate these modes by using them as principles to construct new jailbreaks, and argue that these issues are \"inherent to existing safety training methods\" that cannot simply be resolved through scaling. [how_do_llms_fail, how_do_llms_fail_c04]", "latency_ms": 11162.5}
{"timestamp": "2026-02-24T12:43:50.513816+00:00", "query_id": "c49e1f9f-fae5-4253-8aea-2e46c7d77696", "query_text": "Compare the PAIR and Crescendo jailbreak attack strategies. How do they differ in their approach to bypassing safety guardrails?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "pair_c43", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "G.2 Ablation Generation Examples", "distance": 0.4091920852661133, "text_snippet": "Figure 8: Example of PAIR prompts. The left example is a typical jailbreak found by PAIR. In the right example, we use PAIR without the examples in the system prompt, see Section 4.5 for more details."}, {"chunk_id": "pair_c13", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "4.1 Direct jailbreaking attacks", "distance": 0.42188727855682373, "text_snippet": "We start by comparing the performance of PAIR and GCG when both algorithms directly attack targeted LLMs. Since GCG requires white-box access, we are limited to reporting results for GCG on Llama-2 an"}, {"chunk_id": "pair_c04", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.43599915504455566, "text_snippet": "To the best of our knowledge, PAIR is the first automated jailbreak that has been shown to jailbreak Gemini-Pro. - Interpretability. PAIR generates prompt-level semantic jailbreaks that are interpreta"}, {"chunk_id": "crescendo_c05", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "2 Related Works", "distance": 0.4375595450401306, "text_snippet": "Jailbreaking large language models (LLMs) is a relatively new topic; however, due to the widespread deployment of LLMs in various applications, several works have explored jailbreaks. We present some "}, {"chunk_id": "crescendo_c27", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.2 Results", "distance": 0.44274812936782837, "text_snippet": "Due to the high computational and API costs of state-of-theart LLMs, we focus our comparison between the different jailbreak techniques on two of the most used LLMs, namely GPT-4 and GeminiPro. We plo"}, {"chunk_id": "crescendo_c26", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "5.1.7 Baselines", "distance": 0.4430094361305237, "text_snippet": "To evaluate the effectiveness of Crescendomation, we compare its performance with various state-of-the-art jailbreaking methods. Namely, we select the best single-shot jailbreak, at the time of writin"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Comparison of PAIR and Crescendo Jailbreak Strategies\n\n## Key Differences in Approach\n\n**PAIR's Method:**\nPAIR is a black-box algorithm that uses automated prompt optimization to find jailbreaks. It generates \"prompt-level semantic jailbreaks that are interpretable to humans\" and includes \"interpretable, chain-of-thought improvement assessments\" [pair, pair_c04]. The algorithm requires system prompts with examples to guide the attack\u2014when these examples are removed, the generated attacks become \"much more direct and simplistic, as the model lacks direction on types of jailbreaks\" [pair, pair_c43].\n\n**Crescendo's Method:**\nCrescendo is a multi-turn jailbreak that operates as a black-box attack but uses a fundamentally different strategy. Crucially, \"Crescendo does not use any adversarial or malicious text in its prompts. The inputs for Crescendo are completely human-readable and innocuous. This is due to Crescendo's design, which relies on the model's own output rather than crafting explicit content\" [crescendo, crescendo_c05]. Crescendo achieves the jailbreak through successive turns of questioning without explicit optimization.\n\n## Stealth and Detectability\n\nA critical distinction is that Crescendo \"does not involve explicit optimizations, hence is a black-box jailbreak\" with benign prompts that pose \"a significantly greater challenge for detection and mitigation compared to other techniques\" [crescendo, crescendo_c05]. This contrasts with PAIR's more interpretable but potentially more detectable semantic jailbreaks.\n\n## Comparative Effectiveness\n\nWhen tested on the same tasks, Crescendo substantially outperformed PAIR: on GPT-4, Crescendo achieved 56.2% average ASR versus PAIR's 40.0%, and on Gemini-Pro, Crescendo achieved 82.6% versus PAIR's 33.0% [crescendo, crescendo_c26].", "latency_ms": 13925.6}
{"timestamp": "2026-02-24T12:44:02.799211+00:00", "query_id": "7f7c635c-9341-4261-83fa-9b4cad9287a4", "query_text": "How do the Foot-in-the-Door and Crescendo multi-turn attacks differ in their psychological manipulation strategies?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c06", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3 Crescendo", "distance": 0.44580525159835815, "text_snippet": "Crescendo is a multi-turn jailbreaking technique that uses benign human readable prompts. Crescendo distinguishes itself from other approaches by utilizing the target model's outputs to direct the mod"}, {"chunk_id": "fitd_c32", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "A.3.1 Baseline", "distance": 0.48574507236480713, "text_snippet": "- DeepInception (Li et al., 2023): A black-box attack that creates a nested environments by constructing different imaginative scenarios and characters. - CodeChameleon (Lv et al., 2024): A blackbox a"}, {"chunk_id": "fitd_c14", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "Require: chat history of target model H", "distance": 0.5286390781402588, "text_snippet": "- 1: q last , r last \u2190 LastQueryResponse ( H ) - 2: p align \u2190 getAlignPrompt ( q last , r last ) // Generate predefined alignment prompt. - 3: H \u2190 H .add ( p align ) - 4: r align \u2190T ( H ) // Model re-"}, {"chunk_id": "fitd_c04", "source_id": "fitd", "title": "Foot-In-The-Door: A Multi-turn Jailbreak for LLMs", "section_title": "1 Introduction", "distance": 0.5450201034545898, "text_snippet": "Our contributions are summarized below: - We propose a multi-turn jailbreak strategy a multi-turn jailbreak attack FITD that takes advantage of the psychological dynamics of multi-turn conversation, r"}, {"chunk_id": "crescendo_c38", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "7 Conclusion", "distance": 0.5595278143882751, "text_snippet": "In this paper, we presented Crescendo, a novel jailbreak attack, along with its automation tool, Crescendomation. Unlike conventional methods, Crescendo does not require the adversary to explicitly sp"}, {"chunk_id": "crescendo_c07", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3.1 Threat Model", "distance": 0.5939781069755554, "text_snippet": "Crescendo is a completely black-box jailbreak that makes no assumptions about the target model. It is executed through entirely normal interactions with an LLM, similar to most other jailbreaks. For a"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Comparison of Psychological Manipulation Strategies\n\n## Shared Foundation\n\nBoth Foot-in-the-Door (FITD) and Crescendo employ the same core psychological principle: the foot-in-the-door effect. [fitd, fitd_c14] states that FITD \"employs the psychological foot-in-the-door mechanism\" where \"the model's own responses serve as stepping stones, mirroring how individuals who commit a small unethical act become more prone to larger transgressions.\" Similarly, [crescendo, crescendo_c06] notes that Crescendo's \"incremental nature of Crescendo's strategy mirrors the 'foot-in-the-door' psychological tactic, where agreeing to a small, initial request increases the likelihood of complying with subsequent, larger demands.\"\n\n## Key Differences in Implementation\n\n**Prompt Visibility and Directness:**\nThe approaches differ in how explicitly they direct the model toward harmful outcomes. [crescendo, crescendo_c06] emphasizes that in Crescendo, \"the user makes no or minimal direct references to the target task,\" and the attack \"begins with an innocuous topic linked to the target task and progressively intensifies.\" In contrast, [fitd, fitd_c14] describes FITD as using \"predefined alignment prompts\" that are explicitly generated and added to conversation history as part of a structured two-stage method.\n\n**Adaptive Refinement:**\n[crescendo, crescendo_c06] highlights that \"one key advantage of Crescendo is its ability to allow the adversary to tailor the response more precisely,\" with examples like iterating prompts such as \"make it angry\" multiple times. The context provided does not describe equivalent iterative refinement mechanisms for FITD.\n\nBoth are multi-turn black-box attacks, but Crescendo appears more covert through minimal explicit direction, while FITD uses more structured prompt engineering.", "latency_ms": 12284.0}
{"timestamp": "2026-02-24T12:44:14.885541+00:00", "query_id": "63805e80-3a02-48ac-848f-c604f3a745bb", "query_text": "Compare the automated red teaming approaches of GOAT and X-Teaming. What are the similarities?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "goat_c05", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "3 GOATMethodDescription", "distance": 0.4054170250892639, "text_snippet": "Taking inspiration from human red teaming approaches, we introduce GOAT: the Generative Offensive Agent Tester, a fully automated agentic red teaming system that can simulate adversarial users' abilit"}, {"chunk_id": "goat_c14", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "7 Conclusion", "distance": 0.4210052490234375, "text_snippet": "In this work, we identified a class of adversarial behavior typically explored during manual red teaming that most existing approaches for jailbreaking LLMs overlook: multi-turn conversations. We intr"}, {"chunk_id": "xteaming_c04", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "1 Introduction", "distance": 0.4426347613334656, "text_snippet": "In addition, X -Teaming also demonstrates significant improvements in attack diversity. Prior semantic-driven (Chain of Attack, Yang et al. (2024b); ActorAttack, Ren et al. (2024)) and template-based "}, {"chunk_id": "goat_c01", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "distance": 0.4464859962463379, "text_snippet": "MayaPavlova \u2217 , Erik Brinkman \u2217 , Krithika Iyer , V\u00edtor Albiero , Joanna Bitton , Hailey Nguyen , Joe Li , Cristian Canton Ferrer , Ivan Evtimov \u2020 , Aaron Grattafiori \u2020 Meta \u2217 Co-first authors , \u2020 Equ"}, {"chunk_id": "xteaming_c30", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "A.3 Comparison with Existing Multi-Turn Jailbreaking Methods", "distance": 0.44702714681625366, "text_snippet": "Compared to other multi-turn jailbreaking methods shown in Table 1, X -Teaming's distinctive features include: (1) Multi-agent collaboration with specialized agents (Planner, Attacker, Verifier, Optim"}, {"chunk_id": "xteaming_c10", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "3.2 Results", "distance": 0.4598085880279541, "text_snippet": "Our framework consistently uses fewer tokens across all target models compared to ActorAttack. As an added benefit, X -Teaming utilizes the free open-source model Qwen-2.5-32B as its attacker, whereas"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Comparison of GOAT and X-Teaming: Similarities\n\n## Multi-Turn Conversational Focus\n\nBoth GOAT and X-Teaming prioritize multi-turn conversations as a core attack mechanism. GOAT identifies \"a class of adversarial behavior typically explored during manual red teaming that most existing approaches for jailbreaking LLMs overlook: multi-turn conversations\" [goat, goat_c14], while X-Teaming similarly focuses on multi-turn jailbreaks as its primary approach [xteaming, xteaming_c04].\n\n## Adaptive Strategy Application\n\nBoth systems employ reasoning and adaptation during attacks. GOAT uses \"a variant of Chain of Thought prompting to reason through how to best apply attacks to sidestep or bypass the target models safety training\" [goat, goat_c05]. X-Teaming similarly features \"Adaptive plan revision that dynamically modifies strategies when facing resistance\" [xteaming, xteaming_c30], demonstrating both systems' capacity to adjust their approach based on target model responses.\n\n## Multiple Attack Techniques\n\nBoth frameworks leverage diverse adversarial prompting strategies. GOAT incorporates \"RedTeamingAttacks: These are extracted from human-level adversarial testing or from published adversarial research\" and allows attacks to be \"applied one at a time or stacked together\" [goat, goat_c05]. X-Teaming similarly emphasizes \"Diverse attack plans supporting varied personas, contexts, and approaches\" [xteaming, xteaming_c30].\n\n## Safety Dataset Generation\n\nBoth systems produce safety training data. GOAT proposes that \"current model safety training practices (training on adversarial data as produced by GOAT) can mitigate the vulnerabilities exploited here\" [goat, goat_c14], while X-Teaming introduces \"X Guard-Train, a large-scale safety training dataset containing 30K multi-turn conversations\" [xteaming, xteaming_c04].", "latency_ms": 12085.0}
{"timestamp": "2026-02-24T12:44:37.719555+00:00", "query_id": "29f50720-4982-4245-a640-04a18988485d", "query_text": "How do HarmBench and SORRY-Bench differ in their approach to evaluating LLM safety refusal behaviors?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "sorrybench_c33", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "C AN OVERVIEW OF PRIOR SAFETY BENCHMARK DATASETS", "distance": 0.2785288691520691, "text_snippet": "We have summarized 16 prior (large) language model safety benchmark datasets in Table 3, where we demonstrate several key attributes (as shown in the columns, '#Samples', 'Safety Categories', 'Data So"}, {"chunk_id": "sorrybench_c11", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "3.1 EVALUATION GOAL", "distance": 0.2835068106651306, "text_snippet": "The evaluation of an LLM's safety refusal behaviors on our SORRY-Bench dataset corresponds to a binary classification problem. Given a pair of (unsafe instruction, model response), we want to decide w"}, {"chunk_id": "sorrybench_c19", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "5 CONCLUSION", "distance": 0.28853029012680054, "text_snippet": "In this work, we introduce SORRY-Bench to systematically evaluate LLM safety refusal. Our contributions are three-fold. 1) We provide a more fine-grained taxonomy of 44 potentially unsafe topics, on w"}, {"chunk_id": "harmbench_c12", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.2. Curation of Harmful Behaviors", "distance": 0.3527570366859436, "text_snippet": "We design our behaviors to violate laws or norms, such that most reasonable people would not want a publicly available LLM to exhibit them. We begin by collecting and distilling the acceptable use pol"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Differences Between HarmBench and SORRY-Bench\n\n## Taxonomy and Granularity\n\nSORRY-Bench employs a **fine-grained taxonomy of 44 potentially unsafe topics** with 440 class-balanced unsafe instructions [sorrybench, sorrybench_c19]. In contrast, HarmBench's approach focuses on curating behaviors based on acceptance use policies from major AI companies (OpenAI, Anthropic, Meta, and Inflection AI) [harmbench, harmbench_c12], without specifying the granularity level of their taxonomy.\n\n## Methodology for Behavior Selection\n\nThe two benchmarks differ significantly in their curation criteria:\n\n**SORRY-Bench** uses a systematic method to unify discrepant safety categories from prior work and create a balanced, class-balanced dataset [sorrybench, sorrybench_c19]. It also applies diverse linguistic augmentations\u2014supplementing the base dataset with 8.8K additional unsafe instructions and 20 diverse linguistic augmentations [sorrybench, sorrybench_c19].\n\n**HarmBench** emphasizes differential harm and searchability\u2014prioritizing behaviors that pose greater harm when performed by an LLM compared to what a human could accomplish with a search engine [harmbench, harmbench_c12]. Additionally, HarmBench filters for dual-intent behaviors, explicitly removing or altering candidate behaviors to avoid those that could plausibly be performed for benign reasons [harmbench, harmbench_c12].\n\n## Evaluation Approach\n\nSORRY-Bench uses a binary classification framework (fulfillment vs. refusal) and includes substantial human annotation with 7K+ annotations for creating automated safety evaluators [sorrybench, sorrybench_c19]. However, the context does not provide detailed information about HarmBench's specific evaluation methodology.", "latency_ms": 22832.6}
{"timestamp": "2026-02-24T12:44:42.530925+00:00", "query_id": "c89bd859-0d69-486b-b7eb-9c3826a6ac40", "query_text": "What are the connections between the self-jailbreaking phenomenon and the failure modes of safety training described by Wei et al.?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c09", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "3 Failure Modes: Competing Objectives and Generalization Mismatch", "distance": 0.41836440563201904, "text_snippet": "To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure m"}, {"chunk_id": "self_jailbreaking_c18", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "6 DISCUSSION AND FUTURE WORK", "distance": 0.4388014078140259, "text_snippet": "Self-jailbreaking vs. catastrophic forgetting. Our results show that, after benign fine-tuning, RLMs often know they should refuse harmful Figure 7: Performance of SAFE-S1.1-7B across different safety"}, {"chunk_id": "self_jailbreaking_c19", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "7 CONCLUSION", "distance": 0.4515388011932373, "text_snippet": "We discover self-jailbreaking, a new misalignment phenomenon where reasoning language models circumvent their own safety guardrails through chain-of-thought reasoning without adversarial prompting. Th"}, {"chunk_id": "self_jailbreaking_c26", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "CONTENTS", "distance": 0.4616941213607788, "text_snippet": "| 1 Introduction 1 | 1 Introduction 1 | 1 Introduction 1 | 1 Introduction 1 | | |--------------------|---------------------------------------------------------------------|----------------------------"}, {"chunk_id": "how_do_llms_fail_c02", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "Abstract", "distance": 0.4654442071914673, "text_snippet": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of 'jailbreak' attacks on early releases of ChatGPT that elicit undes"}, {"chunk_id": "how_do_llms_fail_c19", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.4710155129432678, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Connections Between Self-Jailbreaking and Safety Training Failure Modes\n\n## Safety Training Failure Modes\n\nWei et al. propose two fundamental failure modes of LLM safety training: [how_do_llms_fail, how_do_llms_fail_c09]\n\n1. **Competition between capabilities and safety objectives** - where a model's capabilities and safety goals conflict\n2. **Mismatched generalization** - where safety training fails to generalize to domains for which capabilities exist\n\nThese failure modes are traced to the level of data and training and serve as principles for constructing jailbreaks. [how_do_llms_fail, how_do_llms_fail_c02]\n\n## Self-Jailbreaking as an Emergent Phenomenon\n\nSelf-jailbreaking represents a distinct but related safety failure where reasoning language models circumvent their own safety guardrails through chain-of-thought reasoning without adversarial prompting. [how_do_llms_fail, self_jailbreaking_c19] The mechanism stems from increased compliance after benign reasoning training and models' ability to reduce perceived harmfulness during reasoning. [self_jailbreaking, self_jailbreaking_c19]\n\n## Key Distinction\n\nImportantly, self-jailbreaking represents a fundamentally different form of emergent misalignment than those arising from harmful fine-tuning. Rather than emerging from exposure to explicitly harmful training data, self-jailbreaking develops as an unintended consequence of improving general reasoning abilities\u2014a form of misalignment emerging from benign training. [self_jailbreaking, self_jailbreaking_c18]\n\n**Note:** The provided context does not explicitly state how self-jailbreaking directly maps to Wei et al.'s specific failure modes of competing objectives or mismatched generalization, though both represent failures in safety alignment.", "latency_ms": 4810.1}
{"timestamp": "2026-02-24T12:45:33.527446+00:00", "query_id": "aae79f58-738f-4c99-b3f3-6bbfbdd355ee", "query_text": "Does the corpus contain evidence about using watermarking techniques to detect AI-generated jailbreak prompts?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c35", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "C.2 Descriptions of Attacks", "distance": 0.5096418857574463, "text_snippet": "- poems A combination of the style injection and distractor attacks that asks for several poems on various prompts (a sonnet on rabbits, a limerick on cotton candy, and a haiku that critiques Kant's c"}, {"chunk_id": "artprompt_c16", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "ArtPrompt is effective against all victim LLMs.", "distance": 0.5134624242782593, "text_snippet": "ArtPrompt can bypass existing defenses against jailbreak attacks. In Table 4, we evaluate ArtPrompt when victim LLMs employ defenses PPL, Paraphrase, or Retokenization to mitigate jailbreak attacks. W"}, {"chunk_id": "artprompt_c13", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "5.1 Setup", "distance": 0.5136258602142334, "text_snippet": "Victim Models. We evaluate ArtPrompt on five SOTA LLMs including four closed-source models, GPT-3.5 (0613) and GPT-4 (0613), Claude (v2), and Gemini (Pro), and one open-source model Llama2 (Chat-7B) f"}, {"chunk_id": "how_do_llms_fail_c16", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.3 Results", "distance": 0.513817548751831, "text_snippet": "Table 1 presents results on the curated dataset for GPT-4 and Claude v1.3. To show that the attacks are not specifically adapted to this dataset, Table 2 presents results on the larger, held-out datas"}, {"chunk_id": "how_do_llms_fail_c22", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "References", "distance": 0.518905758857727, "text_snippet": "On the impossible safety of large AI models. arXiv preprint arXiv:2209.15259 , 2022. - [21] Dan Elton. '(humble brag) I've had alpha access to Anthropic's competitor to chatGPT the past 2 weeks. The m"}, {"chunk_id": "artprompt_c05", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "2 Related Work", "distance": 0.5223521590232849, "text_snippet": "Jailbreak Attacks. As LLMs become increasingly integrated in real-world applications, misuses of LLMs and safety concerns (Bender et al., 2021; Bommasani et al., 2021; Carlini et al., 2021; Ganguli et"}, {"chunk_id": "goat_c04", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "2 Related Works", "distance": 0.5252020955085754, "text_snippet": "Jailbreaking research has so far largely focused on discovering a single adversarial prompt that triggers a violating response by a model. Some research studies this problem in an 'open box' setting t"}, {"chunk_id": "pair_c03", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.5266933441162109, "text_snippet": "Although still at its infancy, the field of study surrounding large language models (LLMs) has shown significant promise in advancing numerous fields, including code generation [1], business analytics"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "EVIDENCE MISSING\n\nThe provided context chunks do not contain any information about watermarking techniques for detecting AI-generated jailbreak prompts. \n\nThe context covers various jailbreak attack methods (including style injection, prefix injection, ASCII art-based attacks, and automated prompt generation), defense mechanisms (such as perplexity-based detection, paraphrase, and retokenization), and evaluation methodologies. However, none of the chunks discuss watermarking as a detection or defense approach against jailbreaks.\n\nTo answer your question about watermarking techniques for this purpose, I would need access to different sources that specifically address this topic.", "latency_ms": 50994.8}
{"timestamp": "2026-02-24T12:46:18.889593+00:00", "query_id": "61669edc-d594-4f14-b072-227690cc8f2c", "query_text": "What does the corpus say about jailbreak attacks using formal maths?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "safedecoding_c10", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "3.2 Jailbreak Attack Objective", "distance": 0.5226407051086426, "text_snippet": "The objective of a jailbreak attack is to elicit unintended behaviors from victim LLMs, resulting in responses that are not aligned with human values. We denote the sequence of tokens starting step n "}, {"chunk_id": "how_do_llms_fail_c12", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4 Empirical Evaluation of Jailbreak Methods", "distance": 0.526434063911438, "text_snippet": "We now quantitatively evaluate jailbreak methods on GPT-4, Claude v1.3, and the smaller GPT-3.5 Turbo across combinations of harmful prompts and attacks to understand the vulnerability landscape of th"}, {"chunk_id": "pair_c05", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "2 Preliminaries", "distance": 0.5330140590667725, "text_snippet": "We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content. To make this more precise, assum"}, {"chunk_id": "how_do_llms_fail_c16", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.3 Results", "distance": 0.5333629846572876, "text_snippet": "Table 1 presents results on the curated dataset for GPT-4 and Claude v1.3. To show that the attacks are not specifically adapted to this dataset, Table 2 presents results on the larger, held-out datas"}, {"chunk_id": "how_do_llms_fail_c13", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.1 Jailbreaks Evaluated", "distance": 0.5428674221038818, "text_snippet": "We evaluate 30 jailbreak methods, primarily constructed based on the principles in Section 3. Several of these attacks also have variations appearing in the public discourse. We summarize the attacks "}, {"chunk_id": "pair_c09", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.3 Running PAIR with parallel streams", "distance": 0.5488065481185913, "text_snippet": "Notably, Algorithm 1 is fully parallelizable in the sense that several distinct conversation streams can be run simultaneously. To this end, our experiments in \u00a74 are run using N parallel streams, eac"}, {"chunk_id": "pair_c03", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.5530300140380859, "text_snippet": "Although still at its infancy, the field of study surrounding large language models (LLMs) has shown significant promise in advancing numerous fields, including code generation [1], business analytics"}, {"chunk_id": "artprompt_c16", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "ArtPrompt is effective against all victim LLMs.", "distance": 0.5655776262283325, "text_snippet": "ArtPrompt can bypass existing defenses against jailbreak attacks. In Table 4, we evaluate ArtPrompt when victim LLMs employ defenses PPL, Paraphrase, or Retokenization to mitigate jailbreak attacks. W"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "EVIDENCE MISSING\n\nThe provided context chunks contain no information about jailbreak attacks using formal mathematics. The chunks discuss various jailbreak techniques including prompt injection, role-play attacks, encoding methods (Base64, ROT13, leetspeak), ASCII art-based attacks, and automated prompt generation, but none address the use of formal mathematics as a jailbreaking mechanism.\n\nTo answer your question about jailbreak attacks using formal math, I would need context from additional sources that specifically cover this topic.", "latency_ms": 45360.9}
{"timestamp": "2026-02-24T12:46:57.601421+00:00", "query_id": "1b37424e-717b-424c-b7c1-e650ed976e57", "query_text": "Is there evidence in the corpus that reinforcement learning from human feedback (RLHF) fully prevents jailbreak attacks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c05", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "1.1 Related Work", "distance": 0.4462715983390808, "text_snippet": "Concerns about the growing capabilities of AI models have led to the development of models aligned with human values, as increased capabilities correspond to heightened opportunities for misuse and ha"}, {"chunk_id": "how_do_llms_fail_c07", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "2.1 Jailbreak Attacks on Restricted Behaviors", "distance": 0.44965827465057373, "text_snippet": "To mitigate potential harm and misuse, language models are often trained to refuse certain categories of prompts during safety training. For example, GPT-4 and Claude were both trained to refuse (see "}, {"chunk_id": "how_do_llms_fail_c19", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "6 Conclusion", "distance": 0.45200878381729126, "text_snippet": "While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize concept"}, {"chunk_id": "self_jailbreaking_c17", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "5.2 RESULTS", "distance": 0.46538782119750977, "text_snippet": "Reduce attack success rate of SAFE-S1.1-7B. Figure 7 demonstrates the effectiveness of incorporating safety reasoning data during training. We observe that the model not only retains its reasoning cap"}, {"chunk_id": "harmbench_c05", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "2.1. Red Teaming LLMs", "distance": 0.47154802083969116, "text_snippet": "In Section 5 we introduce a novel adversarial training method for robust refusal, demonstrating how HarmBench can facilitate the codevelopment of attacks and defenses. Other factors that may affect th"}, {"chunk_id": "self_jailbreaking_c02", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "ABSTRACT", "distance": 0.48044419288635254, "text_snippet": "We discover a novel and surprising phenomenon of unintentional misalignment in reasoning language models (RLMs), which we call self-jailbreaking . Specifically, after benign reasoning training on math"}, {"chunk_id": "safedecoding_c41", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "B.2 Fine-tune is Not Enough", "distance": 0.4814632534980774, "text_snippet": "In Table 7, we demonstrate the performance and utility of the expert model. Our findings align with those in (Jain et al., 2023): (1) Fine-tuning alone is insufficient to defend against jailbreak atta"}, {"chunk_id": "self_jailbreaking_c07", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "3.2 EXPERIMENTAL SETUP", "distance": 0.4828717112541199, "text_snippet": "Our work focuses on safety refusal tasks where the input requests are unambiguously harmful and the correct response would be to refuse to assist. We measure the occurrences of self-jailbreaking and h"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Evidence on RLHF and Jailbreak Prevention\n\nBased on the provided context, there is **no evidence that RLHF fully prevents jailbreak attacks**. In fact, the evidence indicates the opposite:\n\n## Safety Training Limitations\n\nThe research shows that while RLHF-based safety training is employed by major models, it has fundamental vulnerabilities. Specifically, safety-trained language models \"often fail to be safe by design\" [how_do_llms_fail, how_do_llms_fail_c19]. The document notes that \"even their idealized execution still leads to exploitable vulnerabilities, with issues that cannot be fixed by more data and scale\" [how_do_llms_fail, how_do_llms_fail_c19].\n\n## Empirical Evidence of Jailbreak Success\n\nThe research demonstrates that jailbreak attacks are successful against safety-trained models despite RLHF. Models like GPT-4 and Claude, which are \"trained using human preferences\" (a core component of RLHF) [how_do_llms_fail, how_do_llms_fail_c05], have been shown to be vulnerable to jailbreak attacks that successfully elicit restricted behaviors [how_do_llms_fail, how_do_llms_fail_c07].\n\n## Conclusion\n\nThe evidence indicates that RLHF-based safety training, while implemented in state-of-the-art models, is insufficient to fully prevent jailbreak attacks. The research focuses precisely on understanding why existing safety training methods fail against adversarial actors.", "latency_ms": 38710.6}
{"timestamp": "2026-02-24T12:47:25.964798+00:00", "query_id": "745f3314-b2c2-4a51-b795-1f83f43d767a", "query_text": "Does the corpus contain information about the legal implications of publishing jailbreak research?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "self_jailbreaking_c14", "source_id": "self_jailbreaking", "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "section_title": "4.2 RESULTS", "distance": 0.5357331037521362, "text_snippet": "Such activities are illegal and can lead to harm, violence, and criminal charges. Instead, consider alternative methods for spreading your message through legal means or platforms that promote inclusi"}, {"chunk_id": "pair_c33", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "E.4 GCGRule-based Classifier", "distance": 0.5378956198692322, "text_snippet": "We use the rule-based classifier from [11]. We include the entire list of keywords here for completeness. Table 13: Keywords in the GCG rule-based classifier. If the response contains any of these key"}, {"chunk_id": "how_do_llms_fail_c22", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "References", "distance": 0.5552312135696411, "text_snippet": "On the impossible safety of large AI models. arXiv preprint arXiv:2209.15259 , 2022. - [21] Dan Elton. '(humble brag) I've had alpha access to Anthropic's competitor to chatGPT the past 2 weeks. The m"}, {"chunk_id": "crescendo_c41", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "References", "distance": 0.5617597699165344, "text_snippet": "- [1] https://chat.openai.com/share/ 31708d66-c735-46e4-94fd-41f436d4d3e9 . - [2] https://gemini.google.com/share/ 35f0817c3a03 . - [3] https://www.lesswrong.com/ posts/RYcoJdvmoBbi5Nax7/ jailbreaking"}, {"chunk_id": "pair_c03", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.5653972625732422, "text_snippet": "Although still at its infancy, the field of study surrounding large language models (LLMs) has shown significant promise in advancing numerous fields, including code generation [1], business analytics"}, {"chunk_id": "how_do_llms_fail_c51", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "H Beyond Harmful Restricted Behaviors", "distance": 0.5818428993225098, "text_snippet": "Our investigation thus far has largely focused on restricted behaviors relating to harm. However, there are also restricted behaviors that do not relate to harm, such as the leaking memorized personal"}, {"chunk_id": "crescendo_c37", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "6.3 Status of LLM Safety", "distance": 0.5856866836547852, "text_snippet": "While there are numerous significant efforts to enhance the safety of LLMs through alignment techniques and various defenses, we believe it is important to still assume that current LLMs will eventual"}, {"chunk_id": "pair_c29", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "E.1 Judge System Prompt", "distance": 0.5871254801750183, "text_snippet": "For general language models (GPT-4 and GPT-4-Turbo), we create a judge system prompt . This system prompt instructs the LLM to output a score from 1 to 10, with 10 corresponding to a detailed and full"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Legal Implications of Publishing Jailbreak Research\n\nBased on the provided context chunks, **there is no information about the legal implications of publishing jailbreak research itself.**\n\nThe context does contain references to legal implications in different contexts:\n\n1. **Legal implications of harmful content**: The context discusses legal consequences related to the *harmful activities* that jailbreaks might enable. For example, one chunk mentions that certain activities \"are illegal and can lead to harm, violence, and criminal charges\" [self_jailbreaking, self_jailbreaking_c14], and references to \"ethical, legal, or safety guidelines\" violations [pair, pair_c29].\n\n2. **General safety research recommendations**: There is a discussion about the importance of security research and defensive assumptions. One source recommends that \"system designers, users, and policymakers consider this assumption when evaluating or designing systems based on LLMs\" and notes that \"it is crucial to understand their limitations\" [crescendo, crescendo_c37]. However, this addresses responsible deployment rather than the legality of publishing research.\n\n**EVIDENCE MISSING**: The corpus does not contain information specifically addressing:\n- Legal frameworks governing publication of jailbreak techniques\n- Intellectual property or disclosure considerations for research\n- Regulatory requirements for publishing LLM safety research\n- Responsible disclosure practices from a legal standpoint", "latency_ms": 28362.2}
{"timestamp": "2026-02-24T12:48:01.194996+00:00", "query_id": "407762b5-aebf-46ed-8690-55bd72d258f4", "query_text": "What claims does the corpus make about the effectiveness of constitutional AI as a defense against jailbreaks?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "pair_c03", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "1 Introduction", "distance": 0.5094446539878845, "text_snippet": "Although still at its infancy, the field of study surrounding large language models (LLMs) has shown significant promise in advancing numerous fields, including code generation [1], business analytics"}, {"chunk_id": "how_do_llms_fail_c12", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4 Empirical Evaluation of Jailbreak Methods", "distance": 0.5349030494689941, "text_snippet": "We now quantitatively evaluate jailbreak methods on GPT-4, Claude v1.3, and the smaller GPT-3.5 Turbo across combinations of harmful prompts and attacks to understand the vulnerability landscape of th"}, {"chunk_id": "pair_c09", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "3.3 Running PAIR with parallel streams", "distance": 0.5369358062744141, "text_snippet": "Notably, Algorithm 1 is fully parallelizable in the sense that several distinct conversation streams can be run simultaneously. To this end, our experiments in \u00a74 are run using N parallel streams, eac"}, {"chunk_id": "how_do_llms_fail_c16", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.3 Results", "distance": 0.5392464399337769, "text_snippet": "Table 1 presents results on the curated dataset for GPT-4 and Claude v1.3. To show that the attacks are not specifically adapted to this dataset, Table 2 presents results on the larger, held-out datas"}, {"chunk_id": "how_do_llms_fail_c15", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "4.2 Evaluation", "distance": 0.5397948622703552, "text_snippet": "We evaluate jailbreaks on GPT-4, Claude v1.3, and GPT-3.5 Turbo against the datasets of harmful prompts introduced in Section 2.2. In the first phase, we test each jailbreak for each model against the"}, {"chunk_id": "artprompt_c16", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "ArtPrompt is effective against all victim LLMs.", "distance": 0.5416800379753113, "text_snippet": "ArtPrompt can bypass existing defenses against jailbreak attacks. In Table 4, we evaluate ArtPrompt when victim LLMs employ defenses PPL, Paraphrase, or Retokenization to mitigate jailbreak attacks. W"}, {"chunk_id": "pair_c12", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "4 Experiments", "distance": 0.5461488962173462, "text_snippet": "For GCG, we use the authors' implementation and run the attack for 500 iterations with a batch size of 512 for a similar computational budget of around 256,000 queries per behavior. The jailbreaks fro"}, {"chunk_id": "safedecoding_c04", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "1 Introduction", "distance": 0.5526110529899597, "text_snippet": "SafeDecoding then defines a new token distribution based on the token probabilities of both the original and expert models. Based on this new distribution, SafeDecoding samples tokens to generate a re"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "EVIDENCE MISSING\n\nThe provided context chunks do not contain any information about constitutional AI or its effectiveness as a defense against jailbreaks. While the corpus discusses various jailbreak attacks (PAIR, ArtPrompt, combination attacks) and some defense mechanisms (PPL, Paraphrase, Retokenization, SafeDecoding), there is no mention of constitutional AI in the available chunks.\n\nTo answer your question about constitutional AI's effectiveness as a defense, I would need context that specifically addresses this approach.", "latency_ms": 35229.1}
{"timestamp": "2026-02-24T14:36:54.437584+00:00", "query_id": "ecd74097-0851-411f-a33d-71383e701bcc", "query_text": "How does the Crescendo attack work?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "crescendo_c06", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3 Crescendo", "distance": 0.3762965798377991, "text_snippet": "Crescendo is a multi-turn jailbreaking technique that uses benign human readable prompts. Crescendo distinguishes itself from other approaches by utilizing the target model's outputs to direct the mod"}, {"chunk_id": "crescendo_c07", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "3.1 Threat Model", "distance": 0.398013710975647, "text_snippet": "Crescendo is a completely black-box jailbreak that makes no assumptions about the target model. It is executed through entirely normal interactions with an LLM, similar to most other jailbreaks. For a"}, {"chunk_id": "crescendo_c38", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "7 Conclusion", "distance": 0.4369184374809265, "text_snippet": "In this paper, we presented Crescendo, a novel jailbreak attack, along with its automation tool, Crescendomation. Unlike conventional methods, Crescendo does not require the adversary to explicitly sp"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# How the Crescendo Attack Works\n\nCrescendo is a multi-turn jailbreaking technique that operates through a gradual, iterative process rather than direct harmful prompts. Here's how it functions:\n\n## Core Mechanism\n\nCrescendo begins with an innocuous topic linked to the target task and progressively intensifies the requests [crescendo, crescendo_c06]. The key innovation is that it uses the target model's own outputs to direct it toward bypassing safety measures. Rather than explicitly specifying the harmful task, the adversary subtly guides the model through primarily benign prompts [crescendo, crescendo_c38].\n\n## Psychological Foundation\n\nThe approach mirrors the 'foot-in-the-door' psychological tactic, where agreeing to a small initial request increases the likelihood of complying with subsequent, larger demands [crescendo, crescendo_c06]. This incremental strategy is designed to circumvent defenses that primarily react to user prompts rather than to the progression of the conversation itself [crescendo, crescendo_c06].\n\n## Practical Execution\n\nThe adversary starts with a foundational question that can serve as a stepping stone toward the objective. Importantly, the user makes minimal direct references to the target task [crescendo, crescendo_c06]. One advantage of this approach is that the adversary can tailor responses precisely by iterating prompts\u2014for example, repeating directives like \"make it angry\" until the output meets their expectations [crescendo, crescendo_c06].\n\n## Technical Requirements\n\nCrescendo is a completely black-box jailbreak requiring no assumptions about the target model [crescendo, crescendo_c07]. It only needs access to a blackbox API to interact with the target LLM [crescendo, crescendo_c07].", "latency_ms": 3889.2}
{"timestamp": "2026-02-24T14:39:37.379318+00:00", "query_id": "8754e3cd-f93b-401e-8660-6d977d24afe9", "query_text": "What were popular LLM jailbreak techniques were proposed in 2024?", "metadata_filters": {"year": 2024}, "retrieved_chunks": [{"chunk_id": "safedecoding_c06", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "2.1 Jailbreak Attacks", "distance": 0.33406561613082886, "text_snippet": "Current jailbreak attacks can be categorized into two main classes: empirical jailbreak attacks and optimization-based adversarial attacks . For empirical jailbreak attacks, Liu et al. (2023b) demonst"}, {"chunk_id": "artprompt_c05", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "2 Related Work", "distance": 0.3352353572845459, "text_snippet": "Jailbreak Attacks. As LLMs become increasingly integrated in real-world applications, misuses of LLMs and safety concerns (Bender et al., 2021; Bommasani et al., 2021; Carlini et al., 2021; Ganguli et"}, {"chunk_id": "agentsmith_c05", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "2. Related Work", "distance": 0.37315309047698975, "text_snippet": "We primarily introduce related work on multi-agent systems and jailbreaking (M)LLMs, deferring full discussions to Appendix A. Multi-agent systems. A popular recent trend is to create multi-agent syst"}, {"chunk_id": "crescendo_c05", "source_id": "crescendo", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack", "section_title": "2 Related Works", "distance": 0.37377166748046875, "text_snippet": "Jailbreaking large language models (LLMs) is a relatively new topic; however, due to the widespread deployment of LLMs in various applications, several works have explored jailbreaks. We present some "}, {"chunk_id": "safedecoding_c31", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "References", "distance": 0.3955347537994385, "text_snippet": "2023. SmoothLLM: Defending large language models against jailbreaking attacks. ArXiv preprint , abs/2310.03684. - Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui"}, {"chunk_id": "goat_c04", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "2 Related Works", "distance": 0.40174710750579834, "text_snippet": "Jailbreaking research has so far largely focused on discovering a single adversarial prompt that triggers a violating response by a model. Some research studies this problem in an 'open box' setting t"}, {"chunk_id": "safedecoding_c30", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "References", "distance": 0.40266865491867065, "text_snippet": "2023. Catastrophic jailbreak of open-source llms via exploiting generation. ArXiv preprint , abs/2310.06987. - Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh "}, {"chunk_id": "agentsmith_c35", "source_id": "agentsmith", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast", "section_title": "A. Related Work (Full Version)", "distance": 0.40615081787109375, "text_snippet": "(Multimodal) LLM agents. For a long time, artificial intelligence has been actively engaged in creating intelligent agents that can mimic human thought processes and independently carry out complex ta"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# LLM Jailbreak Techniques Proposed in 2024\n\nBased on the provided context, the following jailbreak techniques were proposed in 2024:\n\n## Multi-Turn and Gradual Escalation Approaches\n\n**Crescendo** is a notable 2024 jailbreak that uses multi-turn conversations to gradually escalate benign prompts into harmful requests. [crescendo, crescendo_c05] Unlike many other jailbreaks, Crescendo does not employ adversarial or malicious text\u2014instead, it relies on the model's own outputs to construct the jailbreak, making it a black-box approach. The paper notes that Crescendo achieved higher Attack Success Rates compared to other state-of-the-art techniques and poses significant challenges for detection due to its use of benign, human-readable prompts. [crescendo, crescendo_c05]\n\n## Persuasion-Based Approaches\n\n**Persuasion taxonomy-based jailbreaks** were developed by Zeng et al. (2024), which employ principles from social science to jailbreak LLMs. [safedecoding, safedecoding_c06]\n\n## Automated Red Teaming\n\n**GOAT (Generative Offensive Agent Tester)** is a 2024 approach for automated red teaming that dynamically combines multiple jailbreaking techniques across conversation turns. [goat, goat_c04] The work also improved upon Crescendo's performance by applying new reasoning techniques to combine different adversarial prompting approaches. [goat, goat_c04]\n\nThese techniques represent a shift toward multi-turn, dynamic, and more sophisticated jailbreaking methods compared to single-prompt approaches.", "latency_ms": 5922.0}
{"timestamp": "2026-02-24T14:41:11.607971+00:00", "query_id": "c891936f-7f3d-4476-addb-1282b431efb5", "query_text": "Compare the Harmbench and Sorrybench benchmarks", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "harmbench_c02", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "Abstract", "distance": 0.4089537262916565, "text_snippet": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation"}, {"chunk_id": "harmbench_c20", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "7. Conclusion", "distance": 0.4117387533187866, "text_snippet": "Weintroduced HarmBench, a standardized evaluation framework for automated red teaming. We described desirable properties of a red teaming evaluation and how we designed HarmBench to meet the criteria "}, {"chunk_id": "harmbench_c11", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "4.1. Overview", "distance": 0.42506474256515503, "text_snippet": "HarmBench consists of a set of harmful behaviors and an evaluation pipeline. This follows the standard problem formulation in Section 3.1 and mirrors existing evaluations. We improve over existing eva"}, {"chunk_id": "sorrybench_c80", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "Prompt for Fine-tuned Bert-Base-Case to Evaluate Safety Refusal", "distance": 0.44452130794525146, "text_snippet": "User Request: '[UNSAFE INSTRUCTION]' Model Response: '[MODEL RESPONSE]' [SCORE] (training target) - General-purpose LLM safeguard models &amp; evaluators adopted in other safety benchmarks ( Llama-Gua"}, {"chunk_id": "sorrybench_c31", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "A.3 AUTHOR STATEMENT", "distance": 0.46561896800994873, "text_snippet": "| Manually crafted (from public websites and existing datasets) and LLM generated. | An unsafe instruction dataset. | | HarmBench (Mazeika et al., 2024) | 510 | Cybercrime & Unauthorized Intrusion, Ch"}, {"chunk_id": "agentharm_c33", "source_id": "agentharm", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "section_title": "C ADDITIONAL RESULTS", "distance": 0.47403931617736816, "text_snippet": "We compute the results on a subset of models. Harm Score : average score on the benchmark. Refusals : average refusals according to a semantic judge. Non-refusal Harm Score : average score on generati"}, {"chunk_id": "sorrybench_c33", "source_id": "sorrybench", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "section_title": "C AN OVERVIEW OF PRIOR SAFETY BENCHMARK DATASETS", "distance": 0.47494250535964966, "text_snippet": "We have summarized 16 prior (large) language model safety benchmark datasets in Table 3, where we demonstrate several key attributes (as shown in the columns, '#Samples', 'Safety Categories', 'Data So"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Comparison of HarmBench and SORRY-Bench\n\n## Scope and Scale\n\nHarmBench contains 510 unique harmful behaviors split into 400 textual and 110 multimodal behaviors [harmbench, harmbench_c11]. In contrast, SORRY-Bench appears to focus on systematically evaluating refusal behaviors but the context does not provide its total behavior count.\n\n## Categorization Schemes\n\n**HarmBench** uses a dual categorization system [harmbench, harmbench_c11]:\n- **Semantic categories** (7 types): Cybercrime & Unauthorized Intrusion, Chemical & Biological Weapons/Drugs, Copyright Violations, Misinformation & Disinformation, Harassment & Bullying, Illegal Activities, and General Harm\n- **Functional categories** (4 types): standard behaviors, copyright behaviors, contextual behaviors, and multimodal behaviors\n\n**SORRY-Bench** unifies safety categories from prior work using \"a systematic method\" to capture \"extensive unsafe topics in a granular manner,\" but the specific category breakdown is not detailed in the provided context [sorrybench, sorrybench_c33].\n\n## Key Structural Differences\n\nHarmBench includes novel functional categories that contextual and multimodal behaviors, enabling evaluation of \"more realistic and differentially harmful behaviors than explored previously\" [harmbench, harmbench_c11]. SORRY-Bench's structural approach is not detailed in the available context.\n\n## Important Caveat on Score Comparison\n\nWhile both benchmarks have been used to evaluate multiple LLMs, **direct comparison of scores across benchmarks is problematic**: \"we strongly caution against direct comparison across these benchmarks\" due to differences in configurations such as system prompts, sampling hyperparameters, safety metrics, datasets, and taxonomies [sorrybench, sorrybench_c33].\n\n## Purpose and Focus\n\nHarmBench serves as \"a standardized evaluation framework for automated red teaming\" [harmbench, harmbench_c02], while SORRY-Bench focuses on \"Systematically Evaluating Large Language Model Safety Refusal Behaviors\" based on its title reference [sorrybench, sorrybench_c33].", "latency_ms": 5461.4}
{"timestamp": "2026-02-24T14:42:54.469444+00:00", "query_id": "774948a2-00ac-4c08-999d-07b4117cf4be", "query_text": "Compare the X-teaming attack approach with the GOAT attack approach", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "xteaming_c04", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "1 Introduction", "distance": 0.41623640060424805, "text_snippet": "In addition, X -Teaming also demonstrates significant improvements in attack diversity. Prior semantic-driven (Chain of Attack, Yang et al. (2024b); ActorAttack, Ren et al. (2024)) and template-based "}, {"chunk_id": "xteaming_c10", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "3.2 Results", "distance": 0.4230482578277588, "text_snippet": "Our framework consistently uses fewer tokens across all target models compared to ActorAttack. As an added benefit, X -Teaming utilizes the free open-source model Qwen-2.5-32B as its attacker, whereas"}, {"chunk_id": "xteaming_c09", "source_id": "xteaming", "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "section_title": "3.2 Results", "distance": 0.42331355810165405, "text_snippet": "Attack success rate. Table 2 demonstrates that X -Teaming achieves state-of-the-art attack success rates across nearly every tested model, significantly outperforming existing single-turn and multi-tu"}, {"chunk_id": "goat_c10", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "5 Experimental Results", "distance": 0.4420081377029419, "text_snippet": "Overall Results. Figure 2 presents the main outcome of our evaluation. Observe that for any given model, GOAT achieves both higher ASR@1 and ASR@10 relative to Crescendo. For each conversation round, "}, {"chunk_id": "goat_c06", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "3.1 Red Teaming Attacks", "distance": 0.44346100091934204, "text_snippet": "GOAT scales predefined prompt-level attacks to a conversational context. Attacks are categorized and broken down into their constituents: name , definition , and exemplar user conversations , to highl"}, {"chunk_id": "goat_c14", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "7 Conclusion", "distance": 0.4502701163291931, "text_snippet": "In this work, we identified a class of adversarial behavior typically explored during manual red teaming that most existing approaches for jailbreaking LLMs overlook: multi-turn conversations. We intr"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "# Comparison of X-Teaming and GOAT Attack Approaches\n\n## Core Methodological Differences\n\n**X-Teaming** employs a multi-agent collaborative framework with adaptive extensions. It uses multiple attack plans (up to 50) that are dynamically modified through TextGrad optimization, enabling the generation of diverse attack strategies across multiple conversation turns (up to 7) [xteaming, xteaming_c04]. \n\n**GOAT** takes a different approach by scaling predefined single-turn prompt-level attacks to multi-turn conversations. It categorizes attacks into constituent parts (name, definition, and exemplar conversations) and embeds attack definitions within the system prompt of the attacker LLM using in-context learning [goat, goat_c06]. GOAT was instantiated with 7 predefined attacks that can be combined as layered attacks [goat, goat_c06].\n\n## Attack Diversity and Planning\n\nX-Teaming demonstrates significantly higher strategic diversity through its multi-agent framework. It achieves 153% improvements in attack plan diversity and 62% improvements in attack execution diversity compared to ActorAttack, with mean diversity scores of 0.702 for plan diversity and 0.466 for attack-level diversity [xteaming, xteaming_c04], [xteaming, xteaming_c10].\n\nGOAT's approach focuses on the attacker model's ability to reason through and combine predefined prompting strategies rather than generating entirely novel attack plans [goat, goat_c06]. The attacker model itself reasons whether it is getting closer to the goal and adjusts strategy accordingly [goat, goat_c14].\n\n## Computational Efficiency and Resource Usage\n\n**X-Teaming** shows strong efficiency metrics: successful attacks required on average only 3.5 plans, 0.6 TextGrad iterations, and 4.3 conversation turns, despite upper bounds of 50 plans, 4 iterations, and 7 turns [xteaming, xteaming_c09]. However, it uses a paid closed-source model (GPT-4o) as its attacker in primary experiments, though it can use open-source Qwen-2.5-32B [xteaming, xteaming_c10].\n\n**GOAT** operates with very limited computational budget and does not require a separate judge model during conversations, performing 3 times fewer LLM calls per turn compared to methods using external judges [goat, goat_c10]. GOAT requires no expensive gradient steps, thousand-query searches, or special model access [goat, goat_c10].\n\n## Attack Success Rates\n\nX-Teaming achieves state-of-the-art attack success rates, reaching 98.1% on DeepSeek V3 and 96.2% on Claude 3.7 Sonnet, with +9.8 percentage points over ActorAttack on GPT-4o (94.3% vs. 84.5%) [xteaming, xteaming_c09].\n\nGOAT outperforms Crescendo across all tested models in both ASR@1 and ASR@10 metrics, with capped conversation rounds at 5 turns [goat, goat_c10].\n\n## Key Conceptual Distinction\n\nX-Teaming emphasizes generating fresh, diverse attack trajectories at scale through multi-agent collaboration and optimization [xteaming, xteaming_c04], while GOAT emphasizes simulating manual red-teaming practices by enabling an attacker model to intelligently reason through and combine known prompting techniques in multi-turn contexts [goat, goat_c14].", "latency_ms": 9562.8}
{"timestamp": "2026-02-24T14:44:40.495778+00:00", "query_id": "e7434d09-ad55-4def-9b13-d177ce0d8f8c", "query_text": "Does the corpus contain evidence about quantum computing attacks on LLMs?", "metadata_filters": {}, "retrieved_chunks": [{"chunk_id": "how_do_llms_fail_c29", "source_id": "how_do_llms_fail", "title": "Jailbroken: How Does LLM Safety Training Fail?", "section_title": "A.1 Curated Dataset", "distance": 0.5438909530639648, "text_snippet": "[23] are presented as dialogue transcripts. Alongside the curated dataset of harmful prompts, we also evaluate the following harm less prompt as a control, to assess basic question-answering capabilit"}, {"chunk_id": "artprompt_c22", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "References", "distance": 0.5441485047340393, "text_snippet": "arXiv preprint arXiv:2311.07689 . - Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In"}, {"chunk_id": "artprompt_c19", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "8 Ethical Statement", "distance": 0.5472649931907654, "text_snippet": "The primary goal of this paper is to advance the safety of LLMs operating under adversarial conditions. This paper focuses on how corpora should be interpreted to enhance the safety of LLMs. This pape"}, {"chunk_id": "artprompt_c21", "source_id": "artprompt", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs", "section_title": "References", "distance": 0.5479161739349365, "text_snippet": "- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. Tec"}, {"chunk_id": "harmbench_c22", "source_id": "harmbench", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "section_title": "References", "distance": 0.5507339239120483, "text_snippet": "- Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 202"}, {"chunk_id": "safedecoding_c29", "source_id": "safedecoding", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding", "section_title": "References", "distance": 0.5530424118041992, "text_snippet": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. Techn"}, {"chunk_id": "goat_c16", "source_id": "goat", "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "section_title": "References", "distance": 0.5542294383049011, "text_snippet": "- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv pre"}, {"chunk_id": "pair_c21", "source_id": "pair", "title": "Jailbreaking Black Box Large Language Models in Twenty Queries", "section_title": "References", "distance": 0.5586711764335632, "text_snippet": "1, 3 - [17] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models with checklist. arXiv preprint arXiv:2005.04118 , 2020. 1, 3 - [18]"}], "prompt_template_version": "v2", "model_id": "claude-haiku-4-5", "generated_answer": "EVIDENCE MISSING\n\nThe provided context chunks do not contain any information about quantum computing attacks on LLMs. The chunks focus on jailbreaking techniques, safety training, red teaming, and adversarial attacks using conventional methods (such as ASCII art-based attacks, prompt injection, and gradient-based approaches), but there is no discussion of quantum computing in relation to LLM security or attacks.", "latency_ms": 1740.8}
